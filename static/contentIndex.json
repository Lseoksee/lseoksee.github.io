{"CLI-프로그램-명령어/Conda-명령어-정리":{"title":"Conda 명령어 정리","links":["리눅스/패키지-설치"],"tags":[],"content":"\n\n                  \n                  같이보기 \n                  \n                \n\n\nConda 설치\n\n\n\n기본 사용법\n처음 설치시\n\nconda init\n\n콘솔 초기화\n\n\n\n환경변수\nC:\\ProgramData\\miniconda3\nC:\\ProgramData\\miniconda3\\Scripts\nC:\\ProgramData\\miniconda3\\Library\nC:\\Users\\Seoksee\\AppData\\Roaming\\Python\\Python312\\Scripts\n가상환경 생성 이후\n\nconda activate\n\n이거 무조건 해야함\n안하면 모든 패키지가 전역으로 설치됨\n\n\n\nconda 자체 버전 업데이트 하기\n\n관리자 권한으로 입력\n# conda 자체 버전 업데이트\nconda update -n base conda\n \n# 모든 패키지 업데이트\nconda update --all\n \n# bash python 버전 업데이트 (권장하진 않는듯)\n# 해당 작업 후 conda update --all 권장\nconda install python=&lt;버전&gt;\n\n\n종속성 관리 전략\n\nenvironment.yml\nname: .conda\nchannels:\n    - defaults\ndependencies:\n    - pip=버전\n    - python=버전\n    - 기타...\n    - pip:\n          - 패키지==버전\nprefix: .conda\n\n패키지 설치전 pip show &amp;  conda search로 사용 가능한 버전 확인\nenvironment.yml에 작성 사용하려는 패키지, 버전 작성\nconda env update —prune\n\n명령으로 패키지 갱신\n\n\n이렇게해서 약간 node package.json  을 수기로 작성하여 갱신하는 느낌\npytorch 사용시 channels에 pytorch 추가\n\npip 패키지 삭제 갱신 (update —prune) 먹히게 하기\nconda config --set pip_interop_enabled True\n\n이게 실험기능으로 기본적으로 막혀있다\n\n--from-history 이건 여전히 pip 지원 안하는듯\n\n\n문제가 좀 큰 패키지가 있으면 개느려진다…\n공식문서 보기\n\nconda 기본 채널\n\nrepo.anaconda.com/pkgs/main/win-64/\n\n명령어 정리\nconda install [패키지]\n\n패키지 설치\n\nconda install pip\n\n프로젝트 내 pip 사용\n\nconda uninstall [패키지]\n\n패키지 삭제\n\nconda update [패키지]\n\n패키지 업데이트\n\nconda search [패키지]\n\n사용 가능한 버전 확인\n해당 명령으로 사용가능한 python 버전까지 사용가능\n\nconda create\n\n가상환경 만들기\n\n옵션\n\n-n [이름]\n\n가상환경 이름지정\n\n\npython= [버전]\n\n파이썬 버전 지정\n\n\n-p | --prefix [경로]\n\n가상 환경은 보통 사용자 홈 .conda 에 만들어 지는데 이를 프로젝트 폴더에 생성하는 식으로 경로를 지정 할 수있음\n==-p .conda: 이제 이러면 약간 node_modules 같이 관리가 가능해짐 그니까 이 방식으로 보통 생성할듯==\n\n\n\nconda activate [가상환경 이름 | 경로]\n\n가상환경 쉘에 접속한다.\n가상환경을-p 옵션으로 만들었을 경우 경로로 이동하여  conda activate ./[경로]\n안되면 ~/miniconda3/etc/profile.d/conda.sh\n\nconda env remove\n\n가상환경 삭제\n\n옵션\n\n-n [이름]\n\n[이름] 가상 환경 재거\n\n\n-p | --prefix [경로]\n\n프로젝트 폴더에 있는 가상환경 제거\n\n\n\nconda list\n\n설치된 패키지 확인\n\nconda env export -f environment.yml\n\n가상환경 공유하기\nyaml 형태로 종속성 밑 여러 환경이 저장된다.\n\n옵션\n\n--from-history:\n\n내가 직접 설치한것만 포함 (단 pip로 설치된건 미포함임)\n\n\n--no-builds:\n\n세부 빌드 버전 제외(os마다 버전이 다르니 왠만하면 제거)\n--from-history와 같이 사용 불가\n\n\n\nconda env create\n\nenvironment.yml 파일로 가상환경 만들기\n\n옵션\n\n-f [파일명]\n\n파일명 지정(default: environment.yml)\n\n\n-p | --prefix\n\n경로지정\n\n\n\nconda env update —prune\n\n해당 가상환경 설정파일에 패키지만 업데이트 하는거\n설치, 버전업데이트, 패키지 삭제 된거를 똑같이 적용하는\n\n옵션\n\n-f [파일명]\n\n파일명 지정 (default: environment.yml)\n\n\n-n [이름]\n\n가상환경 이름지정  (default: environment.yml에 지정된 이름)\n\n\n-p | --prefix\n\n경로지정\n\n\n\nconda clean -all -y\n\n케시 삭제\n\nconda config —set auto_activate_base false\n\n쉘에 (base) 뜨는거 비활성화\n"},"CLI-프로그램-명령어/Docker-명령어-정리":{"title":"Docker 명령어 정리","links":["스터디/Docker-스터디","리눅스/패키지-설치"],"tags":[],"content":"\n\n                  \n                  같이보기 \n                  \n                \n\n\nDocker 스터디\n리눅스 Docker 설치\n\n\n\n규칙\n\ncommit 이나 기타 이미지 선택할 때 이미지의 tag를 지정할 수 있다\n&lt;이미지명&gt;:&lt;테그&gt;\n\n동명의 이미지를 생성 하고 태그를 다르게 해서 버전을 관리한다\n\n\n원격저장소 이미지 이름 규칙:\n\n&lt;hub아이디&gt;/이미지명\n\n\n태그 규칙:\n\n아무것도 지정안하면 latest\n태그는 hub에서는 커밋로그 임. 버전관리를 확실히 할려면 태그를 붙이는게 좋다\n\n\n명령 구문 여러게 주기\n\nbash -c “명령어1 &amp;&amp; 명령어2”\n\n\n\n기본 사용법\n컨테이너 생성 &amp; 실행\n# 이미지를 Docker Hub 에서 내려 받기\ndocker pull &lt;이미지&gt;\n \n# 이미지 목록 확인\ndocker images \n \n# 해당 이미지 id 로 컨테이너를 실행하고 bash 를 활성화하여 접속\ndocker run -it &lt;이미지id&gt;\n컨테이너 종료\n\n\n컨테이너의 접속 중인 상태라면\n\nexit 쳐서 나오기\nctrl+d로 나오기\n\n\n\n컨테이너가 백그라운드 상태라면\n# 컨테이너 목록 확인\ndocker ps -a \n \ndocker stop &lt;컨테이너id&gt;\n \n# 강제종료\n===\ndocker kill &lt;컨테이너id&gt;\n===\n\n\n컨테이너 다시 실행\n# 컨테이너 목록 확인\ndocker ps -a \n \n# 컨테이너 재시작\ndocker restart &lt;컨테이너 ID&gt;\n \n# 컨테이너 진입 (attach 사용) \ndocker attach &lt;컨테이너 ID&gt;\n \n# 컨테이너 진입 (exec 사용)\ndocker exec -it &lt;컨테이너 ID&gt; /bin/bash\n\nattach: 이전과 동일한 쉘 세션에 진입\n\n만일 이전에 /home/test 에 접속되있던 상태였다면 똑같이 거기로 진입됨\n\n\nexec: 새로운 쉘 세션을 생성하여 진입\n\n그냥 putty를 하나 더 켜서 같은 사용자에 접근 했다 보면 됨\n\n\n\n컨테이너 &amp; 이미지 삭제\n\n컨테이너가 종료되있다 가정\n참고로 실행중인 컨테이너의 이미지를 삭제 하고 싶다고 -f 옵션을 붙어 강제 삭제 한다고 해도 걍 테크만 없어지는 거지 디스크에서 삭제되는건 아니기에 삭제할 이유가 없다\n\n# 컨테이너 id 확인 \ndocker ps -a\n \n# 컨테이너 삭제\ndocker rm &lt;컨테이너 id&gt;\n \n# 이미지 id 확인\ndocker images\n \n# 이미지 삭제\ndocker rm &lt;컨테이너 id&gt;\n컨테이너를 종료하지 않고 쉘 빠져 나오기\n\n(Ctrl + P) + Q  단축키 입력\n\nDocker Hub 에 로그인\n\nprivate 저장소 pull 받거나 push 할때 필요\n\ndocker login\nDocker Hub에 이미지 push 하기\n\n위에 로그인이 되어있어야\n태그는 안 붙이면 latest\n이미 원격저장소에 있는 상태이고 새로운 커밋을 작성할 시\n\n저장소 이미지 이름이랑 commit 된 이미지 이름이 같아야함\n단 태그는 달라도 무관\n같은 태그면 해당 태그의 커밋이 바뀐다\n\n\n\n# 원하는 컨테이너 커밋\ndocker commit &lt;컨테이너id&gt; &lt;hub 아이디&gt;/&lt;저장할이미지 명&gt;:&lt;태그 | null&gt;\n \n# 만일 이미지 이름이 원격이랑 안맞는 경우\n# 이미지 이름 바꾸기\n===\ndocker tag &lt;이미지id&gt; &lt;hub아이디&gt;/&lt;저장할이미지 명&gt;:&lt;태그 | null&gt;\n===\n \n# Docker Hub에 push\ndocker push &lt;컨테이너명&gt;\n이미지, 컨테이너 저장 &amp; 불러오기\n\n사실 이 방법 보다 Dockerfile 쓰는게 더 좋긴 하다\n\n\n\n이미지 저장 &amp; 불러오기\n# 이미지 tar로 저장\ndocker save -o &lt;파일명&gt;.tar &lt;이미지id&gt;\n \n# 이미지 불러오기 \ndocker load -i &lt;파일명&gt;.tar\n\n\n컨테이너 저장 &amp; 불러오기\n(여기서 함정은 불러오기 하면 이미지로 불러와지는)\n# 컨테이너 tar로 저장\ndocker export -o &lt;파일명&gt;.tar &lt;컨테이너id&gt;\n \n# 저장한 컨테이너를 이미지로 불러오기\n# cat 으로 tar 모두 읽어서 파이프 라인으로 하는 방법\ncat &lt;파일명&gt;.tar | docker import - &lt;이미지명&gt;\n\n\n백그라운드 실행\n\n\nattach 를 쓰는 방법\n# 표준 입/출력 콘솔 활성화 하고 백그라운드로 실행한다\n docker run -it -d &lt;이미지id&gt; &lt;명령&gt;\n \n# 진행중인 쉘에 진입\ndocker attach &lt;컨테이너id&gt;\n\n컨테이너가 실행중 일때만 사용가능하다\n다만 이 방법은 기존에 진행중이던 로그는 체크하지 못한다\n표준 입력이 필요한 상황에서는 해당 방법을 써야 함\n\n\n\nlogs를 쓰는 방법\n# 표준 입/출력 콘솔 활성화 하고 백그라운드로 실행한다\n docker run -it -d &lt;이미지id&gt; &lt;명령&gt;\n \n# 컨테이너 진행 로그를 실시간으로 출력\n docker logs -f &lt;컨테이너id&gt;\n\n컨테이너가 종료되었을 경우에도 사용가능하다\n다만 표준 입력은 처리하지 못한다\n대신 모든 진행 로그를 볼 수있다\n\n\n\n포그라운드 실행 후 백그라운드로\n# 포그라운드로 실행\n docker run -it &lt;이미지id&gt; &lt;명령&gt;\n \n# 이후 종료하지 않고 쉘에 빠져 나오기\n(Ctrl + P) + Q  단축키 입력\n\n이렇게 실행해놓고 위 2가지 방법 중 하나로 로그를 보면 된다\n\n\n\n백그라운드 종료 방지\n\n명령이 종료되면 자동으로 컨테이너가 중지한다\n명령에 sleep infinity를 추가해 컨테이너를 유지 시킨다\n\n# sleep infinity 만 사용\n docker run -i -d 35a88802559d sleep infinity\n \n# 다른 커멘드와 사용\ndocker run -i -d 35a88802559d bash -c &quot;apt update &amp;&amp; sleep infinity&quot;\nDockerfile로 이미지 빌드하기\n크로스 플렛폼\n\narm, x86 등 여러 플렛폼 지원 가능하기 빌드하기\n# 베이스 이미지의 지원 플렛폼 보기\n# 당연한말 이지만 베이스 이미지가 지원하는 플렛폼만 가능하니까 \ndocker buildx imagetools inspect &lt;베이스이미지&gt;\n \n# platform 값을 원하는 플렛폼 만큼 추가 하여 빌드\n# --load 를 --push로 바꾸면 바로 원격에 push 하는것도 가능\ndocker buildx build --platform linux/amd64,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x --load -t &lt;이미지:테그&gt; .\n\n\n처음 빌드 할때 작업\n\n\ncontainerd 활성화 하기\n# /etc/docker/daemon.json 파일 만들기\n sudo [편집기] /etc/docker/daemon.json \n \n\t#해당 파일에 이렇게 입력하고 저장\n\t{\n\t  &quot;features&quot;: {\n\t    &quot;containerd-snapshotter&quot;: true\n\t  }\n\t}\n \n#docker 재시작\nsystemctl restart docker\n\n\nbuilder 생성하기\n\n기본 builder 는 크로스 플렛폼 미지원이라 docker-container  드라이버로 builder 생성\n\n# platform 값을 원하는 플렛폼 만큼 추가 하여 빌드\ndocker buildx create --name multi-arch-builder --platform linux/amd64,linux/arm/v7,linux/arm64/v8,linux/ppc64le,linux/s390x --driver docker-container --bootstrap --use\n\n\n(참고) builder 기본값으로 되돌리기\n# 빌더 목록 보기\ndocker buildx ls\n \n# default 빌더로 되돌리기\ndocker buildx use default\n \n# 만든 빌더 삭제\ndocker buildx rm &lt;생성한 빌더이름&gt;\n단일 플렛폼\n\n일관성을 위해 docker buildx build 사용\n\n\n\narm, x86등 그 중 하나만 빌드가능 (PC 따라감)\n# --load 를 --push로 바꾸면 바로 원격에 push 하는것도 가능\ndocker buildx build --load -t &lt;이미지이름:테그&gt; .\n\n\n빌드이후 정리\ndocker buildx prune -f --all &amp;&amp; docker rm -f buildx_buildkit_multi-arch-builder0 &amp;&amp; docker rmi moby/buildkit:buildx-stable-1\n\n이게 방식이 임시 컨테이너인 moby/buildkit 을 pull 을 받아 거기서 작업하는거라 빌드 완료 후 에도 컨테이너가 남아있음\n\n케시제거\n# 대부분의 케시 제거\ndocker system prune -a\n \n# 빌드 케시 제거\ndocker buildx prune -f --all\n \n==혹시나 정리했는데도 과하게 용량이 적은경우==\n==이거 하면은 기존 컨테이너 사용불가==\nsudo rm -rf /var/lib/docker/overlay2\n# 서비스 재시작\nsudo systemctl stop docker\nsudo systemctl start docker\nDocker Compose\nCompose 기본 옵션\n# 파일 경로 또는 파일명이 docker-compose.yml 이 아닌경우\ndocker compose -f &lt;yml파일&gt;\n \n# 프로젝트 이름 지정\ndocker compose -p &lt;프로젝트 이름&gt;\nCompose 컨테이너 생성 &amp; 삭제\n# 컨테이너 빌드 &amp; 생성(재생성) 하고 실행\ndocker compose up -d\n \n#특정 작업만(yml 에 services에 정의된) 실행\ndocker compose run --rm &lt;작업명&gt; &lt;명령|null&gt;\n \n# 프로젝트, yml 파일 지정 예\n===\ndocker -f test.yml -p my-project compose up\n===\n \n# 컨테이너 삭제 볼륨, 네트워크, 이미지 삭제\ndocker compose down --rmi all\nCompose 컨테이너 실행 &amp; 중지\n# compose 컨테이너 실행\ndocker compose start\n \n# 프로젝트 지정 예\n===\ndocker -p my-project compose start\n===\n \n# compose 컨테이너 중지\ndocker compose stop\nCompose 상태 관리\n# Compose 프로젝트 목록 보기\ndocker compose ls --all\n \n# Compose 프로젝트 실행 상태 보기\ndocker compose ps --all\n \n# Compose 로그 출력\ndocker compose logs -f\nDocker Network 제어\n# 네트워크 목록 보기\ndocker network ls\n \n# 네트워크 세부 정보\ndocker network inspect &lt;네트워크ID&gt;\n \n# 네트워크 생성\ndocker network create --driver=&lt;드라이버&gt; &lt;네트워크 이름&gt;\n \n# 컨테이너의 네트워크 연결\ndocker network connect &lt;네트워크 이름&gt; &lt;컨테이너id&gt;\n \n# 컨테이너와 네트워크 연결 끊기\ndocker network disconnect &lt;네트워크 이름&gt; &lt;컨테이너id&gt;\n \n# 네트워크 제거\ndocker network rm &lt;네트워크ID&gt;\n호스트 ←&gt; 컨테이너 간 파일 전송\n호스트 → 컨테이너\ndocker cp &lt;호스트파일경로&gt; &lt;컨테이너이름&gt;:&lt;컨테이너 내부 경로&gt;\n컨테이너 → 호스트\ndocker cp &lt;컨테이너이름&gt;:&lt;컨테이너 내부 경로&gt; &lt;호스트파일경로&gt; \n명령어 정리\ndocker pull &lt;이미지&gt;\n\n허브 에서 이미지 pull 하기\n\ndocker run &lt;옵션&gt; &lt;이미지id&gt; &lt;명령 | null&gt;\n\n컨테이너 생성 &amp; 실행\n\n\n명령어를 지정하지 않으면 대부분 bash\n컨테이너 생성시 지정한 옵션값은 변경 불가\n\ndocker run -it &lt;이미지id&gt;\n\n컨테이너를 터미널을 사용하여 진입한다\n\n\nbash 사용시 -it 옵션 안하면 컨테이너가 자동으로 종료된다\n\n옵션\n\n--name [컨테이너 이름]\n\n컨테이너 이름지정 (이름을 컨테이너 식별값으로 사용가능)\n\n\n-p [호스트포트]:[컨테이너포트]/[tcp/udp | null]\n\n컨테이너 포트 포워딩 &amp; 포트 개방\n여러개 개방 하고 싶으면 -p 를 여러번 쓰면 됨\n\n\n-d\n\n컨테이너를 백그라운드 모드로\n\n\n-i\n\n표준입력을 열어둔다\n\n\n-t\n\n터미널을 할당한다\n\n\n--rm:\n\n컨테이너가 종료된 이후에 자동삭제\n명령을 지정한경우 해당 명령어 실행하면 자동 삭제\n\n\n--gpus [gpu]:\n\nall: 모든 gpu 사용\n호스트 pc에 엔비디아 그래픽 드라이버 설치 작업 필요\n\n\n--network [네트워크 이름]:\n\n연결할 네트워크 지정\n\n\n-v [볼륨|폴더경로:마운트될 위치]:\n\n볼륨 마운트 또는, 폴더 마운트\n이때 폴더를 마운트 하면 컨테이너에서 파일을 건들면 호스트 PC에 파일도 수정된다\n\n\n--hostname [호스트 이름]:\n\n쉘 호스트 이름 변경\n\n\n\ndocker create &lt;옵션&gt; &lt;이미지id&gt; &lt;명령 | null&gt;\n\n컨테이너를 실행하지 않고 생성\n\n\n명령어를 지정하지 않으면 대부분 bash\n컨테이너 생성시 지정한 옵션값은 변경 불가\n\n옵션\nrun 과 동일\ndocker container restart &lt;컨테이너id | 컨테이너 이름&gt;\n\n컨테이너 재시작 및 시작\n\n\n컨테이너 생성 시 입력한 명령이 있으면 그거 또 다시 실행 하는거\n\ndocker attach &lt;컨테이너id | 컨테이너 이름&gt;\n\n실행중인 컨테이너의 쉘에 접속\n\n\n정확히 말하면 표준 입/출력 신호를 전달\nattach의 경우 이전과 동일한 쉘 세션에 진입\n\n만일 docker run -d로 백그라운드 실행 시 포그라운드 전환 하는게 가능하다\n이전 진행 로그를 볼수는 없기 때문에 조금 불편하긴하다\n다만 표준 입력이 있는 프로그램 이렇게 진행 할 수밖에 없다\n\n\n\ndocker exec &lt;옵션&gt; &lt;컨테이너id | 컨테이너 이름&gt; &lt;명령&gt;\n\n실행중인 컨테이너의 명령전달\n\ndocker exec -it &lt;컨테이너id | 컨테이너 이름&gt; /bin/bash\n\n실행중인 컨테이너의 쉘에 접속\n\n\nexec의 경우 새로운 쉘을 생성하여 접속됨\n\n별일 없으면 attach 로 접속함\n그러나 기본 쉘이 서버를 굴리고 있어서 무한대기 상태인 경우에는 해당 명령으로\n\n\n\n옵션\n\n-d:\n\n백그라운드로 명령 전달\n\n\n\ndocker ps -a\n\n컨테이너 목록 확인\n\ndocker images\n\n이미지 목록 확인\n\ndocker stop &lt;컨테이너id | 컨테이너 이름&gt;\n\n컨테이너 종료\n\nsudo docker kill &lt;컨테이너id | 컨테이너 이름&gt;\n\n컨테이너 강제 종료\n\ndocker rm &lt;컨테이너id | 컨테이너 이름&gt;\n\n컨테이너 삭제\n\ndocker rmi ubuntu &lt;이미지&gt;\n\n이미지 삭제\n\ndocker logs -f &lt;컨테이너id | 컨테이너 이름&gt;\n\n실시간으로 컨테이너 로그 출력\n\n\n만약 한번만 출력하려면 -f 빼면됨\n\ndocker commit &lt;옵션&gt; &lt;컨테이너id | 컨테이너 이름&gt; &lt;저장 이미지 명&gt;\n\n컨테이너의 작업한 모든 사항을 이미지로 만들기\n\n\n만들어진 이미지는 docker images를 통해 볼 수 있음\n저장 이미지 명=이미지:태그 (태그 다는법)\n\n옵션\n\n-m: 커밋 메시지\n\ndocker tag &lt;이미지id&gt; &lt;바꿀 이미지 명&gt;\n\n이미지에 태그, 이미지 이름 변경\n\n\n사실 바꾸는게 아니라 이미지가 복제 되는것\n태그 따로 설정안하면 latest\n저장 이미지 명=이미지:태그 (태그 다는법)\n\ndocker buildx build &lt;옵션&gt; &lt;경로&gt;\n\nDockerfile 빌드하여 이미지로 만들기\n\n\n최신 버전인 buildx 로 사용함\n경로에 경우 현제 경로면 .\n--load, --push 옵션 중 하나는 무조건 필수\n\n옵션\n\n--load:\n\n빌드 이후 이미지 목록에 불러오기\n\n\n--push:\n\n빌드 이후 원격에 push 하기\n\n\n--platform [플렛폼1],[플렛폼2]...:\n\n크로스 플렛폼 지원\n\n\n-f [파일명]:\n\n파일명이 Dockerfile 아닌경우\n\n\nt [tag]:\n\n생성되는 이미지에 태그 달기\n\n\n--build-arg [&quot;키=값&quot;]:\n\nDockerfile ARG 변수의 값 전달\n\n\n\ndocker image history &lt;이미지id&gt;\n\n이미지 history 보기\n\ndocker image inspect &lt;이미지id&gt;\n\n이미지 정보 보기\n\ndocker container inspect &lt;컨테이너id | 컨테이너이름&gt;\n\n컨테이너 정보 보기\n"},"CLI-프로그램-명령어/Docker-Swarm-명령어-정리":{"title":"Docker-Swarm 명령어 정리","links":["스터디/Docker-스터디"],"tags":[],"content":"\n\n                  \n                  같이보기 \n                  \n                \n\n\nDocker Swarm 스터디\n\n\n\n기본 사용법\nSwarm Manager 생성 &amp; Worker 가입\nManager 생성\ndocker swarm init --autolock\n\n\n이때 출력 되는 following key 하단 키를 잘 보관해야함\n\n\n호스트PC 재부팅시 입력해야 될 수도 있음\nprovide the following key:\n\tSWMKEY-1-XDeU3XC75Ku7rvGXixJ0V7evhDJGvIAvq0D8VuEAEaw\n\n\n개방할 포트\n\nTCP-2377: Manager-Worker 간 통신 포트\nTCP/UDP-7946: Overlay 네트워크 노드 검색\nUDP-4789: Overlay 네트워크 트레픽 송/수신\n\nWorker 가입\n\n\nManager 가 init 을 수행하면 이런 출력이 나옴\n해당 명령을 등록하려는 Worker 호스트pc에 입력\n\n\nIP 주소는 내부IP 주소로 출력되므로 외부 호스트 연결은 외부 ip 주소로 대체 해야함\ndocker swarm join --token &lt;토큰&gt; &lt;ip주소&gt;:&lt;포트번호&gt;\n\n\nSwarm 탈퇴 &amp; Swarm 모드 중지\n\n아예 왼전히 Swarm 연관성을 제거하기위해서는 Worker, Manager 쪽 둘 다 해야함\n\nWorker 쪽\ndocker swarm leave --force\nManager 쪽\ndocker swarm leave --force &amp;&amp; docker node rm &lt;호스트ID&gt;\n# 호스트 id는 docker node ls 로 확인 \nswarm 정보확인\n\nManager 나, Worker가 자신이 어떤 swarm에 가입 되있는지 확인 가능\n\ndocker info\n# swarm 부분 확인\nNode 조회 &amp; 관리\n# 연결된 Manager &amp; Worker 조회\ndocker node ls\n \n# 연결된 node 삭제\ndocker node rm &lt;node_id&gt;\n \n# 라벨 달기\ndocker node update --label-add &lt;키&gt;=&lt;값&gt; &lt;node_id&gt;\n서비스 생성\n# 기본 생성\ndocker service create --name &lt;서비스 명&gt; --replicas &lt;replicas 수&gt; &lt;이미지 명&gt;\n \n# 포트 개방 시\ndocker service create --name &lt;서비스 명&gt; --replicas &lt;replicas 수&gt; &lt;이미지 명&gt; \\\n--publish=&lt;바인딩 포트&gt;:&lt;내부포트&gt;\n\nconstraint 와 같은 조건을 걸지 안은 경우 명령을 수행하고 있는 호스트PC 에 replicas 값 만큼 컨테이너가 생성이 된다\n배치 제약 조건 (--constraint):\n\n특정 노드에 서비스를 배치하거나, 제외하고 전부 서비스에 배치가능\n\n\n배치 스프레드 (--placement-pref):\n\n설정한 replicas수 만큼 조건에 맞는 노드에 균등하게 배포함\n만약 replicas=4 라면 A=2개 B=2개 이런 식으로 균등하게 배치해서 로드 벨런싱이 가능한거\n\n\n\n특정 Worker에 서비스 생성하기\ndocker service create --name &lt;서비스 명&gt; --replicas &lt;replicas 수&gt; \\\n--constraint node.hostname==&lt;Worker 호스트 이름&gt; \\\n&lt;이미지 명&gt;\n \n===만약 라벨을 단 경우 이렇게도 조건 가능==\n--constraint node.labels.&lt;키&gt;==&lt;값&gt;\n모든 호스트에 분산 처리하기\n# hostname 을 가진 모든 node 에 replicas 수 만큼 나눠서 배치하기\ndocker service create --name &lt;서비스 명&gt; --replicas &lt;replicas 수&gt; \\\n--placement-pref &#039;spread=node.hostname&#039; \\\n&lt;이미지 명&gt;\n서비스 조회 &amp; 관리\n# 서비스 조회\ndocker service ls\n \n# 서비스 삭제\ndocker service rm &lt;서비스 명&gt;\n \n# 서비스 로그 보기\n# -f 실시간으로 보기, -n 최대출력 줄\ndocker service logs -f -n &lt;숫자&gt; &lt;서비스 명&gt;\nswarm 을 Docker-Compose 로 관리 시 명령어\n\ndocker compose 명령어가 아닌 docker stack 을 써야함\n\n# Compose 배포 \ndocker stack deploy -d -c &lt;Compose 파일&gt; &lt;stack 이름&gt;\n \n# 배포된 swarm compose 목록 확인\ndocker stack ls\n \n# swarm compose 삭제\n# 컨테이너랑 네트워크는 삭제되는데 이미지는 삭제 안됨\ndocker stack rm &lt;stack 이름&gt;\nswarm 에서 GPU 사용하기\n사전 작업\n\n\nGPU GUID 확인\nnvidia-smi -a | grep UUID\n# GPU-XXXXX\n\n\n/etc/docker/daemon.json 파일 수정\n\n없으면 만듬\n\n{\n\t&quot;runtimes&quot;: {\n\t\t&quot;nvidia&quot;: {\n\t\t\t&quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,\n\t\t\t&quot;runtimeArgs&quot;: []\n\t\t}\n\t},\n\t&quot;default-runtime&quot;: &quot;nvidia&quot;,\n\t\t&quot;node-generic-resources&quot;: [\n\t\t\t&quot;NVIDIA-GPU=&lt;위에서 확인한 GUID&quot;\n\t\t]\n\t}\n\n\n/etc/nvidia-container-runtime/config.toml에 아래 구문 추가\nswarm-resource = &quot;DOCKER_RESOURCE_GPU&quot;\n\n\ndocker 재시작\nsudo systemctl restart docker\n\n\n서비스 생성 시\n\n\nENV 에 NVIDIA_VISIBLE_DEVICES 를 추가\ndocker service create --replicas 1 -e NVIDIA_VISIBLE_DEVICES=&#039;ALL&#039; &lt;이미지&gt;\n\n\n(참고) docker-compose 사용 시\nservices:\n    run:\n        deploy:\n            replicas: 1\n        environment:\n            # docker swarm에서 gpu 사용을 위한 환경 변수 \n            - NVIDIA_VISIBLE_DEVICES=ALL\n\n\n명령어 정리\ndocker service create &lt;옵션&gt; &lt;이미지명&gt;\n\n서비스 생성 &amp; 실행\n\n옵션\n\n--name &lt;서비스 이름&gt;\n\n서비스 이름 지정\n\n\n--hostname:\n\n호스트 이름 지정\n\n\n--label:\n\n라벨 추가\n\n\n--replicas &lt;replicas 수&gt;:\n\nreplicas 수 지정 (컨테이너 하나만 만들려면 1로)\n\n\n--publish=&lt;바인딩 포트&gt;:&lt;내부포트&gt;:\n\nservice 포트 개방\n\n\n--constraint:\n\n배치 제약 조건\n특정 노드에 서비스를 배치하거나, 제외하고 전부 서비스에 배치가능\n\n\n--placement-pref:\n\n배치 스프래드\n설정한 replicas수 만큼 조건에 맞는 노드에 균등하게 배포함\n만약 replicas=4 라면 A=2개 B=2개 이런 식으로 균등하게 배치해서 로드 벨런싱이 가능한거\n\n\n\ndocker service update &lt;옵션&gt; &lt;서비스 명&gt;\n\n해당 서비스 옵션 업데이트\n\n옵션\n\n--publish-add  published=&lt;바인딩 포트&gt;,target=&lt;내부포트&gt;:\n\n개방할 포트 추가\n\n\n--publish-rm:\n\n개방 포트 제거\n\n\n-t:\n\ntty 할당\n\n\n\ndocker node update &lt;옵션&gt; &lt;적용할 node_id&gt;\n\nnode 정보 업데이트\n\n옵션\n\n--availability &lt;active | pause | drain&gt;:\n\nnode 상태 변경\n\n\n--label-add &lt;라벨 키&gt;:\n\n라벨 추가\n\n\n--label-rm\n\nnode 라벨 삭제\n\n\n\ndocker node promote &lt;적용할 node_id&gt;\n\n해당 node를 Manager로 승격\n다만 잘 안되는듯\n\ndocker node demote &lt;적용할 node_id&gt;\n\n해당 node를 Worker로 강등\n"},"CLI-프로그램-명령어/Git-명령어-정리":{"title":"Git 명령어 정리","links":["스터디/Git-스터디"],"tags":[],"content":"\n\n                  \n                  같이보기 \n                  \n                \n\n\nGit 스터디\n\n\n\ngit update-index —assume-unchanged &lt;파일명&gt;\n\n커밋무시\n\ngit update-index —no-assume-unchanged &lt;파일명&gt;\n\n커밋 무시 되돌리기\n\ngit fetch —all\n\n모든 브랜치 패치하기\n\ngit fetch —all —prune\n\n원격저장소 삭제된 브랜치 정보 정리\n\ngit rm -r —cached .\n\n.gitignore 적용 안될때 캐시제거\n\ngit add &lt;. | 파일명&gt;\n\n변경사항 스테이징 (스테이징 해야만 커밋가능)\n. 은 모두 선택\n\ngit restore &lt;. | 파일명&gt;\n\n변경사항 취소 (스테이징 된거 제외)`\n. 은 모두 선택\n\n옵션\n\n--staged:\n\n스테이징 취소하기 (변경사항은 유지)\n\n\n\ngit pull\n\n원격 pull 하기\n만일 커밋 이후에 pull을 통해서 원격 브랜치 변경사항을 받아오면 merge 커밋이 생김\n커밋안한 변경사항 있는 상태에서 pull 할때 해당 변경사항과 충돌이 생기는 경우 pull 이 안되기에 stash에 저장하거나, 커밋하거나 버리거나 해서 변경사항 없는 상태로 만들어야 함\n\ngit pull &lt;원격저장소&gt; &lt;브랜치&gt;\n\n특정 브랜치 pull 하기\n\n옵션\n\n--ff-only\n\npull 할때 merge 안생기게 fast-forwarded 로 하게끔\n\n\n--rebase\n\n--ff-only 가 안먹히는 경우 rebase\n\n\n\ngit push\n\n커밋 push 하기\n\ngit push &lt;원격저장소&gt; &lt;브랜치&gt;\n\n특정 브랜치 push 하기\n\n옵션\n\n-f\n\n로컬 커밋으로 강제 푸쉬 진행하기\nreset, rebase 진행 시 사용\n\n\n\ngit status\n\ngit 상태 보기\npull 할꺼, push 할꺼 체크 가능\n변경 사항 체크가능\n\n빨강: 스테이징 안된것\n초록: 스테이징 된것\n\n\nUnmerged paths:\n\n해결해야하는 병합 파일 목록\n\n\n\ngit diff &lt;옵션 | null&gt; &lt;비교대상 | null&gt;\n\n변경 사항 보기 (기본은 스테이징 안한것만)\n새로 만든 파일인 경우 반드시 스테이징 해야만 출력된다\n\ngit difftool &lt;옵션 | null&gt; &lt;비교대상 | null&gt;\n\n이건 아예 vim 같은 CLI 텍스트 에디터로 비교해줌\n사용볍은 diff와 같음\n\n기본옵션\n\n--staged:\n\n스테이징 항목 변경사항보기\n그냥 하면 스테이징 안된것만\n\n\n\n비교대상\ngit diff HEAD\n\n스테이징 포함하여 모든 현재 변경사항 출력\n\ngit diff &lt;브랜치A&gt; &lt;브랜치B&gt;\n\n두 브랜치 간 비교\n\ngit diff &lt;커밋A&gt; &lt;커밋B&gt;\n\n두 커밋 간 비교\n\ngit diff &lt;파일명&gt;\n\n특정 파일에 변경사항 확인\n\ngit merge —abort\n\n병합 해드 취소\n\ngit reset &lt;리셋타입 | null&gt; &lt;돌아갈 주소&gt;\n\n커밋 리셋\n리셋이지만 앞 작업으로 가는 것도 가능하다\npush 하려면 push -f로 강제 푸쉬해야 함\n\n누군가 reset 되기 전까지의 커밋까지 정보를 가지고 있는 경우 해당 사람도 똑같이 reset 진행 해야함\n\n\n\n옵션\n리셋타입\n\n--soft | --hard:\n\nreset 타입 지정\n\n\n\n돌아갈 주소\n\n커밋 값:\n\ngit log 에서 확인\n\n\n작업로그 값:\n\ngit reflog 에서 확인\n\n\n브랜치명:\n\n해당 브랜치 까지 리셋 (원격도 가능 원격저장소/브랜치명)\n원격 브랜치로 리셋 할 경우 pull 받지 않고 로컬 브랜치를 강제로 원격 브랜치에 위치시킴\n\n로컬 브랜치가 꼬여 pull이 안되는 경우 사용하기 좋다.\n대신에 모든 로컬 커밋은 취소된다.\n\n\n\n\nHEAD~:\n\n이전 커밋 취소\n\n\n\ngit log\n\n커밋로그 보기 (빠져 나올때 “q”)`\n\ngit log —all —oneline —graph\n\n그래프 형태로 보기\n\ngit reflog &lt;브랜치&gt;\n\n브랜치 작업로그 보기  (빠져 나올때 “q”)\n\ngit rebase &lt;브랜치&gt;\n\nrebase 하기\npush 하려면 push -f로 강제 푸쉬해야 함\n\n누군가 rebase 되기 전까지의 커밋을 보유하고 있는 경우\ngit reset --hard 원격저장소/브랜치 해서 원격 브랜치로 reset 해야함\n\n\n\n옵션\n\n--continue:\n\nrebase 작업 계속하기\n\n\n--skip\n\nHEAD(Rebase 타겟)가 가지고 있는 사항으로 유지 시킨다\n충돌해결 안해도 상관없는경우 해당부분 스킵\n\n\n--abort:\n\nrebase 되돌리기\n\n\n-s ours\n\n충돌나는 모든 커밋을 타겟 브랜치 커밋으로 교체\n\n\n-X ours\n\n위와 반대\n충돌나는 모든 커밋을 기존 브랜치 커밋으로 교체\n\n\n\ngit branch &lt;브랜치&gt;\n\n브랜치 생성\n\ngit branch —all\n\n모든 브랜치 보기\n원격 브랜치는 remotes 빼고 보면 됨\n\ngit branch -D &lt;브랜치&gt;\n\n로컬 브랜치 삭제\n\ngit push &lt;원격 저장소&gt; &lt;브랜치&gt;\n\n원격 브랜치 생성\n원격 저장소는 보통 origin\n\ngit push &lt;원격 저장소&gt; -d &lt;브랜치&gt;\n\n원격 브랜치 삭제\n\ngit branch -m &lt;현재이름&gt; &lt;바꿀이름&gt;\n\n브랜치명 변경\n이름 변경이후 원격 브랜치 push 는 이전 브랜치 삭제 → 변경된 브랜치 push\n\ngit switch &lt;브랜치&gt;\n\n브랜치 간 전환\n\n옵션\n\n-c:\n\n브랜치 생성  &amp; 전환\n\n\n-f:\n\n변경사항(커밋안한) 존재할 시 버리고 전환\n\n\n-m:\n\n변경사항과(커밋안한) 병합하기\n\n\n\n브랜치\n\n-:\n\n최신 커밋으로 돌아가기\n\n\n\ngit checkout &lt;커밋값 | 브랜치&gt;\n\n브랜치, 커밋 전환\nswitch 로는 커밋간 전환이 안되므로 체크아웃으로\n\ngit checkout &lt;커밋값 | 브랜치&gt; — .\n\n브랜치 전환하지 않고 해당 브랜치의 변경사항을 가져오기\n\n옵션\n\n-f:\n\n변경사항(커밋안한) 존재할 시 버리고 전환\n\n\n-m:\n\n변경사항과(커밋안한)  병합하기\n\n\n\ngit stash &lt;작업 | null&gt; &lt;옵션 | null&gt;\n\ngit 임시 저장소, 스텍구조로 되어있음\n\n\nstash에 변경사항 저장하기\n-m 설정 안하면 0, 1, 2 순으로 저장\n\n기본옵션\n\n-m:\n\n저장할 이름\n\n\n -S:\n\n스테이지에 있는것만\n\n\n\n작업\ngit stash list\n\nstash 목록보기\n\ngit stash drop &lt;이름 | null&gt;\n\nstash 제거하기\n이름 안정하면 가장 최근꺼\n\ngit stash apply &lt;index | null&gt;\n\nstash 적용하기 (이름은 쓸 수 없음 list 명령에서 앞에 인덱스 확인)\n이름 없으면 가장 최근꺼\n\ngit stash pop\n\n가장최근꺼 적용하고 삭제\n스텍구조라 drop 으로 하면 인덱스가 뒤섞임\n\ngit submodule &lt;작업&gt;\n\ngit 에서 심볼릭 링크처럼 다른 리포지토리를 엮어 쓰는방법\n\ngit submodule add &lt;리포지토리url&gt; &lt;적용경로&gt;\n\n&lt;적용경로&gt; 에 리포지토리 추가\n\ngit submodule init\n\nsubmodule 이 적용된 리포지토리를 clone 받고나서 바로 이걸 해줘야됨\n그래야 update 가능\n\ngit submodule update —remote —recursive\n\nsubmodule 브랜치 정보 업데이트\nsubmodule 쪽 브랜치에 새로운 커밋이 올라오면 해당 커밋에 해드 sha 값으로 업뎃\n\n참고\n로컬, 원격 브랜치 선택\n\n로컬:\n\n그냥 브랜치명 master\n\n\n원격\n\n원격 저장소명/브랜치명 origin/master\n\n\n\n모든 것을 의미하는 문자\n\n.\n"},"CLI-프로그램-명령어/MySql-명령어-정리":{"title":"MySql 명령어 정리","links":[],"tags":[],"content":"접속\n\nmysql -u [계정이름] -p -h [DB 주소]\n\nDB 백업 &amp; 복원\n백업\n# 시스템 DB 를 포함한 모든 DB 백업\nmysqldump -u&lt;이름&gt; -p&lt;비번&gt; --all-databases --add-drop-database &gt; &lt;저장파일&gt;.sql\n \n#특정 DB만 백업\nmysqldump -u&lt;이름&gt; -p&lt;비번&gt; --databases &lt;DB1&gt; &lt;DB2&gt;... --add-drop-database &gt; &lt;저장파일&gt;.sql\n \n복원\n백업된 전체 DB를 복원하려는 경우\n# --all-databases 옵션으로 전체 백업한경우 시스템 DB 때문에 --force 옵션을 추가해야한다\n# 시스템 테이블 때문에 오류걸릴수 있는데 무시해도 됨\nmysql -u &lt;이름&gt; -p&lt;비번&gt; --force &lt; &lt;백업파일&gt;.sql\n백업 DB 중에 특정 DB 만 백업 하려는 경우\n\n이때는 특이하게 복원하려는 DB가 mysql에 존재해야 한다 (Unknown database 애러뜸)\n따라서 복원하려는 DB가 없으면 생성해주자\nCREATE DATABASE &lt;복원하려는DB&gt;;\n\n\n# 복원하지 말아야 할 DB까지 복원 시키는거 같은 오류가 뜰수 있는데\n# 상관 하지 않아도 됨\nmysql -u &lt;이름&gt; -p&lt;비번&gt; --force --one-database &lt;복원하려는DB&gt; &lt; &lt;백업파일&gt;.sql\nDB 백업을 안했는데 서버가 나가 버린경우\n\n\n디스크만 따로 뽑아서 다른 PC에 연결한다\n\n리눅스는 ext4 파일시스템이라 윈도우에서는 따로 프로그램 깔아야함\nLinux File Systems for Windows\n\n\n\n/var/lib/mysql 폴더를 백업 한다\n\n\n새롭게 mysql 을 설치하고 서비스를 잠시 중단한다\nsudo systemctl start mysql\n\n\n새로 설치한 mysql이 생성한 /var/lib/mysql 폴더를 지운다\n\n\n백업한 /var/lib/mysql 폴더를 그 자리에 위치 시킨다\n\n\n해당 명령어를 차례대로 입력한다\n# 소유권을 mysql:mysql로 변경 \nsudo chown -R mysql:mysql /var/lib/mysql\n \n# 디렉토리 권한 설정\nsudo find /var/lib/mysql -type d -exec chmod 750 {} \\;\n \n# 파일 권한 설정\nsudo find /var/lib/mysql -type f -exec chmod 660 {} \\;\n\n\n다시 mysql 서비스를 실행한다\nsudo systemctl start mysql\n\n"},"CLI-프로그램-명령어/Pnpm-명령어-정리":{"title":"Pnpm 명령어 정리","links":[],"tags":[],"content":"실행 명령은 pnpm 으로\npnpm i\n\n의존 패키지 모두 설치\n\npnpm add [패키지]\n\n해당 패키지와 의존된 모든 패키지 설치\n\n옵션\n\n-g 글로벌 설치\n-D devDependencies에 적용\n\npnpm remove [패키지]\n\n해당 패키지 재거\n\npnpm exec [실행명령]\n\nnpx tsc 이런걸 대체 하는거인듯?\n\npnpm dlx [실행명령]\n\nnpx create-react-app 이런거 대체\n\npnpm config set store-dir [경로]\n\n해당 경로로 실제 패키지가 저장됨\n\npnpm store prune\n\n어디에도 쓰지지 않은 패키지 재거 (케쉬제거 하는)\n"},"CLI-프로그램-명령어/Poetry-명령어-정리":{"title":"Poetry 명령어 정리","links":[],"tags":[],"content":"poetry init\n\n가상환경 생성\n\npoetry install\n\n의존 패키지 모두 설치\n\npoetry add [패키지]\n\n해당 패키지와 의존된 모든 패키지 설치\n\npoetry remove [패키지]\n\n해당 패키지와 의존된 모든 패키지 설치\n\npoetry show\n\n설치된 패키지 목록 보기\n\npoetry run [실행패키지]\n\n패키지에 실행 가능한 패키지 있으면 실행\n\npoetry run fastapi\npoetry run uvicorn\n이런것들\n\n\n\npoetry env use [경로]\n\n가상환경 경로 이어주기\n\npoetry env remove —all\n\n해당 프로젝트 모든 가상환경 지우기\n\npoetry cache clear —all .\n\n모든 캐시 삭제\n\npoetry export -f requirements.txt —output requirements.txt\n\nrequirements.txt로 export 하기\n"},"CLI-프로그램-명령어/pip-명령어-정리":{"title":"pip 명령어 정리","links":[],"tags":[],"content":"pip install [패키지]\n\n패키지 설치\n\n옵션\n\n-r [requirements.txt 파일]\n\nrequirements.txt 에 있는 패키지 설치\n\n\n\npip show [패키지]\n\n해당 패키지에 최신버전과 여러 정보 나열\n\npip freeze &gt; requirements.txt\n\n설치된 패키지를 requirements.txt 에 저장\n\npip cache purge\n\n케시 삭제\n\npip install [패키지] —upgrade\n\n패키지 업데이트\n"},"README":{"title":"README","links":[],"tags":[],"content":"My-Study-File\nworkflows 수정법\n\n\nWORKFLOW_ID 확인 방법\ncurl -L \\\n-H &quot;Accept: application/vnd.github+json&quot; \\\n-H &quot;Authorization: Bearer &lt;토큰&gt;&quot; \\\n-H &quot;X-GitHub-Api-Version: 2022-11-28&quot; \\\napi.github.com/repos/Lseoksee/lseoksee.github.io/actions/workflows\n\ncall 을 원하는 workflow id확인\n\n\n"},"index":{"title":"Seoksee Study","links":["개발-언어/","리눅스/","스터디/","시험-정리/","유용한-것들-모음/","CLI-프로그램-명령어/"],"tags":[],"content":"Hello 👋 I’m a college student majoring in computer science!\n탐색\n\n\n개발 언어\n\n\n리눅스\n\n\n스터디\n\n\n시험 정리\n\n\n유용한 것들 모음\n\n\nCLI 프로그램 명령어\n\n"},"개발-언어/C--and--C++/vcpkg--and--CMake":{"title":"vcpkg & CMake","links":[],"tags":[],"content":"\nC / C++ 용 패키지 관리자\n"},"개발-언어/CSS":{"title":"CSS","links":[],"tags":[],"content":"Flex 없이 요소 가운데 배치\nmargin: auto;\n\n가운데 배치하고 싶은 요소 에다 저렇게 주면은 자동으로 margin이 양쪽으로 균등하게 배치됨\n\ndiv 안에 이미지 박을때 해당 div에서 이미지 크기조정\n\n\nimg 테그 수정\nwidth: 100%;\nheight: 100%;\n\n\n상위 div 에서 크기 조정\nwidth: 400px;\n\n"},"개발-언어/HTML":{"title":"HTML","links":[],"tags":[],"content":"유니버설 링크 &amp; 앱 링크\n\n웹에서 안드로이드나, IOS 앱을 실행 시킬 수 있는 방법\n이거 말고도 딥 링크 라는 방법도 있음\n\nIOS (유니버설 링크)\n&lt;a href=&quot;example.com/open-app App&lt;/a&gt;\n\nexample.com: 이게 앱 경로\nuserId=12345: 데이터 전달\nIOS 에서는 HTTPS 가 필수임\nAssociated Domains 설정 필요\n\n안드로이드 (앱 링크)\n&lt;a href=&quot;intent://open?userId=12345#Intent;scheme=myapp;package=com.example.myapp;end&quot;&gt;Open App&lt;/a&gt;\n\nintent://: 안드로이드 앱 실행용 스킴\nscheme=myapp: 앱에 스킴(이름)\npackage: 앱에 패키지 명\nuserId=12345: intent 에 값 전달\n\ngetIntent().getData() 로 사용가능\n\n\nDigital Asset Links 설정 필요\n"},"개발-언어/JS--and--TS/JS--and--TS-문법":{"title":"JS & TS 문법","links":[],"tags":[],"content":"jsDoc\n배열 타입 명시\n/** @type {[string, int]} */\nconst [index1, index2] = 함수();\ntype 객체에 특정 객체의 타입으로 명시\n/** @type {Type[&#039;키값&#039;]} */\n===대충 이런느낌으로 타입이 선언되있는 경우===\ntype Type = {\n\t&quot;키값&quot;: {\n\t\t&quot;키1&quot;: string;\n\t\t&quot;키2&quot;: int;\t\n\t}\n} \nHTTPS 에서 HTTP 요청 불가\n\nNode.js 에서는 해당 안됨\n\n\n보안 문제로 HTTPS로 호스팅 하고있는 사이트에서는 HTTP API에 ajex 요청이 불가하다\nMixed Content 애러가 발생한다\n이는 WebSocket 도 마찬가지라 ws가 아닌 wss로 연결해야 한다\n"},"개발-언어/JS--and--TS/Node.js":{"title":"Node.js","links":[],"tags":[],"content":"EventEmitter\n\nNode.js 로 이벤트를 건내고 처리하는 객체\n\n이벤트 등록\nconst eventListener = new EventEmitter();\neventListener.on(&quot;이벤트명&quot;, (보낸값) =&gt; {\n\t// 콜백처리\n});\n \n이벤트 발생\nconst eventListener = new EventEmitter();\neventListener.emit(&quot;이벤트명&quot;, 보넬 값);\n당연하지만 등록 &amp; 발생 시 사용하는 EventEmitter 객체는 같은 걸 사용해야함\nEventEmitter 활용\nimport { EventEmitter } from &quot;events&quot;;\nimport mysql = require(&quot;mysql&quot;);\n \n// 이벤트 리스너 사용\nconst eventListener = new EventEmitter();\nlet connection: mysql.Connection | undefined;\n \nfunction CreateDBConnection(DBConfig: mysql.ConnectionConfig) {\n    const dbconect = mysql.createConnection(DBConfig);\n    dbconect.connect((err) =&gt; {\n        if (err) {\n            eventListener.emit(&quot;error&quot;, err);\n        } else {\n            connection = dbconect;\n            eventListener.emit(&quot;connected&quot;, dbconect);\n        }\n    });\n}\n \neventListener.on(&quot;connected&quot;, (dbconect: mysql.Connection) =&gt; {\n\tconsole.log(&quot;DB 연결 완료&quot;);\n});\n \n// Promise 와 결합하여 사용할때\nfunction getDBConnection(): Promise&lt;mysql.Connection&gt; {\n    return new Promise((resolve, reject) =&gt; {\n        if (connection) {\n            resolve(connection);\n        } else {\n            eventListener.on(&quot;connected&quot;, (dbconect: mysql.Connection) =&gt; {\n                resolve(dbconect);\n            });\n        }\n    });\n}\nCallback 또는 Promise로 반환되는 비동기 작업에 결과로 나온 변수를 전역적으로 사용하려고 할 때 사용하기 좋은 방법\n예를들어 어떤 비동기 작업이 해당 작업을 호출하지 않은 곳에서도 해당 작업이 완료됬는지 확인 할때 쓰기 좋음"},"개발-언어/Java":{"title":"Java","links":["스터디/문자-인코딩"],"tags":[],"content":"File I/O\n문자열 (String I/O)\n\n한 단어 또는 한 문자열 처리\n\n\n문자 하나하나 또는 문자열을 설정된 인코딩에 맞게 변환해서 문자열 다룸\n\nSupplementary Planes 에 해당하는 문자열은 char[]를 하나 더 읽어야 됨\n확인법: Character.isSupplementaryCodePoint(codePoint)\n\n\n기본 Stream에 문자 인코딩 처리를 위한 인코더가 추가된 거라 당연하게도\n바이너리를 해당 객체들로 다루면 데이터 깨짐\n자바에서 문자열을 저장하는 방식\n\n문자열을 UTF-16 으로 인코딩 하고 있다가 따로 참조할일 있을때 디코딩 해서 표현\n\n\n\nReader &amp; Writer\n\n기본 String I/O 객체\n\nBufferedReader &amp; BufferedWriter\n\n메모리 버퍼를 사용하는 String I/O 객체\n\nFileReader &amp; FileWriter\n\n파일 입출력 String I/O 객체\n\nInputStreamReader &amp; OutputStreamWriter\n\nbinry I/O 스트림을 String I/O 로 변환 해주는 객체\n\n\n아래 바이너리 항목처럼 Stream 내 데이터가 문자열인 경우 binry I/O 로 문자열을 다루게 되면 비효율적이므로\n이걸 String I/O로 변환 시켜 줌\n\n바이너리 (binry I/O)\n\n데이터를 1바이트 단위로 처리함\n\n\n해당 객체들로 텍스트 문자도 처리가 가능하지만\n추가적인 인코딩 처리가 필요하고, 비효율적임\n인코딩 마다 각각 할당되는 바이트 수가 달라서 1바이트 읽고 그걸 문자열로 인코딩 한다면 당연하게도 문자가 깨짐\n그렇기에 모든 바이트 읽고 그걸 인코딩 해서 전체 바이트를 해석하게 해야 함\n\nInputStream &amp; OutputStream\n\n기본 binry I/O 객체\n\nBufferedInputStream &amp; BufferedOutputStream\n\n메모리 버퍼를 사용하는 binry I/O 객체\n\nFileInputStream &amp; FileOutputStream\n\n파일 입출력 binry I/O 객체\n\n\n텍스트 파일이 아닌 모든건 이걸로 해야함\n\nStream에서 문자열 다루기\n\n\n잘못된 예\nInputStream stream = new FileInputStream(&quot;test.txt&quot;);\nbyte buffer[] = new byte[1024];\n \nint head = 0;\nwhile ((head = stream.read(buffer)) &gt; 0) {\n\tSystem.out.println(new String(buffer, 0, head));\n}\n\n1024 바이트 씩 끊어서 읽는데 만약 1023바이트에 해당 하는 문자열이 3바이트에 걸쳐서 해석되야 하는경우\n중간에 바이트가 끊기기 떄문에 문자열이 깨짐\n\n\n\n옳은 예\nInputStream stream = new FileInputStream(&quot;test.txt&quot;);\nbyte buffer[] = new byte[1024];\nByteArrayOutputStream bos = new ByteArrayOutputStream();\n \n \nint head = 0;\nwhile ((head = stream.read(buffer)) &gt; 0) {\n\tbos.write(buffer, 0, head);\n}\n \nbos.flush();\nSystem.out.println(bos.toString());\n\n이렇게 전체 바이트를 받고 그걸 String 으로 변환하여 써야함\n\n\n\nBuffered 의 장/단점\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n구분Non-BufferedBuffered최고효율작은크기 &amp; 적은빈도 접근큰크기 &amp; 반복적 접근결론작은 데이터나 한 번에 데이터를 다루는 경우 적합큰 데이터 파일, 반복적으로 데이터를 다루는 경우 적합\nStream API\n\ncollection 객체들에서 람다를 사용하여 제어 할 수있는\n\n\nJS 같이 filter 나 map, foreach 함수들을 람다 표현으로 사용가능\n\n진수 표현 (n진수 리터럴 표기법)\n\n\n2진수\nint a=0b10;\n\n\n16진수\nint b=0x10\n\n\nJava 에서의 바이트 표현\n\n예를들어 0xFF 즉 1111 1111 은 10진수로 표현하면 255\n\nunsigned byte 로 표현 할 수 있는 최대값\n\n\n하지만 byte 자료형은 부호가 있는 2의 보수로 표현되기에 -128 ~ 127  까지의 숫자만 저장 가능하다\n\nbyte num = (byte) 0xFF;\n// 값은 -1\n\nunsigned byte 자료를 다룰땐 byte 로 형변환 하여 2의 보수법 으로 해석 하게한다\n실제로 File 이나 기타 바이너리 자료를 다룰 때 byte[] 단위로 저장될텐데 각 배열 값을 바이트 값을 2의 보수법으로 해석 하게 된다, 그래서 음수가 나올 수 있는것\n\nString 저장방식\n\ncl8d.tistory.com/39\n\n업케스팅 &amp; 다운케스팅\n동적 바인딩\n\n부모로 부터 Overriding된 메소드는 부모 메소드는 무시되고 자식 메소드가 호출된다\n\n// 슈퍼클래스\nclass Animal {\n    public void sound() {\n        System.out.println(&quot;Animal makes a sound&quot;);\n    }\n}\n \nclass Cat extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(&quot;Cat meows&quot;);\n    }\n}\n \npublic class Main {\n    public static void main(String[] args) {\n        Cat mycat = new Cat();\n        mycat.sound();\n\t    // 자식 메소드에 구현된 sound() 가 수행됨\n    }\n}\n업케스팅 (Upcasting)\n// 슈퍼클래스\nclass Animal {\n    public void sound() {\n        System.out.println(&quot;Animal makes a sound&quot;);\n    }\n}\n \n// 서브클래스\nclass Dog extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(&quot;Dog barks&quot;);\n    }\n}\n \nclass Cat extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(&quot;Cat meows&quot;);\n    }\n}\n \npublic class Main {\n    public static void main(String[] args) {\n        Animal myDog = new Dog();\n        Animal myCat = new Cat(); \n        \n        myDog.sound();\n        myCat.sound();\n        \n        Animal[] animals = { new Dog(), new Cat() };\n        for (Animal animal : animals) {\n            animal.sound();\n        }\n    }\n}\n\n자식 클래스를 부모 객체로 초기화 하는 방법\n이렇게 초기화 하더라도 동적바인딩이 작동하여 자식에 구현된 메소드만 호출한다\n\n사용하는 이유?\n\n\n가장 큰 의문이 그냥 자식 그 자체로 객체를 초기화 하더라도 동적 바인딩 같은 동작에는 아무런 문제가 없다.\n\n\n그래서 업케스팅이 필요한 이유를 가장 잘 설명한 게 아래 예제 이다\n// 핵심 메소드\npublic void makeAnimalSound(Animal animal) {\n    animal.sound();\n}\n \nAnimal dog = new Dog();\nAnimal cat = new Cat();\n \nmakeAnimalSound(dog);  // Dog의 sound() 호출\nmakeAnimalSound(cat);  // Cat의 sound() 호출\n\nDog 와 Cat 둘 다  Animal 상속 받은 상태라 내가 설계하는 메소드는\nCat, Dog 와 같이 Animal을 상속받은 모든 객체면 상관없는 상황이라고 가정한다\n그럴때는 위 makeAnimalSound() 메소드 처럼 Animal 이라는 타입으로\n뭉뚱그려서 타입을 명시 할 수 있기 때문이다.\n그래서 업케스팅은 그냥 하위 객체 전체의 타입을 명시하기 위한 수단 정도로만 기억하자\n\n\n\n다운 케스팅 (DownCasting)\n// 슈퍼클래스\nclass Animal {\n    public void sound() {\n        System.out.println(&quot;Animal makes a sound&quot;);\n    }\n}\n \n// 서브클래스\nclass Dog extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(&quot;Dog barks&quot;);\n    }\n\tpublic void bark() {\n        System.out.println(&quot;이건 나한테만 있음&quot;);\n    }\n}\n \n// 핵심 메소드\npublic void makeAnimalSound(Animal animal) {\n\tanimal.sound();\n \n\t// 만약 Dog 객체면\n\tif (animal instanceof Dog) {\n\t    Dog dog = (Dog) animal;  // 다운캐스팅\n\t    dog.bark();  // Dog의 고유 메소드 사용 가능\n\t}\n}\n\n위 예제 처럼 makeAnimalSound() 에서 Animal 타입에 공통 함수인 sound()를 수행하고\n각각의 하위 클래스에 구현된 고유한 메소드나, 변수를 사용하기 위해\n완전히 자식 클래스로 전환 하는 것\n저렇게 뭉뚱그려서 Animal 로 타입을 지정한 경우 호출할때 사용되는\n객체 각각에 대해 따로 처리할 때 사용한다\n"},"개발-언어/Python/FastAPI":{"title":"FastAPI","links":[],"tags":[],"content":"바이너리 Response\n \n#이제 이 버퍼에다 바이너리 값을 wirte 하면 됨\nbuffer = io.BytesIO()\n# 대충 이런느낌으로 파일 인자에 buffer 를 줘서 쓰기도 함\nopen(buffer, &quot;w&quot;)\n \n# buffer.getvalue(): bytes\n# bytes 타입으로 넣어주면됨\nreturn Response(buffer.getvalue(), media_type=&quot;image/png&quot; )\n\nbytes 타입으로 Response에 넣어줌\n\nuvicorn-CLI 패키지 경로 문제 해결\n\n\n이게 프로젝트 경로가 루트로 되어있어서 실행서버 코드가 루트에 없으면 패키지 경로가 달라져서 그렇다\n# --app-dir설정하면 처음 서버 코드지정은 패키지 없이\nuvicorn Server:server --app-dir &lt;서버코드경로&gt;\n\n\nuvicorn log 옵션 원본\nLOGGING_CONFIG: = {\n    &quot;version&quot;: 1,\n    &quot;disable_existing_loggers&quot;: False,\n    &quot;formatters&quot;: {\n        &quot;default&quot;: {\n            &quot;()&quot;: &quot;uvicorn.logging.DefaultFormatter&quot;,\n            &quot;fmt&quot;: &quot;%(levelprefix)s %(message)s&quot;,\n            &quot;use_colors&quot;: None,\n        },\n        &quot;access&quot;: {\n            &quot;()&quot;: &quot;uvicorn.logging.AccessFormatter&quot;,\n            &quot;fmt&quot;: &#039;%(levelprefix)s %(client_addr)s - &quot;%(request_line)s&quot; %(status_code)s&#039;,\n        },\n    },\n    &quot;handlers&quot;: {\n        &quot;default&quot;: {\n            &quot;formatter&quot;: &quot;default&quot;,\n            &quot;class&quot;: &quot;logging.StreamHandler&quot;,\n            &quot;stream&quot;: &quot;ext://sys.stderr&quot;,\n        },\n        &quot;access&quot;: {\n            &quot;formatter&quot;: &quot;access&quot;,\n            &quot;class&quot;: &quot;logging.StreamHandler&quot;,\n            &quot;stream&quot;: &quot;ext://sys.stdout&quot;,\n        },\n    },\n    &quot;loggers&quot;: {\n        &quot;uvicorn&quot;: {&quot;handlers&quot;: [&quot;default&quot;], &quot;level&quot;: &quot;INFO&quot;, &quot;propagate&quot;: False},\n        &quot;uvicorn.error&quot;: {&quot;level&quot;: &quot;INFO&quot;},\n        &quot;uvicorn.access&quot;: {&quot;handlers&quot;: [&quot;access&quot;], &quot;level&quot;: &quot;INFO&quot;, &quot;propagate&quot;: False},\n    },\n}\nURI 리버스 프록시 구성 시 swagger 경로 문제\n\n왜인지 모르겠지만 swagger 로드에 필요한 openapi.json 경로가 무조건 / 이걸로 되어있어\n최상위 URI 가 / 아닌경우 못 불러온다\n\n해결 방법\n\n\n서버 구성 시 root_path 설정\nserver = FastAPI(root_path=&quot;/리버스 프록시와 동일한 경로&quot;)\n\nuvicorn 명령에서 수정할수도 있긴한데 redirct 발생 시 재대로 경로를 못찾아가는 문제가 있다\n다행히도 / 경로에서도 api가 문제없이 작동한다 즉 / 경로나, root_path 설정한 경로 둘 다 문제 없이 작동한다\n\n\n"},"개발-언어/Python/PyTorch":{"title":"PyTorch","links":[],"tags":[],"content":"연산 장치 선택 (CPU, CUDA 등)\n\nPyTorch로 GPU나 NPU 등을 사용 하려는 연산\n실제 CUDA나 이런 장치를 사용하려면 무조건 이 방식으로 해야한다\n\n장치 확인\nif torch.cuda.is_available():\n\tprint(&quot;CUDA 사용 가능&quot;)\nelse:\n\tprint(&quot;CUDA 사용 불가능&quot;)\n실제 연산\n# GPU에 경우 cuda()\na = torch.tensor([1.0, 2.0]).cuda()\nb = torch.tensor([3.0, 4.0]).cuda()\n \nc = a + b\n\ncuda() 를 호출하면 해당 Tensor가 GPU메모리에 복사가 된다\n사실상 torch 객체에 어디든 이런식으로 사용이 가능하다\n거의 웬만한 데에서 cuda() 이런식으로 뒤에 호출 해주는 편이 좋다\n\n다른 장치와 혼합 불가\na = torch.tensor([1.0, 2.0]).cuda()\nb = torch.tensor([3.0, 4.0])  # CPU Tensor\n \n# 오류 발생: RuntimeError: Expected all tensors to be on the same device\nc = a + b\n장치를 자동 선택\n\n\n위 방식에 가장 큰 문제는 모든 코드를 cuda()로 도배 해버리면 cpu만 사용 가능한 상황에서 문제가 발생한다\n\n\n따라서 to(str: device) 함수를 통해 변수값을 사용하여 장치변환 함수를 선택하자\nif torch.cuda.is_available():\n\tdevice = &quot;cuda&quot;\nelse:\n\tdevice = &quot;cpu&quot;\n \na = torch.tensor([1.0, 2.0]).to(device)\n\n"},"개발-언어/Python/Python":{"title":"Python","links":[],"tags":[],"content":"Magic Method\n\n__ 로 시작해서 끝나는 기본 클래스 정의 메소드\n이것 이외의 인덱스 관련함수 목록\n\n__getitem__(index)\ndef __getitem__(self, index):\n\tpass\n# index는 [] 안에 값이\n\n객체의 [] 연산자를 사용하여 조회 할 때 작동하는 메소드\nnumpy의 [3,4] 이런 슬라이싱 방식이 이걸로 구현되어있다\nlist[10] == list.__getitem__(10)\n\n__setitem__(index, key)\ndef __setitem__(self, index, key):\n\tpass\n# index는 [] 안에 값, key는 대입 값\n\n객체의 [] 연산자를 사용해서 변수를 지정할 때\nlist[10] = 1 == list.__setitem__(10, 1)\n\n__str__()\n\n객체의 직접 print 를 줄시 나오는 출력 구문 커스텀\n\n__call__()\n\n객체를 함수처럼 호출할 수 있게 해주는 메소드\n\nobj = Object() \nobj()\n# __call__() 함수가 호출된다\nenumerate\nfor index, item in enumerate(arr):\n\n리스트 행목들로 for 돌릴시 인덱스도 같이 얻는 객체\n\ntyping\nLiteral\na: Literal[&quot;이거&quot;, &quot;저거&quot;] = &quot;이거&quot;\n\n타입을 “이거” “저거”로 만 명시 하는\n특정 Literal 따라 타입이 동적으로 변해야 하는경우는 명시 불가\n\n딕셔너리 안에 딕셔너리 typing\n딕셔너리 = {\n\t&quot;키1&quot;: dict[Literal[&quot;항목1&quot;, &quot;항목2&quot;], int](\n\t\t{\n\t\t\t&quot;항목1&quot;: 1,\n\t\t\t&quot;항목2&quot;: 2,\n\t\t}\n\t),\n}\n\n이때는 dict 의 생성자를 사용해 typing 가능하다\n그러면 딕셔너리[&quot;키1&quot;][&quot;항목1&quot;] 까지 typing 된다\n\nSequence\n\n이건 list 와 tuple 같은 열거형 자료형을 의미\n\na: Sequence[int] = [1, 2]\nGIL (Global Interpreter Lock) 문제\n\nPython에 제일 큰 문제는 1코어 1쓰레드로 밖에 동작하지 못한다\n\n\n만약 threading API로 여러 쓰레드로 작업한다고 해도\nGIL 땜에 사실상 동시에 실행된다는 전제는 아니다\n다만 완전하게 의미는 없는게 아닌게 I/O 작업같이 Windows API 같은\nOS API 의 의존하거나, 라이브러리 자체가 C, C++로 구현되있는 경우\nGIL이 관여하지 않으므로 멀티쓰레딩 사용이 가능하다\n하지만 순수 Python 코드로 되어있는 CPU 연산작업에는 별로 큰 의미가 없어진다\n\n이떄는 사실상 비동기와 큰 차이가 없어지는 것이다.\n단지 구조적인 차이와, 비동기 관리를 언어 수준에서 하냐, OS 수준에서 하냐 정도\n\n\nPython 3.13 이후에는 이게 선택으로 바뀐다는데 아직까지 제대로된 정보를 잘 모르겠다\n\nmultiprocessing\n\n파이썬에서 멀티프로세싱을 수행하는 객체\n\nfrom multiprocessing import Pool\n \ndef f(x):\n    return x * x\n \nif __name__ == &#039;__main__&#039;:\n    with Pool(5) as p:\n        print(p.map(f, [1, 2, 3, 4, 5]))\n\nPython 에서는 이런 GIL 문제를 우회하고자 멀티프로세싱을 많이 쓰는거 같다\n다만 Frozen Start 문제 때문에 별로 오래 안걸리는 작업에는 멀티쓰레딩이 몇배는더 빠르다\n\n설명\n\nf() 함수를 각각 1, 2, 3, 4, 5의 인자를 전달받아 연산한다는 개념\n만약 Pool 을 3으로 변경하면 3개의 프로세스를 1, 2, 3에 할당하여 연산 시키고, 가장 빨리 끝난 순으로 나머지 4, 5를 연산\n\n멀티프로세싱의 특징\n\n현재 프로그램을 실행하는 프로세스와 완전히 다른 프로세스를 생성하는것\n독립된 Memory-Map 과, 자원을 할당받음\n같은 Memory-Map을 쓰지 않으므로 각각의 프로세스 간 통신은 IPC로 이루어짐\n\n리스트 컴프리션 (list comprehension)\nx2 = [ n*2 for n in x ]\nx2\n# 연산 for 변수 in 참조값 \nAsterisk\n\nPython 에서의 *\n\nUnpacking (*)\n\n일반 코드에서의 사용\n\nlist_data = [1,2,3,4,5]\n \nprint(*list_data)\n-&gt; 1 2 3 4 5\n\n마치 JS 의 ... 연산자 처럼 리스트나, 튜플을 풀어주는 역할을한다\n\ndef fuc(a: int, b: int, c: int):\n\tpass\n \nlist_data = [1,2,3]\n# 또는 (1,2,3)\n \nfuc(*list_data)\n\n이렇게 여러개의 인자를 전달해야 하는경우 list나 tuple을 만들어서 전달도 가능해진다\n\ndict_data = {1: &quot;name&quot;, 2: &quot;age&quot;, 3: &quot;height&quot;, 4: &quot;weight&quot;}\nprint(*dict_data)\n-&gt; 1 2 3 4\n\n딕셔너리의 경우 키값이 Unpacking 된다\n\npositional argument (*)\n\n함수에서 사용\n\ndef test(*args):\n    print(args)\n \ntest(1 ,2, &quot;하이&quot;)\n-&gt; (1, 2, &quot;하이&quot;)\n\n함수 인자를 튜플 형태로 받는다\n아래 keyword arguements 와 다르게 키값이 필요하지 않아 그냥 무작위 값을 마음데로 넣을 수 있다\n모호성 문제로 positional argument을 두번 쓰는건 허용하지 않는다\n\nkeyword arguements (**)\n\n함수에서 사용\n\ndef test(tes, **kwargs):\n    print(kwargs[&quot;key&quot;])\n \ntest(1 ,key=10)\n-&gt; {&#039;key&#039;: 10}\n\n함수 인자를 동적 딕셔너리로 받는다\n함수 호출 시 kwargs 에는 반드시 key-value 형태로 전달해야 함\n모호성 문제로 keyword arguements을 두번 쓰는건 허용하지 않는다\n\nsubprocess\nsubprocess.run\n\n명령어를 동기적으로 실행\n\ntry:\n# check=True로 설정하여 실패한 프로세스를 감지\nresult = subprocess.run(\n    [&quot;bash&quot; &quot;-c&quot;, &quot;echo&quot;, &quot;테스트&quot; ], \n    check=True, \n    capture_output=True, \n    text=True\n)\nexcept subprocess.CalledProcessError as e:\n    # 실패 시 예외 처리\n\tprint(&quot;예외발생&quot;)\nsubprocess.Popen\n\n명령어를 비동기로 실행\n\n# 명령을 비동기로 실행\nsubprocess.Popen(\n\t[&quot;bash&quot; &quot;-c&quot;, &quot;echo&quot;, &quot;테스트&quot; ],\n\tstdout=subprocess.PIPE,\n\tstderr=subprocess.PIPE,\n\ttext=True,\n)\n \n# 비동기 실행 대기\nstderr, stdout = pr.communicate()\n \n# 예외처리\nif pr.returncode != 0:\n\tprint(&quot;실패&quot;)\nargparse\n\n파이썬 실행 인자 주기\n\nparser.add_argument(&#039;--args&#039;, required=True, help=&#039;설명&#039;)\n \n# cli에서 단순 옵션 활성화 시\nparser.add_argument(&#039;--enable&#039;, required=True, action=&quot;store_true&quot;)\nargs = parser.parse_args()\n \n# 아예 변수명이 - 땐 model 이게 된다 \nprint(args.model)\n# 해당 인자가 있으면(설정되있으면) True, 없으면 False\nprint(args.enable)\nCLI 사용\npython train/cardDict_train.py --args 값입력\n패키지 구조\n\nPython 에서는 각 패키지 간 import가 실행시킨 홈 기준이다\n\n설명\n\nmain.py를 실행 파일이라고 가정\n\nlib:\n\tlib.py\nsrc:\n\tsub.py\n\tsub2.py\nmain.py\ntest.py\n\n\n\nsub.py가 lib.py 를 import 하고 싶다면 이렇게 해야한다\nimport src.sub.py\n\n\n이런 원리로 같은 패키지더라도 단순 import 가 안된다\n#sub.py\nimport sub2 #이거 안됨\n \n===올바른 예제===\nimport src.sub2\n\n\n대신  sub.py 가 test.py 를 import 할 때는 가능하다\n# sub.py\nimport test\n\n\nimport 위치를 해결하는 방법\n\n\nsytem path를 추가하자\nimport sys\nsys.path.append(&quot;./바꿀경로&quot;)\n\n\n예외처리\n예외 발생시키기\nraise Exception(&quot;예외 발생ㅎㅎ&quot;)\nbool 타입 변환 문제\n\n\npython 에서 string bool 문자열(false, true)을 실제 bool 타입으로 변환 하려면 아래 코드를 쓸 수 있다\nbool(&quot;true&quot;)\n\n\n근데문제는 false 문자열도 True로 반환한다\n\n\n이게 빈 문자열 이거나, 0 이거나 할때 false만 false로 반환된다\n\n\n그래서 bool 문자열은 a==&quot;true&quot; b==&quot;false&quot; 이런식으로 처리하자\n\n"},"개발-언어/Shell-Script":{"title":"Shell Script","links":[],"tags":[],"content":"#!/bin/bash\n\nShell Script 를 처음 작성할때 Shell(해시뱅)을 지정해야하는게 원칙이다\n\n기초 문법\n변수\na=&quot;하이&quot;\necho &quot;$a&quot;\n \n# 명령에 출력 결과를 변수로 받는 방법\nres=&quot;$(ls -a)&quot;\n\n변수 선언 시 공백이 발생하면 안됨\n변수 사용 시 숫자 포함하여 무조건  &quot;&quot; 붙여야 함\n\n조건문\nif\nif [ &quot;조건1&quot;==&quot;값&quot; ]; then\n\t명령어\n# if not\nelif ! [ &quot;조건2&quot;==&quot;값&quot; ]; then\n\t명령어\nelse\n\t명령어\nfi\n\n\n파일/디렉토리 존재여부\nif [ -e &quot;파일경로&quot; ]; then\n\techo &quot;파일이 존재함&quot;\nfi\n \nif [ -d &quot;디렉토리 경로&quot; ]; then\n\techo &quot;디렉토리가 존재함&quot;\nfi\n\n\n빈 문자열 확인\nif [ -z &quot;문자열&quot; ]; then\n\techo &quot;문자가 비어있음&quot;\nfi\n\n\n숫자 비교\nif [ 10 -gt 5 ]; then\n\techo &quot;5보다 큼&quot;\nfi\n\n일반적인 연산자가 작동은 하긴 하는데 이상함 그래서 아래 숫자 비교 연산자 에서 참고해서 쓸 것\n\n\n\n문자열 패턴 처리 (Bash 에서)\nstr=&quot;omg hello&quot;\nif [[ &quot;$str&quot; == *&quot;hello&quot;* ]]; then\n    echo &quot;hello 라는 문자열이 포함&quot;\nfi\n\n\n논리 연산 (다중 조건)\n# or\nif [ 조건1 ] || [ 조건2 ]; then\n 명령어\nfi\n \n# and\nif [ 조건1 ] &amp;&amp; [ 조건2 ]; then\n 명령어\nfi\n\n\nswitch-case\ncase &quot;$변수&quot; in\n\t&quot;값1&quot;)\n\t\t명령어\n\t\t;;\n\t&quot;값2&quot;|&quot;값3&quot;)\n\t\t명령어\n\t\t;;\n\t# default (기본 명령 지정 안할 꺼면 없에도됨)\n\t*)\n\t\t명령어\n\t\t;;\nesac\n숫자 비교 연산자\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n비교 연산자의미비교 연산자-eq같음==-ne같지 않음!=-gt보다 큼&gt;-ge보다 크거나 같음&gt;=-lt보다 작음&lt;-le보다 작거나 같음⇐\n반복문\nfor\n===python 스타일===\nfor var in {&lt;시작값&gt;..&lt;종료값&gt;..&lt;증감값&gt;}\ndo\n  echo $var\ndone\n \n===C계열 스타일===\nfor ((var=0; var &lt; 5; var++));\ndo\n  echo $var\ndone\n \n#무한 루프\nfor (( ; ; ));\ndo\n\t  echo &quot;Hello World&quot;\ndone\n\n\n배열 항목 처리\narray=(&quot;항목1&quot; &quot;항목2&quot;)\nstring_array=&quot;항목1 항목2&quot;\n \nfor item in &quot;$array&quot;\ndo\n   echo $item\ndone\n\nshell 에서 for 문에 항목은 문자열 띄어쓰기로도 구분 가능하다 한다 &quot;값1 값2&quot;\n\n\n\nwhile\nwhile [ 조건 ]\ndo\n\t명령어\ndone\n \n# 무한 루프\nwhile : \ndo\n\t명령어\ndone\n기초 연산\n\n근본은 expr &lt;연산&gt; 이지만 아래 방식이 더 편함\n\n\n논리 연산 같은 true, false 형태의 값은 1, 0 으로 취급\n\n# 덧셈\n$((a+b))\n# 뺄샘\n$((a-b))\n# 나눗셈\n$((a/b))\n# 곱셈\n$((a*b))\n# 나머지\n$((a%b))\n# 거듭제곱\n$((a**b))\n# 비교\n$((a&gt;b))\n# 논리\n$((a||b))\n# 증감\n$((a++))\n$((a--))\n# 복합 할당\n$((a+=1))\n표준 입력\nread a\necho &quot;$a&quot;\n실행인자 처리 (args)\n# 각각 따로처리\necho &quot;1번인자: $1&quot;\necho &quot;2번인자: $2&quot;\necho &quot;3번인자: $3&quot;\n \n# 모든 인자를 배열로 취급\nargs=(&quot;$@&quot;)\necho &quot;${args[0]}&quot;\n \n# 가장 마지막 인자 선택\nargs=${!#}\necho &quot;$args&quot;\n배열\n# 배열은 () 내부에 값을 띄어쓰기 형태로 구분하여 생성한다\narray=(&quot;foo&quot; &quot;bar&quot; &quot;foobar&quot;)\n \n# 배열에 값 추가\narray+=(&quot;test&quot;)\n \n# 아무것도 인덱스를 지정하지 않으면 0번 인덱스\necho &quot;$array&quot;\n \n# python 처럼 음수 인덱싱이 가능함 \necho &quot;${array[-1]}&quot;\n \n# 모든 베열 원소 나열\necho &quot;${array[@]}&quot;\n사용 팁\n자체 표준 입력 프로그램 처리\nmysql -u [유저] --password=[패스워드] -h [주소] &lt;&lt; EOF\nuse mars;\nselect *from User;\nEOF\n\nMysql 처럼 cli를 통해 해당 자체 콘솔에 접속되는 경우 &lt;&lt; EOF 구문을 통해 해당 콘솔에 명령을 보낼 수 있다\n\nShell 명령 결과를 echo 로 출력\necho $(ls -a)\n쉘 에서 실행한 프로그램 제어\n#nohup 으로 프로그램 실행\nCOMMAND=&quot;nohup your_command &amp;&quot;\n \n# nohup으로 실행된 프로그램 PID 값 얻기 \nCMD_PID=$!\n\n$! 변수를 사용하여 nohup을 통해 실행한 프로그램 PID 값을 얻을 수 있다\n이를통해 특정 작업 이후 프로세스를 kill 하거나 하는게 가능하다\n\nnohup을 수행하면 콘솔이 대기상태에 들어가는 느낌이 들 수 있는데\n사실 &amp;으로 인해 그렇게 보이는거지 거기가 exit같은걸 입력하면 잘 입력된다\n\n\n근데 npm을 통해 명령 수행하면 npm 자체의 pid값은 얻을 수 있지만 npm이 수행한 pid 값을 얻을 수는 없는 듯 하다\n\n예약 문자열\n\n\nbash -c:\n\n일부 프로그램들에서 shell script 가 사용가능한 경우 여러 명령어를 입력하기 위해 사용\n\ntest bash -c &quot;커맨드1 &amp;&amp; 커맨드2&quot;\n\n\n\\: 문장 끝에 붙이면 개행을 무시하도록 하는\ntest a \\\nb\n \n#이거와 같다\n==\ntest a b\n==\n\n\n&amp;&amp;: 앞에 명령에서 실패 종료코드가 반환되지 않는경우 다음 명령을 수행\n# apt update가 정상적으로 종료되면 apt install -y를 수행\napt update &amp;&amp; apt install -y\n\n\n;: 앞에 명령에 종료 코드와 상관 없이 다음 명령을 수행\n# apt update가 정상적으로 종료되지 않아도 apt install -y를 수행\napt update; apt install -y\n \n# command1이 정상적인 종료 코드면 성공 아니면 실패\ncommand1 &amp;&amp; echo &quot;성공&quot; || echo &quot;실패&quot;\n\n\n실행 프로그램에만 환경변수 설정하기\n키=값 &lt;실행프로그램&gt;\n\n당연하지만 1회성으로 작동한다\n\n현재 쉘에서만 환경변수 설정하기\n\n쉘 끄면 그때 사라짐\n\nexport 키=값\nglobbing 문제\n\n\n스크립트를 사용할때 파일을 선택하는 특수 문자열이 있다. 이것들을 globbing pattern 이라고 한다\n\n*, ?, [] 등\n\n\n\n해당 문자열에 대한 처리가 실행하는 명령어나, 프로그램이 처리하는게 아니라 Bash 같은 쉘이 이걸 직접 처리하게 된다.\n\n\n예를들어 * 을 인자로 넘겨준 경우 쉘이 이걸 변환해서 해당 폴더에 모든 파일들을 나열하게된다.\n\n\n그레서 globbing pattern에 해당하는 문자열은 실행인자로 처리가 불가하다\n\n\n아래 명령어를 실행해보면 이해가 잘 될것이다\necho *\n\n\n유용한 명령어\n\n\n파일 절대경로 반환\nrealpath &quot;상대 경로&quot;\n\n\n문자열 치환\ntr &lt;기존 문자열&gt; &lt;바꿀 문자열&gt;\n\n\n출력 형식을 지정하여 문자열 출력\nprintf &quot;%d+%d=%d&quot; 10 20 30\n\n"},"리눅스/Vim-사용법":{"title":"Vim 사용법","links":[],"tags":[],"content":"\n\n                  \n                  참고자료 \n                  \n                \n\n\nVim 단축키 정리\n\n\n\n\nesc: 명령모드로 전환\ninsert: 입력모드로 전환\n\no: 다음 행 에서 입력모드 전환\n\n\n\nvim 설정\n\nvim ~/.vimrc\n\n\n\n행 번호 표시\nset number\n\n\n텝 크기 설정 (기본값: 8)\nset tabstop=4\n\n\n명령 라인 모드 명령어\n\n명령모드로 전환 후 shift + : 하여 명령어 입력 모드로 전환 가능\n이제 하단 명령어가 입력 가능하다\n\n\n변경사항 저장:  w!\n변경사항 저장하고 나가기:  wq! &lt;파일명 | null&gt;\n\n단축키: shift+z+z\n\n\nvim 종료: q!\n\n단축키: shift+z+q\n\n\n\n파일 수정 명령어\n\n모든라인 삭제: %d\n\n파일 조회 명령어\n\n해당 줄로 이동: &lt;행번호&gt;\n단어 찾기: /&lt;검색어&gt;\n\n명령 모드 단축키\n\n주의 할께 한/영 대, 소문자 전부 따진다.\nshift 들어가는 명령어는 원래 대문자로 입력하는 단축키인데 편의상 shift+&lt;&gt; 로 표기, 따라서 shift 단축키는 CapsLock 을 써도 가능하다\n\n파일 수정 단축키\n\n되돌리기: u\n다시 실행:  ctrl+r\n복사: y\n잘라내기: d\n붙어넣기: p\n현재 줄 삭제:  dd\n현제 커서 문자 삭제: x | del\n커서 뒤 행 삭제: shift + d\n소괄호 () 안에있는 문장 삭제: dib\n중괄호 {} 안에있는 문장 삭제: di + shift+b\n\n여러번 복사 후 한번에 붙어넣기\n\n레지스터 사용\n\n\n처음 줄 레지스터 할당 (처음으로 복사될꺼 지정): &quot;ayy\n다른 줄 레지스터에 복사: &quot;ay\n모두 붙여넣기: &quot;ap\n\n편집기 제어\n\n\n문서의 빈 공간 상/하 이동: shift+ } &amp; shift+ {\n\n\n한 줄씩 아래로 스크롤 : ctrl + e\n\n\n한 줄씩 위로 스크롤: ctrl + y\n\n\n현재 페이지 중간쯤 아래로 스크롤: ctrl+d\n\n\n현재 페이지 중간쯤 위로 스크롤: ctrl+u\n\n\n페이지 맨 위로 커서 이동: gg\n\n\n페이지 아래로 커서 이동: shift + G\n\n\n다음 단어로 이동: w\n\n\n이전 단어로 이동: b\n\n\n파일 조회 단축키\n\n범위 선택: v &amp; 커서 이동 (또는 w로 다음단어)\n"},"리눅스/단축키":{"title":"단축키","links":[],"tags":[],"content":"Putty 복사 &amp; 붙여넣기\n\n복사: shift+Ins\n붙여넣기: ctrl+Ins\n"},"리눅스/리눅스-개념-정리":{"title":"리눅스 개념 정리","links":[],"tags":[],"content":"리눅스 폴더 구조\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n리눅스 디렉토리 구조 한눈에 정리\n\n\n\nTTY (Teletypewriter)\n\n터미널과 함께 콘솔의 한 종류\n\n\n\n리눅스 등에서 이런 화면을 보통은 TTY 라고 생각하면 된다\nseoksee@ubuntu:~$\n\nDocker에서 -t 옵션을 사용하지 않고 -t를 사용한 경우 해당 화면이\n출력되지 않고 그냥 표준 입/출력만 가능하다\n\n\n\n만약 Putty여러개로 한 클라이언트를 접속한다고 치자\n그럼 각각의 TTY 가 할당 되게 된다\n즉, 하나의 클라이언트에서 콘솔 여러개를 띄울 수 있는게 이 TTY 의 존재 덕분이다\n# 자신 tty 번호확인 명령\ntty\n\n\nPTY (Pseudo-TTY)\n\nSSH 이용시에서는 TTY 가 아니라 가상 터미널인 PTY 가 사용됨\n"},"리눅스/리눅스-명령어-정리":{"title":"리눅스 명령어 정리","links":["스터디/네트워크-통신"],"tags":[],"content":"1024 미만 포트 허용하기\n\nroot 계정이 아닌경우 기본적으로 막혀있긴함\n그래서 root로 실행하거나 이 방법 쓰거나\n\n\nsudo setcap ‘cap_net_bind_service=+ep’ [실행파일]\n\n실행파일은 node 면 which node 위치\n\n\n\n백그라운드\n단순히 백그라운드 전환\n\n만약 포그라운드에서 급작스럽게 백그라운드 작업을 돌려야 하는 경우\n다만 이때는 로그를 보거나, 포그라운드 전환이 안됨\n\n\nCtrl+Z\n작업 일시 정지\n(이때 나오는 숫자가 작업 번호임)\nbg &lt;작업번호&gt;\n작업 백그라운드 전환\nfg &lt;백그라운드 번호&gt;\n포그라운드 전환\n(다만 이전 로그는 못봄)\n스크립트 사용시\n\n&lt;작업&gt; &amp;\n\n\n\nscreen\n\nnohup에 경우에 백그라운드 실행시 다시 포그라운드 전환이 불가 하다\n따라서 이 방법으로 하는 게 좋다\n\n# 스크린 만들기\nscreen -S [스크린 이름] \n \n# 같은 이름 스크린 있으면 접속하고 없으면 만들기\nscreen -R [스크린 이름]\n \n# 스크린 목록출력\nscreen -ls (.뒤에 있는게 스크린 이름이다)\n \n# 스크린 다시 진입\nscreen -x [스크린 이름]\n \n# 스크린 종료 (백그라운드 종료)\n# 또는 실행하고 있는 스크린에 진입 하여 exit\nscreen -X -S [스크린 이름] kill\n \n# .sh 파일 같이 실행\nscreen [...옵션] ./test.sh (근데 홈부터 풀 경로로 줘야함)\n \n# 쉘 스크립트랑 같이 실행\nscreen -S [스크린 이름] bash -c &quot;스크립트&quot;\n \n# 백그라운드로 스크린 만들기\nscreen -dmS [스크린 이름]\n \n# 로그 파일 남기\nscreen [...실행 옵션] -L -Logfile [파일]\n \n# 이미 screen 만들어진 경우 로그\nscreen -S [스크린 이름] -X logfile [파일] &amp;&amp; screen -X log\n윈도우 관리\n\n약간 윈도우처럼 하나의 screen 에서 여러 창을 관리 할 수 있다\nctrl + a를 누른 다음 손을 떼고 다음 키를 누르는 식으로 사용한다.\n\n\nCtrl + A &amp; W\n윈도우 목록 확인 (하단에 윈도우 번호가 뜬다)\nCtrl + A &amp; C\n새 윈도우 생성하기\nCtrl + A &amp; &lt;윈도우 번호&gt;\n해당 윈도우로 전환\nCtrl + A &amp; Shift + S\n가로로 윈도우 분활\nCtrl + A &amp; Shift + \\\n세로로 윈도우 분활 (이걸많이 쓸듯)\nCtrl + A &amp; Shift + X\n현재 윈도우 종료\nCtrl + A &amp; Shift + Q\n현재 윈도우만 남기고 종료\nCtrl + A &amp; A + Tab\n윈도우가 분활된경우 윈도우 간 전환 (마우스가 사용불가함)\n\nscreen 내부 단축키\n\nCtrl+a, d\n현재 스크린으로 부터 탈출 (이게 백그라운드 실행에 근간)\n\nnohup\n실행\n\n백그라운드 실행\nnohup [작업] &amp;\n실시간 로그보기;\ntail -f nohup.out\n\n중지\n\n실행중인 프로세스 목록 (첫번째 숫자 확인)\n\nps -ef\nps auxf | grep [검색키워드]\n\n\nkill -9 [1 에서 나온 첫 숫자]\n\n사용자 추가\n\n여기 나온건 AWS인 것을 가정한 것\n\n# 관리자 전환\nsudo su\n \n# 계정 생성\nsudo adduser [유저이름]\n \n# [유저이름] 홈 디렉토리에 .ssh 폴더 생성\nsudo mkdir /home/[유저이름]/.ssh\n \n# authorized_key 파일 복사\nsudo cp /home/ubuntu/.ssh/authorized_keys /home/[유저이름]/.ssh\n \n# 권한 변경\nsudo chown -R new_user:new_user /home/new_user/.ssh\n \n# sudo 그룹에 추가 하여 sudo 사용가능하게\nsudo usermod -aG sudo [유저이름]\n비번 없이 루트 권한 사용하기\n\n보안땜에 신중히 사용할것\n\n\n관리자로 전환\nsudo su\nvisudo\n해당 파일 최하단에 반드시 해당구문 추가\n[계정명]   ALL=(ALL) NOPASSWD: ALL\n\n\n비밀번호 로그인 활성화/비활성화 하기\n\nauthorized 키 파일로 로그인이 가능하니 비번 로그인은 가급적 끄는게 좋다\n반대로 하면 활성화다\n\n#PasswordAuthentication 를 no 로 바꾸고 저장\nsudo vim /etc/ssh/sshd_config\n\n만약 그래도 적용이 안되는 경우 sshd_config.d 폴더가 있는지 확인하고\n해당 폴더에 50-cloud-init.conf 과 같은 설정파일이 또 있으면 거기서도 똑같이 수정\n\n아마 초기화용 백업본인거 같다\n\n\n\n패키지 관리\n\n초기셋팅:\n\nsudo apt update\napt list —upgradable\n\n\n모든 패키지 업데이트:\n\nsudo apt upgrade\n\n\n패키지 이름 으로 패키지 검색\n\ndpkg —list | grep &lt;패키지 이름&gt;\n\n\n패키지 이름이 들어간 모든 패키지 삭제\n\nsudo apt-get remove —purge &lt;패키지 이름&gt;\n\n와일드카드 (*) 사용가능\n\n\n찌꺼기 파일 확인\n\nsudo find / -name &quot;&lt;패키지 이름&gt;*&quot;\n\n\n\n\n패키지 정보 확인\n\nsudo apt show &lt;패키지 이름&gt;\nsudo apt-cache search &lt;패키지 이름&gt;\n\n\n패키지에 폴더 정보 확인\n\ndpkg -L [패키지 이름]\n\n\napt 저장소 위치\n\n/etc/apt\n(keyring 관련 문제가 발생하면 해당 폴더를 뒤지자)\n\n\n\n프로그램 메뉴얼 보기\n\nman &lt;프로그램&gt;\n\n파이썬 관련\n\n우분투에 파이썬은 커널과 매우 밀접한 관계를 가지므로 함부로 삭제하면 안됨!!!\n\n파일 관리\n\n\n폴더 조회:\n\nls &lt;폴더&gt;\n\n-R: 하위까지 출력\n\n\n\n\n\n파일 생성:\n\ntouch &lt;파일&gt;\n\n\n\n파일 삭제:\n\nrm &lt;폴더|파일명&gt;\n\n-rf: 폴더 삭제시\n\n\n\n\n\n파일 복사:\n\ncp &lt;대상&gt; &lt;위치&gt;\n\n-r : 디랙토리 복사\n\n\nrsync -av &lt;복사할 폴더&gt;  &lt;대상폴더&gt;\n\n--exclude [제외할 파일형식]: 파일 제외\n\n\n\n\n\n파일 이동:\n\n폴더 쓰면 해당 폴더 안에 있는거 통째로\n\n\nmv &lt;이동할 파일명&gt;  &lt;이동할 위치&gt;\n\n\n\n폴더 생성:\n\nmkdir &lt;이름&gt;\n\n-r: 하위 폴더 까지 만들기\n\n\n\n\n\n파일 권한\nchmod &lt;옵션 | null&gt; &lt;8진법, 기호&gt; &lt;파일&gt;\n\n파일 실행 권한 관리\n\n기호로 권한 설정\n\n1번째:\n\nu: 나, g: 그룹, o: 기타, a: 전부\n\n\n2번째:\n\n+: 권한추가, -: 권한 재거, =: 권한지정\n\n\n3번째:\n\nr: 읽기, w: 쓰기, x: 실행\n\n\n\n# 파일에 실행권한 주기\n# 따로 타겟을 지정 안하면 모든 사용자 권한 수정\nchmod +x &lt;파일이름&gt;\n \n# 그룹에 읽기, 쓰기 권한 부여\nchmod g+rw &lt;파일이름&gt;\n \n# 폴더 및 하위 파일/폴더 권한 부여\nchmod -R a+rw &lt;폴더이름&gt;\n8진법으로 권한 설정\n\n\n1번째: 내 권한\n2번째: 그룹 권한\n3번째: 다른 사용자 권한\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrwx421\n# 나는 읽기, 쓰기 권한 부여\n# 그룹은 읽기 권한만 부여\n# 다른 사용자는 일기 권한만 부여\nchmod 644 &lt;파일이름&gt;\n \n# 폴더 및 하위 파일/폴더 권한 변경\nchmod -R 644 &lt;폴더이름&gt;\nchown\n\n파일 소유권 관리\n\n\n특정 파일에 소유권이 u1 한테 있다면 이걸 읽거나 수정할 수 있는건 오직 root 와 u1 뿐이다\n다른 사용자가 그걸 수정 하거나 하려면 root 권한을 얻거나\n그 소유권을 가진 사용자가 chmod로 다른 사용자에 대한 권한을 부여 해줘야 함\n\n# 현재 폴더 내 파일들의 소유권 보기\nls -l\n \n# 해당 파일의 소유권을 &#039;소유자&#039; 로 변경\nchown &lt;소유자&gt; &lt;파일&gt;\n \n# 해당 파일의 소유권을 &#039;소유자&#039; 로 변경 하고 그룹을 지정\nchown &lt;소유자&gt;:&lt;그룹&gt; &lt;파일&gt;\n \n# 해당 폴더 및 하위 디렉토리의 파일, 디렉토리 모두 &#039;소유자&#039; 로 변경\nchown -R &lt;소유자&gt; &lt;폴더&gt;\n용량 확인\n#파티션별 용량확인\ndf -h\n \n#사용중인 모든 용량확인\ndf -P | grep -v ^Filesystem | awk &#039;{sum += $3} END { print sum/1024/1024 &quot; GB&quot; }&#039;\n \n#남은 모든 용량 확인\ndf -P | grep -v ^Filesystem | awk &#039;{sum += $4} END { print sum/1024/1024 &quot; GB&quot; }&#039;\n \n#해당 폴더 내 폴더들의 용량 확인\ndu -h --max-depth=1\nnano 단축키\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nsungje365.tistory.com/17\n\n\n\n\nCtal + K\n줄 삭제\nAlt +U\n실행 취소\nAlt +E\n다시 실행\n\n서비스 관리\n\n서비스 종료\nsudo systemctl stop [servicename]\n서비스 삭제\nsudo systemctl disable [servicename]\n서비스 등록\nsudo systemctl enable [servicename]\n서비스 재시작:\nsudo systemctl restart [servicename]\n서비스 로그보기\nsudo systemctl &lt;서비스파일&gt;.service\n\n서비스 만들기\n\n\n서비스 파일 만들기\n# 파일생성\nsudo nano /etc/systemd/system/&lt;서비스명&gt;.service\n\n\n내용작성 예\n\n공식문서\nsystemd service\n\n[Unit]\nDescription=&lt;서비스 설명&gt;\n \n[Service]\n# 기본타입\nType=simple\n#재시작 여부\nRestart=always \nExecStart=&lt;살행 스크립트 (절대 앞에 bash 붙이지 말것)&gt;\n \n[Install]\n#부팅시 시작\nWantedBy=multi-user.target\n\n\n서비스 등록\n#서비스 리로드\nsudo systemctl daemon-reload\n# 부팅시 실행에 등록\nsudo systemctl enable &lt;서비스파일.service&gt;\n# 서비스 시작\nsudo systemctl start &lt;서비스파일.service&gt;\n\n\n사용자 관리\n사용자\n\n사용자 생성\nsudo adduser [계정]\n사용자 삭제\nsudo userdel -rf [계정]\n사용자 목록 확인\ngrep /bin/bash /etc/passwd | cut -f1 -d:\n사용자 이동\nsudo su [계정]\n사용자 비번 변경:\nsudo passwd [계정]\n\n그룹\n\n사용자 그룹 확인\ncat /etc/group\n사용자를 보조 그룹에 추가\nsudo usermod -aG &lt;그룹명&gt; &lt;사용자 이름&gt;\n사용자 기본 그룹 변경\nsudo usermod -g &lt;그룹명&gt; &lt;사용자 이름&gt;\n사용자를 그룹에서 제외\nsudo gpasswd -d &lt;사용자 이름&gt; &lt;그룹명&gt;\n그룹 생성 \ngroupadd [그룹명]\n\nIP 주소 확인\n\n내부 IP 주소\nhostname -I\n외부 IP 주소\ncurl icanhazip.com\n\n환경변수 편집\n사용자\n\n[편집기] ~/.bashrc\nsource ~/.bashrc\n수동으로 처리해야 하는 경우 export PATH=적용할꺼:$PATH 이런식으로 추가\n\n전역\n\nsudo [편집기] /etc/environment\nsource /etc/environment\n\n압축 / 해제\nzip\n\n중요한게 zip 으로 압축하면 리눅스에서 지정한 파일속성이 다 날라간다!!!\n\n\nzip 없으면 설치\napt install zip\n\n# 압축 해제\nunzip &lt;압축파일&gt;.zip\n \n# 폴더지정하여 풀기\nunzip &lt;압축파일&gt;.zip -d &lt;압축 푸는 위치&gt;\n \n# 폴더 압축\nzip -r test.zip &lt;폴더&gt;\ntar\n\n정확히는 tar 은 압축 파일이 아니라서 tar 만들고 gz로 압축하는\n\n# tar.gz 압축 풀기\ntar -zxvf &lt;압축파일명&gt;.tar.gz\n \n# 지정된 경로로 풀기\ntar -zxvf &lt;압축파일명&gt;.tar.gz -C &lt;압축푸는 위치&gt;\n \n# tar.gz 압축\ntar -zcvf &lt;압축파일명&gt;.tar.gz &lt;파일1, 파일2 | 폴더&gt;\n \n# 현재 폴더의 모든걸 압축\ntar -zcvf &lt;압축파일명&gt;.tar.gz *\nSSH 인증키 발급\n\n테스트는 안해봄\n\n\nPuTTYgen을 실행하여 Generate 를 실행하여 키를 발급\nPublicKey 를 저장하고 아래처럼 수정\n\nBefore\n---- BEGIN SSH2 PUBLIC KEY ----\nComment: &quot;rsa-key-20240716&quot;\n[키내용]\n---- END SSH2 PUBLIC KEY ----\n\nAfter\nssh-rsa [키내용]\n\n\n\n\n인증 접속하려는 사용자홈에 .ssh 폴더 만들고 authorized_keys 파일 만들기\nmkdir .ssh\ncd .ssh\n[편집기] authorized_keys\n\nauthorized_keys에 위에 수정한 public key 내용을 적고 저장\nchmod 600 ./authorized_keys 를 입력하여 나만 읽고 쓸 수 있게 끔만듬\nPrivateKey를 ppk 로 저장하여 putty에서 사용하면됨\n\n백업 / 복원하기\ntar 로 백업하기\n\n특수 폴더들 제외하고 시스템 폴더를 포함한 모든걸 tar로 압축 하고\n복원할때 기본 시스템은 클린 설치 하고 압축 풀어서 덮어쓰는 방식으로 이루어짐\n# 백업\nsudo tar -cvpzf /backup/backup.tar.gz \\\n--exclude=/backup \\\n--exclude=/proc \\\n--exclude=/sys \\\n--exclude=/mnt \\\n--exclude=/media \\\n--exclude=/run \\\n--exclude=/dev \\\n--exclude=/lost+found /\n \n# 복원\ntar xvpfz &lt;백업파일&gt; -C\n\n\nimg 파일 만들어서 백업\n\nUSB 같은 외부 저장소가 있을때 사용가능\n\n\n\n파티션 확인하기\nseoksee@ubuntu:~$ sudo fdisk -lsudo fdisk -l\n \n#출력이 이렇게 나오는데\n# mmcblk0 이라는 이름의 디스크에 p1, p2 라는 두개의 파티션이 존재한다는 것\n===\n/dev/mmcblk0p1 *       2048   1050623   1048576  512M  c W95 FAT32 (LBA)\n/dev/mmcblk0p2      1050624 125173726 124123103 59.2G 83 Linux\n===\n\n\nimg 파일 만들기\n# 파티션을 제외한 장치명 mmcblk0만 입력\nsudo dd if=/dev/mmcblk0 of=&lt;외부장치&gt; stutus=progress\n\n\n복원은 따로 이미지를 굽거나 하면 됨\n\n\n스케줄링 사용 (crontab)\n\n주기적으로 시간에 따라 스크립트를 실행하는\n\n구성하는법\n\n먼저 root 계정으로 이동한다 sudo su\n\nsudo 권한 필요한 프로그램 대처하기 위해 root에 crontab 을 수정하는거\n\n\n구성을 편집한다\ncrontab 을 재시작 한다\nservice cron restart\n\n\n구성편집\n\n\n                  \n                  구성설명 \n                  \n                \n\n\ndanmilife.tistory.com/4\n\n\n\n#입력\ncrontab -e\n \n# 맨 끝에 추가\n===\n* * * * * &lt;실행명령&gt;\n===\n\n1번 *: 분 (0~59)\n2번 *: 시 (0~33)\n3번 *: 일 (1~31)\n4번 *: 월 (1~12)\n5번 *: 요일 (0~7)\n\n스케줄링 로그보기\ngrep CRON /var/log/syslog\n추가 명령\n\n구성을 본다\ncrontab -l\n모든 구성을 삭제한다\ncrontab -r\n\nDynamic port forwarding (DPF) 설정하기\n\nSSH를 프록시 서버 로 연결하는\n\nSSH 설정\n\nSSH 포트포워딩 허용하기 sudo nano /etc/ssh/sshd_config\n이렇게 수정\nAllowAgentForwarding yes\nAllowTcpForwarding yes\nGatewayPorts yes\n\n해당 구문을 통해 적용\nsudo systemctl restart ssh\n\n\nDPF 로 SSH 접속하기\n\nPowerShell 애서\n ssh -D [매핑포트] -f -C -N -i [pem 인증키] [사용자]@[호스트]\n\n이후 커넥션을 계속 유지시킨다\n\n\nPutty 에서\n\nConnection → SSH → Tunnels  로 이동\nSorce port에 매핑시킬 포트입력\nDestination 아래 있는 라디오 버튼을 Dynamic 으로 설정\n이후 Open을 눌러 SSH 서버에 접속\n이후 커넥션을 계속 유지시킨다\n\n\n\nSOCKS 프록시로 연결\n\n윈도우 설정 → 네트워크 및 이더넷 → 프록시\n수동 프록시 설정 에서 프록시 서버 허용\n주소 항목에 이렇게 입력\nsocks=127.0.0.1\n\n포트 항목에 매핑포트 를입력\n로컬(인트라넷) 뭐시기 체크 하고 저장\n\n그 외\n\n작업 관리자:\nhtop\nroot 비번 변경:\nsudo passwd root\n시간 설정하기:\nsudo timedatectl set-timezone Asia/Seoul\n패키지 위치확인:\nwhich [패키지]\n기본 텍스트 에디터 변경\nupdate-alternatives —config editor\n\n원하는 번호 입력\n\n\n내가 입력한 모든 명령어보기:\nhistory\n긴 파일 읽기:\nmore &lt;파일&gt;\n&lt;대충 뭔가 출력되는 명령&gt; | more\n문자열 찾기\ngrep\n열린 포트 확인:\nnetstat -tnlp\n\nwget\n\n파일명 지정하여 다운로드:\n\nwget &lt;옵션 | null&gt; &lt; URL &gt; -O [파일명]\n\n\n옵션\n\n--content-disposition: 원본 파일명 유지\n-P &lt;경로&gt;: 경로 지정하여 다운로드\n\n\n\ncurl &lt;옵션&gt; &lt; URL &gt;\n\n파일 다운로드 옵션:\n\n-o &lt;파일명&gt;: 파일명 지정\n-O: 저장 시 원격에 있는 이름 가져오기\n-J: 원본 파일명 유지\n\ncontent-disposition 해더를 읽어옴\n\n\n--output-dir &lt;경로&gt;: 경로지정\n--location: 리디이렉트 사용\n\n\n\n하드디스크 메모리 스왑 설정\n\nAWS 프리티어에 경우 메모리 부족으로 끊기는 것들이 많은데 이걸 메모리 스왑으로 해결\n\n# 메모리 스왑 2GB 정도로 설정 \nsudo dd if=/dev/zero of=/swapfile bs=128M count=16\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n# 성공적으로 됬는지 확인\nsudo swapon -s\nsudo [편집기] /etc/fstab\n\t# 해당 파일 맨끝에 추가\n\t/swapfile swap swap defaults 0 0\n#적용 확인 (shared 부분)\nfree\n \n#재부팅 해주면 좋음\nhostname 변경 (&lt;사용자&gt;@&lt;호스트이름&gt;:~$)\n# 원하는 이름으로 변경\nsudo nano /etc/hostname\n \n===재부팅===\n윈도우에서 리눅스 시스템 파일 보는법\n\n리눅스는 ext4 파일 시스템을 쓴다\n\n\nWSL로 마운트 하거나\nLinux File Systems for Windows 로 마운트\n"},"리눅스/패키지-설치":{"title":"패키지 설치","links":["CLI-프로그램-명령어/Conda-명령어-정리","CLI-프로그램-명령어/MySql-명령어-정리","스터디/Docker-스터디","CLI-프로그램-명령어/Docker-명령어-정리","스터디/Kubernetes-(K8S)-스터디"],"tags":[],"content":"필수 패키지\n\ntree: 폴더 구조를 볼수있는 패키지\n\n사용법: tree\n\n\nnet-tools: 네트워크 정보 보는\n\n사용법: ifconfig\n\n\nlm-sensors: PC 온도 확인 하는\n\n사용법:\n\nsensors-detect: 센서 가능 여부 확인\nsensors: 온도 확인\n\n\n\n\nmc: cli 파일 브라우저\n\n사용법: mc\n\n\n\nNginx 설치\n# 설치\nsudo apt install nginx\n \n# 서비스 시작\nsudo systemctl start nginx\n \n# 서비스 등록\nsudo systemctl enable nginx\nNode &amp; Npm 설치\n1. NVM 설치\n\ncurl -o- raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash\n\n명령 바뀔 수 있으니 github.com/nvm-sh/nvm 에서 확인 바람\n\n\nsource ~/.bashrc\n\n2. NVM 으로 Node &amp; Npm 설치\n\nnvm install —lts\n\nNVM 명령어 정리\n\n현재 설치된 모든 버전 보기\nnvm list\n해당 버전으로 전환\nnvm use [버전]\n해당 버전으로 설치\nnvm install [버전]\n\nConda 설치\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nConda 명령어 정리\n\n\n\n\ndocs.anaconda.com/miniconda/install/#quick-command-line-install 여기 나온대로 하고\nsource ~/.bashrc\n[편집기] ~/.bashrc\n맨 아래 입력\nexport PATH=&quot;/home/[사용자]/miniconda3/bin:$PATH&quot;\n\nsource ~/.bashrc\n사용자 홈 디렉토리로 이동(cd) 하여 해당 명령어 입력\nconda init\n\nsource ~/.bashrc\n\n쉘 스크립트에서 init\n\n쉘 스크립트 에서는 바로 activate 해버리면 작동안함\n먼저 source 명령 한번 하고 사용\n\nsource ~/miniconda3/etc/profile.d/conda.sh\nconda activate ./.conda\nOpenCV libGL.so 애러해결\napt install libgl1\n \n# libgthread-2.0.so.0 이 애러도 난다면\n===\napt install libglib2.0-0\n===\n\n종종 발생하는 문제인거 같아 기록\n\nJDK 21 설치\n\nsudo apt install openjdk-21-jdk\n자바 경로 확인\n\n기본: /usr/lib/jvm/java-[버전]-openjdk-amd64\n\n\n[편집기] ~/.bashrc\n맨 아래 입력\nexport JAVA_HOME=&quot;/usr/lib/jvm/java-21-openjdk-arm64&quot;\nexport PATH=&quot;$JAVA_HOME/bin:$PATH&quot;\n\nsource ~/.bashrc\n\nMySql 설치\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nMySql 명령어 보러가기\n\n\n\n패키지 설치\nsudo apt install mysql-server\n \n#서비스 시작\nsudo systemctl start mysql\n \n# 서비스 자동 시작 등록\nsudo systemctl enable mysql\n \n#만약 ufw 사용중이면\n=== \nsudo ufw allow mysql\n===\n초기셋팅\n\n\nroot 계정으로 mysql로 접속\n\nroot 계정만 sudo 로 접속하고 다른거는 sudo 없이 접속\n\nsudo mysql -u root\n\n\n계정 설정\nuse mysql;\n#root 계정명 변경 (보안땜에 바꾸는게 좋음)\nupdate user set user=&#039;바꿀 계정명&#039; where user=&#039;root&#039;;\n \n# 계정명 적용\nflush privileges;\n \n# 계정에 비번 설정하기\nalter user &#039;바꾼계정명&#039;@&#039;localhost&#039; identified with mysql_native_password by &#039;설정할 비번&#039;;\n \n# 비번 적용\nflush privileges;\n \n# Mysql에서 나가기\nexit\n \n#서비스 중지\nsudo systemctl stop mysql\n \n#서비스 시작\nsudo systemctl start mysql\n\n\n이후에 바꾼 계정으로 접속하기\nmysql -u [바꾼계정명] -p\n\n\n시간 설정하기\n\nmysqld.cnf 하단에 해당 구문 추가\n\nsudo [편집기] /etc/mysql/mysql.conf.d/mysqld.cnf\n\ndefault-time-zone=&quot;+09:00&quot;\n\n\n\n\n외부 접속 허용\n\n어차피 SSH로 터널링해서 사용할 듯 하지만 필요할 수도 있기에\n\n\n\nmysqld.cnf 수정\n\nsudo [편집기] /etc/mysql/mysql.conf.d/mysqld.cnf\n\n이렇게 수정\nbind-address = 0.0.0.0\n\n\n계정에 외부 클라이언트 허용하기 (MySql에 접속해 있다 가정)\nuse mysql;\nUPDATE user SET host=&#039;%&#039; WHERE user=&#039;계정명&#039; and host=&#039;localhost&#039;;\n\nmysql 기본 계정은 기본적으로 내부 사용가능하도록 되어있음\nhost를 %로 주면 모든 호스트를 받겠다는 의미 만약 특정 호스트만 허용하고 싶으면 host 값을 해당 ip로\n\n\n\nMySql 포트로 포트포워딩 (기본 3306)\n\n\n완전삭제하기\nsudo apt-get remove --purge mysql*\n \nsudo rm -rf /etc/mysql /var/lib/mysql\nsudo rm -rf /var/log/mysql\nsudo rm -rf /var/log/mysql.*\nDocker 설치\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nDocker 스터디\nDocker 명령어 정리\n\n\n\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n \necho \\\n  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n \nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n \n#sudo 없이 사용하려면\n=== \nsudo usermod -aG docker &lt;사용자&gt;\n===\n공식문서\n\ndocs.docker.com/engine/install/ubuntu/\n\nKubernetes (kubeadm) 설치\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nKubernetes (K8S) 스터디\n\n\n\nsudo apt-get update\n# apt-transport-https may be a dummy package; if so, you can skip that package\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n \ncurl -fsSL pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho &#039;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] pkgs.k8s.io/core:/stable:/v1.31/deb/ /&#039; | sudo tee /etc/apt/sources.list.d/kubernetes.list\n \nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n \nsudo systemctl enable --now kubelet\n공식문서\n\nkubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\n\nNVIDIA 그래픽 드라이버 설치\nsudo apt-get update\n \n# nvidia-driver 들어간 패키지 검색\nsudo apt-cache search nvidia-driver\n \n# 찾은 버전중 원하는거 설치\nsudo apt-get install nvidia-driver-&lt;버전&gt;-server\nNVIDIA Container Toolkit 설치\n\nDocker 같은 컨테이너 환경에서 필수\n\n\ndocs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-apt\n\nCUDA Toolkit 설치\n\n그래픽 드라이버가 설치되있는 상태에서 CUDA(nvcc)만 따로 설치할 때\nDocker 컨테이너 내부에서 nvcc 를 사용할 때\n\n\n11.8 기준\n\n\nCUDA 설치 방법\n\n\nBase Installer 항목에 나온데로 하고 마지막 (sudo apt-get -y install cuda) 명령은 실행x\n\n\n이후 해당 명령어 입력\n# 버전은 알아서 바꿀것\napt-get install cuda-toolkit-11.8\n\n\n설치가 완료되면 환경 변수 추가\n[편집기] ~/.bashrc\n \n#가장 아래에 추가\n===\nexport PATH=/usr/local/cuda/bin:$PATH \nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n===\n \nsource ~/.bashrc\n \n#적용 완료됬는지 확인\nnvcc -V\n\n\n\n"},"스터디/AWS-스터디--and--프로젝트":{"title":"AWS 스터디 & 프로젝트","links":["스터디/VPN-서버-구성--and--비교"],"tags":[],"content":"RDS 퍼블릭 엑세스 끄고 외부 환경에서 접속하기\n\n2024년 2월 이후로 IPv4 주소 유료화에 따라 RDS 퍼블릭엑세스에 사용되는 ipv4 주소는 더 이상 무료가 아니다. (EC2에 사용되는 IP는 프리티어면 공짜다)\n따라서 퍼블릭 엑세스를 끈 상태에서 접속을 시도해야 한다\n\nSSH 를 사용하여 접속\n\nEC2와 RDS 를 같은 VPC 망으로 사용한다면 EC2의 SSH를 사용하여 RDS 에 접속이 가능하다.\nDataGrap이나 기타 DB관리 IDE 등은 대부분 SSH 접속을 허용한다.\nSSH 호스트, EC2 계정 이름, PPK 파일을 EC2와 동일하게 설정해주면 된다.\n\nEC2 RDS 연결\nvelog.io/@whytili/AWS-RDS%EC%99%80-EC2-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0\nEC2로 VPN 만들기\n\n제일 저렴한 t4g.nano 인스턴스 No-ip DDNS 서비스를 사용하여 OpenVPN 서버 구축\n\n요금\n\nt4g.nano 인스턴스 요금\n\n시간당: 0.0054$\n24시간 1달: 3.8$\n\n\nPubilc IP 요금\n\n시간당: 0.005$\n24시간 1달: 3.6$\n\n\n종합:\n\n시간당: 0.0104$\n24시간 1달: 7.4$\n\n\n\n결론\n\n이렇게 구성하는게 제일 좋을듯 하다\n\n과정\nVPN 서버 구성 &amp; 비교\nVPC로 VPN 만들기\n\nVPC 기능중에 클라이언트 VPN 엔드포인트를 사용하여 VPN을 구현할 수 있다\n\n\n원래 이 기능은 Pubilc ip 할당 안한 aws 서비스들을 vpn 터널링 하여 접근 할 수 있도록 하는게 목적이다\n하지만 인터넷을 연결 시켜서 우리가 보편적으로 생각하는 vpn을 만들 수 있다\n대부분에 VPN 서비스는 정액제로 받기 때문에 내가 안쓰고 있을때도 요금이 나가는 문제가 있는데 이거를 커버 치기 위해서 이짓거리를 한번 해봤다\n\n전략\n\n사용하지 않을 때는 대상 네트워크 연결 탭 가서 서브넷 연결 해제 시키자\n\n서브넷과 연결시켜두기만 해도 요금이 바사삭\n\n\n다시 사용할때\n\n다시 같은 정보로 연결\n클라이언트 VPN 엔드포인트 내 라우팅 테이블 다시설정\n\n\n\n요금\n\n\nAWS Client VPN 엔드포인트 연결\n\n네트워크 연결이 되어있는 상태에 부과 (서브넷이 연결되있는 상태)\n\n\n시간당: 0.15$\n24시간 1달: 108$\n\n\n\nAWS Client VPN 연결\n\nVPN 사용자 수 x 시간당요금 으로 책정\n\n\n시간당: 0.05$\n24시간 1달: 36$\n\n\n\n결론\n\n단순 유지만 하더라도 달에 10만원 이상이 나옴으로 안쓸때는 네트워크를 해제 시켜야 할꺼같다\n생각보다 비싼 요금과 번거로움 때문에 차라리 저렴한 EC2에 VPN 서버 설치해서 하는게 나을듯\n어차피 EC2 VPN도 안쓸때 잠깐 정지해놓으면 거의 요금 안나오니까 말이다\n\n과정\n\n\n                  \n                  추가 가이드 \n                  \n                \n\n\nClient VPN Endpoints로 Private하게 접근하기\nAccess the internet using Client VPN\n\n\n\nAWS Certificate Manager 에서 인증서 등록하기\n\nEasy-RSA를 사용하여 서버, 클라이언트 인증서 발급\n발급 과정 완료하면 pki 폴더 들가서 ca.crt, issued/, private/ 확인\nAWS Certificate Manager 탭 → 가져오기 클릭해서 인증서 등록하기\n\n서버측, 클라이언트측 두 개가 필요하기 때문에 두개를 등록해야함\n서버측 파일:\n\n인증서 본문 = server.crt\n인증서 프라이빗 키= server.key\n인증서 체인 = ca.crt\n\n\n클라이언트측 파일\n\n인증서 본문 = client1.domain.tld.crt\n인증서 프라이빗 키= client1.domain.tld.key\n인증서 체인 = ca.crt\n\n\nYML 형태로된 파일이 있을 수 있는데 YML 문법 제외하고 이렇게 생긴부분 부터 복사\n-----BEGIN PRIVATE KEY----- \n-----END PRIVATE KEY-----\n\n\n\n\nVPC 구성하기\n\nVPC생성 &amp; 해당 VPC 서브넷 생성 (원래꺼 있으면 그거 써도 상관없음)\n해당 VPC에 인터넷 게이트웨이 생성\n라우팅 테이블 → 서브넷 연결 → 서브넷 연결 편집 → 생성한 서브넷 연결\n라우팅 테이블 → 라우팅 → 라우팅 편집 → 라우팅 추가\n\n1번 대상 = 0.0.0.0/0\n2번 대상 = 인터넷 게이트웨이 &amp; 생성한 인터넷 게이트웨이\n\n\n보안그룹 → 같은 VPC 보안그룹 선택 → 인바운드, 아웃바운드 모두\n\nTCP/UDP 모두, 모든포트, 0.0.0.0/0 으로 모든 포트 허용 시키기\n\n\n\n클라이언트 VPN 엔드포인트 생성하기\n\n클라이언트 IPv4 CIDR:\n\nVPC와 다른 CIDR 로 설정 (예: 10.0.0.0/22)\n\n\n서버 인증서 ARN:\n\n서버측으로 생성된 ARN 선택\n\n\n상호 인증 사용 체크:\n\n클라이언트 인증서 ARN: 클라이언트 측으로 생성된 ARN 선택\n\n\nDNS 서버:\n\n보통은 구글로 할듯 (8.8.8.8, 8.8.4.4)\n\n\n전송 프로토콜:\n\nTCP로\n\n\n분할 터널 활성화 체크해제:\n\n활성화 하면 외부요청은 VPN 거치지 않고 요청되고\n내부 요청만 VPN 거치게끔 설계가능함\n\n\nVPC:\n\n생성한 VPC 선택\n\n\n보안그룹:\n\nVPC랑 같은거 선택\n\n\n\n클라이언트 VPN 엔드포인트 구성하기\n\n대상 내트워크 연결\n\nVPC, 서브넷 맞는거 설정\n\n\n권한 부여 규칙 → 권한 부여 규칙 추가\n\n액세스를 활성화할 대상 네트워크 = 0.0.0.0/0\n\n\n클라이언트 VPN 엔드포인트 내 라우팅 테이블 → 경로 생성\n\n경로대상 = 0.0.0.0/0\n대상 네트워크 연결을 위한 서브넷 ID = 위 선택한 서브넷과 동일한 서브넷\n\n\n\nOpenVpn 으로 접속하기\n\nVPC 탭 → 클라이언트 VPN 엔드포인트 → 클라이언트 구성 다운로드\n다운로드 받은 .ovpn 파일에 해당 &lt;/ca&gt; 다음 부분에 추가\n&lt;cert&gt; \nclient1.domain.tld.crt 키 파일 네용\n&lt;/cert&gt; \n\n&lt;key&gt; \n client1.domain.tld.key 키 파일 내용\n&lt;/key&gt;\n\n\n해당 구성 파일을 OpenVpn에 등록하여 VPN 실행\n\nVPC\nVPC 서브넷 종류\n퍼블릭 서브넷\n\n인터넷 게이트웨이(IGW)로 라우팅 되어있는 서브넷\n인터넷과 직접 소통이 가능하다\nEC2 나 이런거 쓸때 외부 연결 하고 싶으면 해당 서브넷 으로 해야한다\n\n프라이빗 서브넷\n\n인터넷 게이트웨이(IGW)로 라우팅 되어있지 않은 서브넷\n인터넷이 끊겨 있어 내부망 통신밖에 하지 못한다\n공인 IP 주소를 할당받은 퍼블릭 NAT 게이트웨이에 라우팅 하면 인터넷 연결이 가능하다\n\n가용역역\n\n이건 한 지역에 다른데이터 센터 그룹임\n가령 서울 리전이라고 치면 A는 강남에 있는거고 B는 구로 에 있는느낌\n"},"스터디/CORS-(Cross-Origin-Resource-Sharing)":{"title":"CORS (Cross-Origin Resource Sharing)","links":[],"tags":[],"content":"\n\n                  \n                  참고자료 \n                  \n                \n\n\n참고자료1. 공식문서\n참고자료2. 악명 높은 CORS 개념 &amp; 해결법 - 정리 끝판왕\n\n\n\n\n교차 출처 리소스 공유\n\n\n브라우저가 자신의 출처가 아닌 것들로 부터 요청을 서버가 허용 해주는 HTTP 해더 메커니즘\n즉 클라이언트 쪽에서 서버의 요청을 허용할 것인지를 결정하게 된다\n\nCORS 애러 이유\n\n결론부터 말하면 CORS 정책을 지키지 않은 요청으로 인해서 발생한다\n백엔드에서 응답줄 때 Access-Control-Allow-Origin해더를 설정해야 함\n\n동일 출처 정책 (Same-Origin)\n\n웹이 구동중인 서버와 HTTP 요청을 통해 불러오는 리소스가 서로 동일한 서버 여야 하는 경우\n즉 요청이 내 서버에서 일어난거 아니면 오류 내뿜게 되는\n\n\n동일 출처의 조건은 HTTP/HTTPS, 호스트, 포트 까지 모두 동일한것을 의미\n브라우저의 JS 의 fetch 와 XMLHttpRequest는 기본적으로 해당 정책을 준수 한다\n하지만 당연하게도 웹에선 필히 외부 요청을 통해 리소스를 가져올 수 있어야한다\n다른 출처 리소스를 불러올때는 반드시 CORS 정책을 지켜서 요청을 보내야 한다\n\n어떤 것들이 해당하는가\n\n브라우저의 JS 의 fetch 와 XMLHttpRequest\n웹 폰트 등등\n\n설명\n\nNode.js 의 fetch 나 기타 다른 언어들의 HTTP Reqests 에는 해당 되지 않는다\n이런 조건으로 서버 쪽에서는 다른 웹 서비스가 내 REST API를 이용 할 수 있도록 설정 하거나 제한 할 수 있다는 이점도 가진다\n\n그러나 CORS는 웹 브라우저 엔진에서만 작동하는거라 어떠한 불특정 다수의 DDOS 공격을 막거나 하는것은 불가하다\n왜냐면 그냥 브라우저가 아닌 상황에서의 HTTP Reqests애서는 CORS 가 의미 없기 때문\n\n\n\n이렇게 해둔 이유?\n\n만약 아무렇게나 요청을 할 수 있다라고 가정하면\n피싱사이트를 개설한 다음 네이버에 요청을 한다고 가정한다\n그럼 네이버에 POST 요청을 보내서 브라우저의 저장된 쿠키로 로그인 한다던가 하는게 가능하지 않겠나?\n브라우저가 아닌 환경에서는 쿠키를 얻는것이 쉽지 않기에 상관 없지만 브라우저는 아니기 때문에\n\nCORS 정책\n\n요청/응답 시 저정된 특수 해더를 붙어야 한다\n\n과정\n\n브라우저측 요청 (Fetch() 사용)\nGET /resources/public-data/ HTTP/1.1\nHost: bar.other\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us,en;q=0.5\nAccept-Encoding: gzip,deflate\nConnection: keep-alive\nOrigin: foo.example\n\n여기서 핵심은 Origin, 현재 요청의 원본 주소를 가르킨다\n\n\n서버 측 응답\nHTTP/1.1 200 OK\nDate: Mon, 01 Dec 2008 00:23:53 GMT\nServer: Apache/2\nAccess-Control-Allow-Origin: *\nKeep-Alive: timeout=2, max=100\nConnection: Keep-Alive\nTransfer-Encoding: chunked\nContent-Type: application/xml\n\n여기서 핵심은 Access-Control-Allow-Origin, 해당 응답의 접근 가능한 주소 목록을 나타낸다\n* 이면 모든 것들 허용\n\n\n\n설명\n\nFetch 함수에서 자동으로 Origin해더를 붙여 원본 주소를 포함한 정보를 서버에 요청을 보낸다\n서버 쪽에서 응답을 받으면 Access-Control-Allow-Origin 의 자신의 도메인(ip) 정보가 있는지 파악한다\n없는 경우 해당 리소스의 접근 권한이 없다 판단하여 CORS 애러를 내뿜는다\n\nCORS 해더\n클라이언트 요청\n\nAPI 가 자동으로 설정해줌\n\n\nOrigin: IP | Domain:\n\n클라이언트 원본 주소\n\n\nAccess-Control-Request-Method:\n\n어떤 메소드 인지\n\n\nAccess-Control-Request-Headers:\n\n어떤 해더에 접근하려는 건지\n\n\n\n서버 측 응답\n\nAccess-Control-Allow-Origin: [IP | Domain | *]:\n\n해당 호스트의 리소스 접근을 허용하는\nCORS 의 핵심\n\n\nAccess-Control-Expose-Headers: [Heders...]:\n\n접근가능한 해더 목록 노출\n\n\nAccess-Control-Allow-Methods: [HTTP Methods...]:\n\n허용하는 HTTP Method\n\n\nAccess-Control-Max-Age: Mills:\n\n응답 캐싱 시간\nPreflight의 응답을 케싱 하려면 설정 필요\n\n\nAccess-Control-Allow-Credentials: boolen:\n\n아래 Credentialed Request 를 사용할 경우 ture로 설정\n\n\n\n예비 요청 (Preflight)\n\n브라우저가 요청 보낼 때 한번에 바로 보내지 않고 먼저 예비 요청을 하는 과정\n\n\nPreflight 는 OPTIONS 메소드를 사용하여 요청한다\n해당 요청으로 서버가 지원하는 Access-Control-Allow-Origin, Access-Control-Allow-Methods 정보들을 안전확보 후 본 요청을 보내게\n하지만 여기서 문제는 응답 시간인데 결국 2번 요청을 보내는 거니까 해결을 위해 Access-Control-Max-Age 해더를 서버측에 설정하면 Preflight 응답을 케싱 할 수 있다\n\n단순요청 (Simple Request)\n\n위 Prefilght  과정 없이 다이렉트로 요청하는\n아래 조건을 만족시켜면 Preflight 과정 없이 진행된다\n\n\n가능 조건:\n\n메소드\n\nGET, HEAD, POST\n\n\n해더:\n\nAccept, Accept-Language, Content-Language, Content-Type, DPR, Downlink, Save-Data, Viewport-Width, Width\n\n\nContent-Type:\n\napplication/x-www-form-urlencoded multipart/form-data, text/plain\n\n\n\n\n\n인증된 요청 (Credentialed Request)\n\n세션id 가 저장되어있는 쿠키, Authorization 해더 토큰값 같은 자격 정보를 보낼 때 사용하는 방식\n\n\n단순요청 조건을 만족하면 단순요청 으로 처리되고 아니면 예비요청 으로 처리됨\n\n제약 사항\n\n해당 요청에서 서버측 해더에 이것들 지키지 않으면 CORS오류 뜸\n\n\nAccess-Control-Allow-Credentials를 true로 설정\nAccess-Control-Allow-Origin, Access-Control-Allow-Methods, Access-Control-Allow-Headers 에서 * 사용불가\n\nAPI 설정\n\nFetch\n\ncredentials 옵션을 include로 설정하기\nfetch(&quot;example.com:1234/users/login&quot;, {\n\tmethod: &quot;POST&quot;,\n\tcredentials: &quot;include&quot;, // 자격 정보를 담겠다는 의미\n})\n\n\n\n"},"스터디/DBMS-이론":{"title":"DBMS 이론","links":[],"tags":[],"content":"DBMS 터널링 관련 문제\n보통 DBMS에 경우 포트를 개방하지 않고 SSH를 터널링을 하여 접속하곤 하는데\n이렇게 SSH 터널링이 성공적으로 끝난 뒤 보통은 자체적으로 Handshake 패킷이 서버에서 날라오게 된다.\n대표적으로 MySQL에 경우 MySQL Handshake Initialization Packet이라고 터널링이 성립된 이후에 바로 클라이언트에게 서버에 정보를 담은 패킷을 전송 하게 된다.\n즉 이 시점에서 MySQL은 클라이언트가 접속했구나라고 판단했기 때문이다.\n여기서 발생하는 문제로 이 시점에 터널링된 클라이언트가 일정 시간 내 해당 패킷에 맞는 응답을 전달하지 못한경우 MySQL은 DB연결을 끊어버린다.\n코딩으로 DBMS와에 SSH 포워딩만 구현하여 포트만 확보해 놓은경우, 특정시간 이내\nDBMS 클라이언트로 연결시도하는 코드를 구현 하지 않았다면 해당 포트를 통해서 DBMS 연결이 불가하다는 뜻이다.\nDB 암호화\n사용자가 입력한 비번은 해싱 해서 저장해야 하는데\n문제는 사용자가 비번을 qwer1234 이따구로 하면 이건 인터넷에 해당하는 해시값이 널려있음…\n이걸 Hash lookup table 이라고 하는데 이게 인터넷 상에 그럴듯한 비번은 올라와있는 경우가 있음\n그래서 salt 라고 버번저장 시 내가 특수 문장을 추가로 집어넣는 것\n가령 qwer1234 저장 한다면 qwer1234kasdh 로 kasdh 라는 추가적으로 문장을 추가하여 그걸 해싱 하는 거임\n검증할때도 qwer1234+kasdh를 해싱한 값으로 검증하면 되고"},"스터디/Docker-스터디":{"title":"Docker 스터디","links":["CLI-프로그램-명령어/Docker-명령어-정리","CLI-프로그램-명령어/Docker-Swarm-명령어-정리","리눅스/패키지-설치","스터디/네트워크-통신","스터디/개발-용어-정리"],"tags":[],"content":"\n리눅스 운영체제 딴 가상화, 컨테이너 기술\n\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nDocker 명령어 정리\nDocker-Swarm 명령어 정리\n리눅스 Docker 설치\n\n\n\n\nConda 같은 느낌으로 가상환경을 생성/관리하는 기술\n개발환경 간 차이를 이미지 라는걸 통해 거의 동일하게 하여 개발환경 셋팅이 복잡한 서비스를 한번에 설치 할 수 있다\n컨테이너 간 격리된 구조로 보안이 좋음\n이미지 를 통해 가상환경을 만든다 라는걸 컨테이너 생성 이라고 함\n\n사용하는 이유\n왜 굳이 Docker를 써야하나요\n\n\n딥러닝 한번 해보면 안다....\n\n논문에 재연된 코드들 대부분은 python 버전 부터 시작해서 cuda 버전 부터 다 제각각 이다.\nconda를 통해서 python 까지는 어느정도 분리 시킨다 하더라도 cuda는 드라이버이기 때문\n그래서 컨테이너 통해서 필요 리눅스 패키지들과 드라이버를 환경을 분리 한채로 이 한계를 극복 가능하다\n\n\n\n성능하락 없음 &amp; 가벼움\n\nVM 으로 구성한다고 치자. 내 시스템에 있는 모든 자원이 해당 VM에 미리 할당을 해놔야 한다\n그러니까 할당된 자원 만큼 VM이 쓰지 않는다면 해당 자원 만큼 시스템이 낭비된다\n하지만 Docker는 하드웨어는 어차피 내꺼를 그대로 쓰니까 그냥 해당 컨테이너가 쓰는 만큼 자원 소모가 된다\n자원을 풀로 써야 하는 경우에도 VM 은 미리 할당 받은 만큼에 성능까지 이지만 Docker는 그냥 내 시스템 자원 100% 를 쓸 수 있다 (물론 이건 설정 가능하다)\n\n\n\n커뮤니티 이미지의 사기성\n\n위 경우에 딥러닝 관련 패키지를 모두 설치하고 cuda 셋팅까지 해놓은 이미지가 Docker Hub에 수도 없이 존재한다\n난 그냥 이미지만 pull 받아서 조금 셋팅 해주면 금방 서비스가 가능하다\n또한 mysql, node.js부터 시작해서 이런 패키지 싹다 설치해놓은 이미지도 금방 찾아 볼 수 있으니 배포에 귀찮음을 덜어낼 수 있다\n\n\n\n시스탬 백업 &amp; 복원:\n\n내가 컨테이너에서 한 모든 내용을 commit 하여 해당 가상환경을 통으로 이미지 를 만드는게 가능하다\n이걸 응용하여 백업 시스템을 구축하는게 가능하다\n\n\n\n작동 원리\n\nDocker는 하나의 커널 안에서 여러 OS를 쓰게된다\n(리눅스 커널의 Cgroup과 NameSpace 기능을 사용)\n\n만약 VM이라면 시스템에서 VM이 실행중인 프로그램을 알 방법이 없다\n허나 Docker는 커널이 같기에 시스템에서 프로그램이 뜨게된다\n\n\n컨테이너 기술은 기본적으로 리눅스에서만 사용가능하다\n\nDocker에서 macOS/Windows 환경에서는 Hipervisor를 통해 가상화 된다\n즉 리눅스가 아닌 환경에서는 VM을 사용하여 리눅스를 가상화 하고 Docker Engine을 통해 컨테이너를 생성하게 된다\n\n\n\n가상머신 (VM)과 차이점\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n특성Docker가상 머신경량화매우 경량화되어 있으며, 커널을 공유하므로 빠르게 시작 가능각 VM은 독립적인 OS 및 커널을 가지고 있어 무겁고 시작 시간이 길다보안커널을 공유하므로, 보안 격리가 VM보다 약할 수 있다각 VM은 독립적인 OS와 커널을 사용하여 높은 보안 격리를 제공운영체제 지원리눅스만 가능다양한 운영체제를 동시에 실행 가능 (Linux, Windows, macOS 등)하드웨어 지원특정 하드웨어 가상화 기능 (예: GPU 가상화) 지원이 제한될 수 있음다양한 하드웨어 가상화 기능을 완벽하게 지원 (예: GPU, 네트워크 카드 등)네트워크 구성네트워크 설정이 복잡할 수 있으며, 대규모 시스템에서는 추가 설정이 필요할 수 있음호스트 PC 와 아예 다른 네트워크 망 취급\n작동 방식\n\n이미지 &amp; 컨테이너 작동 방식\n\n\n\nDockerfile 빌드 하거나, Docker Hub 에서 이미지를 pull 받는다\n\n실제 저장공간에 실행 이미지를 다운받는다\n\n\npull 받은 이미지를 run 하여 컨테이너를 생성한다\n\n이미지를 메모리에 로드 시킨다\n\n\n만들어진 컨테이너의 접속한다\n\n이미지\n\nDoker의 컨테이너 생성은 무조건 이미지 기반, 따라서 base 이미지 없이는 가상화 하지 못함\n즉 내가 ubuntu 사용하고 있는데 내 시스템 기반으로 컨테이너를 생성한다 이딴건 안된다.\n그래서 Docker Hub에 있는 ubuntu 기본 이미지를 pull 받아 작업\n\n\nDocker Hub 에서 다양한 이미지를 gitHub 처럼 호스팅 하고 있음\n예를들어 ubutu 환경에서 Arch Linux 이미지를 pull 받아서 컨테이너를 만들면 Arch Linux 를 가상화 해서 사용 가능한 구조\n만드는 방법\n\nDockerfile 빌드\n컨테이너 commit\n\n\n\nDocker 컨테이너 상의 네트워크\n\n어떤 느낌이냐면 호스트PC 가 공유기 이고 각 컨테이너들이 그 공유기에 물린 컴퓨터들 느낌이다\n\n\nDocker의 각 컨테이너들은 각각 독립적이므로 같은 포트를 쓰는게 가능하다\n\n내부 컨테이너들은 같은 공유기에 연결된 다른 PC 취급인것이다\n각각의 컨테이너가 80 포트로 서버를 호스팅 하는게 가능하다\n\n\n하지만 Docker는 호스트의 네트워크 망을 똑같이 쓴다\n그래서 포트포워딩 과정이 필요하게 된다\n\n하나는 8080 하나는 8081에 메핑 하는등\n\n\n그러니까 컨테이너 내부에서는 같은 포트를 써도 무방한데 호스트 PC는 하나니까 다른 포트로 메핑시켜서 실제로 접속되는 포트는 다르게 설계 해야한다는거\n\n실제\n\n실제 PC 에서 본 네트워크\n\n첫번째로 뜬게 Docker Network 의 게이트웨이\n정확히 말하면 docker0 라는 Docker의 기본 브릿지 네트워크\n두번째가 실제 IP 주소\n\nseoksee@ubuntu:~$ ifconfig\n \ndocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500\n        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255\n        ether 02:42:3d:eb:eb:ff  txqueuelen 0  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n \neth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.0.11  netmask 255.255.255.0  broadcast 192.168.0.255\n        inet6 fe80::2ecf:67ff:fe28:f509  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 2c:cf:67:28:f5:09  txqueuelen 1000  (Ethernet)\n        RX packets 72  bytes 11307 (11.3 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 62  bytes 13242 (13.2 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n        device interrupt 110\n\nDocker Continer 에서 본 네트워크\nroot@e684ca6d5112:/workspace/fitpin_ar_backend\n \neth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255\n        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)\n        RX packets 2191  bytes 3228490 (3.2 MB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 1121  bytes 92127 (92.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\nDocker Network\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n(서버) Docker Network 에 대해\n\n\n\n\n위 컨테이너 상의 네트워크를 Docker Network 구조 라고  부르게 된다\n\n\nDocker의 특성상 하나의 PC 네트워크를 서브넷을 통해 컨테이너 끼리 IP주소를 분배하게 되는데\n이 논리적 Network 기술을 통해 컨테이너 간 통신을 할 수 있개 된다\n\nVeth (Virtual Ethernet Interface)\n\n가상 네트워크 인터페이스\n\n\n실제 컴퓨터에서 렌카드를 통해 이너넷을 연결하듯이 컨테이너 상에서 docker-engine 과 컨테이너를 연결하는 가상의 네트워크 인터페이스를 뜻함\n각각의 컨테이너는 다른 Veth 를 가진다\n\n구조\n\n\n외부 접속은 실제 호스트 PC 와 포트바인딩(포트포워딩)을 통해 이루어진다\n\n예를들어 80 포트를 호스트 8080 포트로 바인딩 시켜 실제 접속은 호스트:8080 이런식으로\n\n\ndocker network create 명령으로 네트워크를 생성하여\n두 컨테이너 간 네트워크를 분리시킨 다면 아예 다른 망이 되어버린다\n\nifconfg 로 살펴 보면 아예 다른 네트워크가 생성되는것을 볼 수 있다\n\n\n\n종류 (Drivers)\n\n네트워크 드라이버 라고도 함\n\nBridge 네트워크\n\nDocker 기본 네트워크 드라이버\n\n\nveth 로 컨테이너의 다른 IP 주소를 할당시키고 호스트 PC 와 포트를 연결 하여 에플리케이션 포트를 외부로 노출 시키는 방법\n\nHost 네트워크\n\nveth를 사용하지 않고 호스트PC IP 주소 그대로 사용 하는방법\n당연하지만 컨테이너 생성시 포트포워딩 과정이 필요하지 않다\n\nNone 네트워크\n\n이름 그대로 네트워크를 사용하지 않는\n그냥 렌 케이블 뺐다 생각함 됨\n\nSwarm 전용\n\nOverlay 네트워크\n\n장점\n\n예를들어 API 서버의 포트 8080을 열었다 치자\nAPI 는 DB와 통신을 하는 구조이다\n이때 API 서버와 DB서버를 각각의 컨테이너로 구성하고 같은  Docker network 를 쓴다 라고 하면\nDB 컨테이너의 외부 접근은 모두 막아둔채 API 서버의 포트만 노출 시킬 수 있게되는 거다\n\nDockerfile\n\ndocker의 이미지를 만드는 스크립트\n\n규칙\n\n파일명은 Dockerfile\n\n다른파일명을 써도 되지만 웬만해선 바꾸지 않는편\n\n\n\n문법\n\n\n                  \n                  참고 \n                  \n                \n\n\n공식문서 참고\n\n\n\n\n\nRUN:\n\n명령어 실행\nRUN 꼭 여러개 써야하는 상황이 아니면 웬만해서는 &amp;&amp; 로 여러 명령어를 실행하는게 좋다\n\nRUN apt install git -y\n \n#Exec 형식으로 사용\nRUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;apt-get install -y git&quot;]\n\n\nWORKDIR:\n\n컨테이너 작업 디렉토리를 변경 (컨테이너 처음 실행시 접속됨)\n베이스에서 WORKDIR 을 지정했을경우 실제 작업경로는 설정된 작업 경로가 맞는데 가끔 프로그램에서 인식 못 할 수 있으므로 다시 정의\n\nWORKDIR &quot;/dir&quot;\n\n\nCMD:\n\n컨테이너가 시작되었을 때 실행할 실행 파일 또는 sh\nDockerfile 내 1회만 사용가능\nRUN 할때 명령을 따로 준다면 이 설정은 무시됨\n\nCMD [&quot;/bin/bash&quot;]\n\n\nENV: 시스템 환경변수 설정\n\n컨테이너 OS 내부에서 관리됨\n다만 .bashrc 파일이 수정되는건 아님\n\nENV LC_ALL=C.utf8\n\n\nARG: 이미지 빌드시 전달할 환경변수\n\n유일하게 FROM 이전에 선언이 가능하다\n따라서 빌드 시 변수로 base 이미지를 바꾸거나 하는게 가능하다\n주의 해야 할 것이 범위가 바로 아래 구문만 적용되므로 변수를 한번 더 쓰리면 ARG 변수명 이렇게 한번 더 선언을 해야한다\n\n# 선언\nARG BASE_VERSION=latest\n# 사용\nFROM ubuntu:${BASE_VERSION}\n \n# 다른데서도 사용하려면 이렇게 또 한번 선언을 해줘야함\nARG BASE_VERSION\nRUN echo &quot;${BASE_VERSION}&quot;\n \n===빌드 시 인자로 이렇게 넘겨주면 됨===\n--build-arg &quot;BASE_VERSION=latest&quot;\n\n\n참고\n\nvelog.io/@tjdwns2243/dockerfile-%EB%AC%B8%EB%B2%95-%EC%9E%91%EC%84%B1%EB%B2%95\ndocs.docker.com/reference/dockerfile/\n\n예제\n# ubuntu 베이스 이미지 사용\nFROM ubuntu:latest\n \nLABEL maintainer=&quot;da864268@naver.com&quot;\nLABEL description=&quot;my-ubuntu&quot;\n \n# 패키지 업데이트 먼저\nRUN apt update\nRUN apt upgrade -y\n \n# 패키지 설치 \nRUN apt install sudo -y\nRUN apt install git -y\n \n===참고: 스크립트로 환경변수 추가하는 예===\n# ENV 로 해도 상관없지만 조건에 따라서 처리하거나\n# TAR 백업시 ENV를 잃는게 싫다면 사용\n# 주의: $PATH 이거 그대로 출력하고 싶다면 \\ 로 이스케이핑\nRUN echo -e &quot;\\n\\n&quot; &gt;&gt; ~/.bashrc &amp;&amp; \\\necho -e &quot;PATH=추가경로:\\$PATH&quot; &gt;&gt;~/.bashrc \\\necho -e &quot;export 키=\\&quot;경로\\&quot;&quot; &gt;&gt;~/.bashrc\n \n# 프로젝트 폴더 구성\nRUN mkdir &quot;/workspace&quot;\nWORKDIR &quot;/workspace&quot;\n컨테이너 root 문제\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nShould I run things inside a docker container as non root for safety?\nDocker 보안 하드닝\n\n\n\n\n대부분에 이미지에서 기본 계정은 root 이다\n아무리 컨테이너 내부라도 하이재킹 같은 보안 문제를 초례할 수 있게때문에 root 계정을 쓰는건 위험하다\n따라서 따로 계정 만들어서 사용하자\n\nRUN adduser --disabled-password &lt;유저이름&gt;\nUSER &lt;유저이름&gt;\nDockerfile에서 쉘\n\n\n이게 각 레이어(명령어) 별로 별도에 쉘이 할당되는 구조라 source 와 같이 쉘에 의존 하는 명령어가 작동하지 않는다\n\n\n만일 그러고 싶다면 &amp;&amp; 연산자를 붙여서 연속적으로 실행하자\nsource ~/.bashrc &amp;&amp; conda init\n\n\nPython docker logs 출력 문제 해결\n\n\n간혹 python 프로그램 실행시 로그가 정상적으로 출력되지 않는 문제가있는데\n\n\n그때는 PYTHONUNBUFFERED 환경변수를 설정하면 된다\n\n\nDockerfile에다 작성해도 되고 추후 컨테이너 생성할때 env 인자로 줘도 상관없다\n===Dockerfile 예===\n# python docker logs 출력대응\nENV PYTHONUNBUFFERED 1\n\n\nDocker-Compose\n\n여러 docker 컨테이너 생성을 별도의 yml 로  정의하여 묶어서 관리 하는 도구\n\n사용 이유\n\n하나의 서비스를 위해서 컨테이너를 여러개를 생성한다 치자\n이런식으로 컨테이너 개수가 늘때마다 입력해야 하는 명령어가 너무나 많아짐으로\ndocker run -d --name wordpress_db \\ --network seunghwan_network \\ -p 3306:3306 \\ -e MYSQL_ROOT_PASSWORD=seosh817 \\ -e MYSQL_DATABASE=seosh817 \\ -e MYSQL_USER=seosh817 \\ -e MYSQL_PASSWORD=seosh817 \\ -v mysql:/var/lib/mysql \\ --restart unless-stopped \\ mysql:8 \ndocker run -d --name seunghwan_wordpress \\ --network seunghwan_network \\ -p 8080:80 \\ --link wordpress_db:mysql \\ -e WORDPRESS_DB_HOST=db:3306 \\ -e WORDPRESS_DB_USER=seosh817 \\ -e WORDPRESS_DB_PASSWORD=seosh817 \\ -e WORDPRESS_DB_NAME=seosh817 \\ --restart unless-stopped \\ wordpress:latest\n\n\ndocker-compose.yml 예제\n\n\n                  \n                  참고 \n                  \n                \n\n\n공식문서 참고\nDocker 도커 컴포즈(Docker compose) - 개념 정리 및 사용법\n\n\n\n\n\n기본예제\nservices:\n  # 서비스 이름\n  fitpin:\n    #이미지 명\n    image: fitpin\n    #이미지를 hub 쪽에서 하는게 아니라 Dockerfile 을 빌드해야 하는 경우\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n    #생성시 커멘드 (run 할때 뒤쪽에 붙는)\n    command: [&quot;executable&quot;, &quot;arg&quot;]\n    ports:\n      - 8080:8080\n\n\nDocker Network 를 구성하는 예제\nservices:\n  # 서비스 이름\n  fitpin:\n    #이미지 명\n    image: fitpin\n    #이미지를 hub 쪽에서 하는게 아니라 Dockerfile 을 빌드해야 하는 경우\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n    #생성시 커멘드 (run 할때 뒤쪽에 붙는)\n    command: [&quot;executable&quot;, &quot;arg&quot;]\n    ports:\n      - 8080:8080\n\tnetworks:\n\t  - default\n\t  - test\n \n# driver 지정 안하면 bridge\nnetworks: test\n\n\nIP 주소 고정 &amp; 폴더를 마운트 하는 예제\n(참고로 이렇게 IP 주소를 고정하려면 기본 네트워크 에서는 안된다)\nservices:\n  server:\n    image: node_test\n    #생성될 컨테이너 이름\n    container_name: server\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n    volumes:\n      - /home/seoksee/home/docker_Compose:/workspace/docker_Compose\n    networks:\n      # 아래서 생성한 네트워크 이름\n      my-networks:\n        ipv4_address: 192.168.0.2\n \n networks:\n  # 네트워크 이름\n  my-networks:\n    ipam:\n      config:\n        - subnet: 192.168.0.0/16\n\n\n참고\n\n최상단에 version을 명시하는 경우도 있는데 이제는 사용안함\n\nswarm 에서 compose.yml 예제\nservices:\n\t# 서비스 이름\n\ttest:\n\t\t# 이미지 명\n\t\timage: test\n\t\tdeploy:\n\t\t\treplicas: 1\n\t\t\tplacement:\n\t\t\t\t# placement-pref 옵션\n\t\t\t\tpreferences:\n\t\t\t\t\t- spread: node.hostname\n\t\t\t\t# constraint 옵션\n\t\t\t\tconstraints:\n\t\t\t\t\t- node.hostname == 노드명\n\t      # 아래서 생성한 네트워크 이름\n\t\tnetworks:\n\t\t\t- my-networks\n \nnetworks:\n\t# 네트워크 이름\n\tmy-networks:\n\t\t# swarm 에서는 무조건 overlay 네트워크 써야함\n\t\tdriver: overlay\n\t\tattachable: true\n\t\tipam:\n\t\t\tconfig:\n\t\t\t\t- subnet: 172.168.0.0/16\n안되는 구문\ncontainer_name | ipv4_address\n\n\nswarm 에서는 ip 주소 고정이 안됨\n\nDocker 컨테이너간 IPC 통신\n\nDocker 컨테이너의 커널은 호스트PC 와 동일하기에\n커널에서 작동하는 named pipe 는 동일하게 사용가능하다\n\n\n컨테이너 생성시 Pipe 를 마운트 시켜준다\ndocker run -d -p 8080:8080 -v \\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine friism/jenkins\n\n\nDocker Swarm\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nDocker Swarm, 제대로 이해하기 - Swarm &amp; Service\nDocker Swarm에서 서비스(Service) 생성하고 다루기\n도커 오버레이 네트워크(Docker Overlay Network) 알아보기 Part 1\n\n\n\n\nDocker 에서 만든 컨테이너 오케스트레이션 툴\n\n\n물리적으로 떨어진 여러 PC 의 컨테이너를 하나의 네트워크로 묶거나 배포 관리 해주는\nLoad Balancing과 장애대응의 목적을 지니고 있다\n기존 Docker 의 단점인 하나의 물리적 PC에서 밖에 안되는 문제를 해결한 것\nK8S(쿠버네티스) 비슷한 것\nDocker의 내장된 기능이긴 하지만 내용이 워낙 방대해서 아예 다른 서비스라고 봐도 무방하다\nswarm 에서는 docker service create 로 컨테이너를 생성하고, compose file 에서는 docker stack deploy로 배포한다\n\n개념\nManager\n\nswarm의 호스트(서버), 연결된 모든 worker(물리적 서버들)들을 관리하고 제어함\nswarm init 하면 해당 호스트 pc가 manager로 생성이 된다\nmanager는 추가를 할 수 있지만 같은 네트워크 에 있는 호스트만  가능한 듯 하다\n\n개방할 포트\n\nTCP-2377:  Manager-Worker 간 통신 포트\nTCP/UDP-7946: Overlay 네트워크 노드 검색\nUDP-4789: Overlay 네트워크 트레픽 송/수신\n\nSwarm Lock\n\nmanager가 여러개 일 경우 다시 manager에 등록한다거나 하면 꼬일 수있어서 Lock 을 걸어 키를 제시하여 재등록 할 수 있도록 한다\n사실상 거의 필수적으로 구성해야 할듯 하다\n\nWorker\n\nmanager 의 종속되어 있는 호스트pc, manager로 부터 컨테이너를 자동으로 할당받고 시작한다\nWorker는 manager가 init으로 발급받은 token 을 통해 docker swarm join으로 해당 manager 의 worker 노드로 등록이 가능하다\n\n구조\n\nService\n\nDocker-Swarm 의 배포단위\n\n\n일반 도커에서는 docker run을 통해 컨테이너를 배포했다면\nSwarm 에서는 docker service create를 통해 컨테이너를 배포한다\n다만 아예 run 명령을 쓸 수 없는 게 아니라 swarm 을 통해 분산 관리를 할 때 사용하는 더 큰 집합의 단위이다\n즉 컨테어너(run) 은 service 의 부분 집합 관계가 된다\n하나의 Service는 여러 컨테이너를 생성할 수 있지만 이미지는 하나 이다\n\n즉 같은 이미지로 여러개의 컨테이너를 생성한다는 것\n\n\n\nReplica\n\nService 의 컨테이너 생성 수\n\n\nDocker Swarm이 에초에 Load Balancing과 장애대응 위해 나온 것이라 서비스 생성 시 하나의 서비스는 하나의 이미지를 가지고 연결된 모든 Node 들에게 같은 컨테이너를 분산 시켜 서비스 할 수가있다\n기본적으로 각 연결된 node(연결된 pc)들의 시스템 상황(램, 점유율) 등을 고려하여 자동으로 어떤 호스트에 배포 될 것이지를 결정하지만 placement-pref 옵션과 Constraints 옵션을 통해 직접 지정할 수 있다\n이 기능을 원하지 않는다면 Replica 수를 1로 해야한다 안그러면 중복으로 컨테이너를 생성한다고 오해 할 수있다\n\nNode\n\nswarm의 연결된 모든 호스트를 관리하는 명령어, 단위\n\n\nswarm Manager 는 등록된 Worker 들을 해당 명령어를 통해 확인 가능하다\n\nOverlay 네트워크\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n도커 오버레이 네트워크(Docker Overlay Network) 알아보기 Part 1\n\n\n\n\nswarm 의 service 에서는 기존에 bridge 네트워크 사용이 불가하다\n각 service 컨테이너들은 Overlay 네트워크를 거쳐서 통신하게 된다\n\n포트 개방 문제\n\nbridge 네트워크 에서는 포트 개방 안해도 호스트 pc 와 컨테이너 간 통신에는 문제가 없었다\n그러나 Overlay 네트워크는 동일 호스트 pc 에서 구동중인 서비스(컨테이너) 라도 포트를 개방 해야만 호스트 pc-컨테이너 간 통신이 가능하다\n동일 호스트pc 내에 서비스(컨테이너)들 끼리는 자유롭게 통신 가능하지만 외부   서비스(컨테이너) 와의 통신 또한 포트를 개방 해야 한다\n\nDocker Hub\n\nGitHub의 Docker 버전이라고 볼 수 있다\n\n\n이곳에서 이미지를 pull 받아 작업하고\n나만의 이미지를 구축하여 push 하여\nDocker Hub 원격 저장소에 저장하는 구조 이다\n\n설치 에디션\n\nDocker Desktop:\n\nGUI 버전. Docker Engine 및 Docker Compose 가 포함됨\nWindows 의 경우 Docker Desktop으로만 설치 가능\n\n\nDocker Engine:\n\nCLI 버전. GUI 를 사용하지 않는 OS는 이걸로 밖에 설치 불가\nDocker Compose는 따로 설치\n\n\n"},"스터디/Express.js-정리":{"title":"Express.js 정리","links":[],"tags":[],"content":"livereload\n\n호스트 중인 HTML에 변경사항 발생시 자동 리로드 해주는 미들웨어\n"},"스터디/File-IO":{"title":"File IO","links":[],"tags":[],"content":"파일 관련 용어 정리\nUTF-8\n\n유니코드를 표시하는 인코딩 방법 중 하나\n\n\n1~4 바이트 가변 인코딩 방식\n아스키에 경우 1바이트로 그대로 인코딩 다른 언어는 2, 3 바이트로 인코딩 하는 방식\n\nStream\n\n파일이 이어지는 데이터들의 집합\n\n\n파일 입출력을 할때 모든걸 한번에 다운로드 받을 수는 없으니 이것을 나눠서 읽어야 하는데 이때 단위 흐름을 Stream 이라고 함\n\n바이너리\n\n영상, 음원들이 저장되는 텍스트\n0101 이런 이진 형태\n이미지, 음원 등이 가진 바이너리의 규칙을 파악하여 이게 음악인지 이미지인지 파악하기도 함\n\n00000 26 50 44 46 2D 31 2E 3G \n\n\n보통 바이너리 뷰어에선 이런식으로 2진수를 16 진수로 변환하여 보여준다\n\nBase64\n\n이런 바이너리 형태를 보여주는 텍스트 규칙\n보통 텍스트 에디터에서는 바이너리를 이런 형태로 표시해준다\n\n��\u0012��K�����\u0003�A��H�\n\n\nBase64를 지원 하지 않는경우 그걸 UTF-8 이런 걸로 변환해서 보여주기도 함\n"},"스터디/Git-스터디":{"title":"Git 스터디","links":["CLI-프로그램-명령어/Git-명령어-정리"],"tags":[],"content":"\n\n                  \n                  같이보기 \n                  \n                \n\n\nGit 명령어 정리\n\n\n\n기본 원칙\n\n무조건 git 프로젝트를 열면\n\n패치, 풀\n\n\nPull 먼저 하고 Push\n\nRebase\n\n말 그대로 브랜치에 베이스를 다시 잡아 병합해주는\n타겟 브랜치에 각 커밋 별로 병합 오류를 해결해야 되므로 다룰때 주의 해야함\n\n\n\nBefore\n\n\n\nRebase\n\n\n\n설명\n\nsub에 커밋을 해당 Rebase 지점인 커밋6 이후에 커밋이 오도록 수정한다\n즉 원래는 new file: test.c 커밋에서 분기가 나뉜걸 커밋6 에서 나뉜 분기로 바꾸는\n\n\n\nmaster에 있는 커밋 로그를 그대로 복사\n\n\n내 브랜치에서 생성된 커밋을 제일 위로 끌어 올린다\n\n\nsub 브랜치에서는 커밋3, 커밋4를 추가했는데 왜 커밋3 만 남을까?\n\n커밋3과 커밋4에서 건드린 파일이 똑같아서 커밋3 에서 충돌 해결한 내용을 커밋4 에도 똑같이 적용 시켰다\n그래서 커밋4와 커밋3의 변경사항이 동일해지게 된다. 그래서 먼저 해결한     커밋3만 커밋로그로 남게됬다\n만일 커밋3 충돌과 커밋4 충돌을 다른 변경사항으로 적용하면 커밋3 이후 커밋에 커밋4가 기록된다. 즉 마지막 커밋이 커밋4 가 된다\n\n\n\n같은 상황에서 merge는 이지랄이 난다\n\n\n\nRebase 되돌리기\n\nRebase를 사용하면 커밋 로그가 왜곡 되기때문에 커밋 로그를 보며 되돌아 가는건 불가능 하다\n대신 git reflog [브랜치] 명령을 사용하여 작업로그를 보며 reset 해야 한다\n93084be (HEAD -&gt; master) master@{0}: rebase (finish): refs/heads/master ...생략\n072d4ac master@{1}: commit: 커밋6\n\n해당 로그에서 rebase 뭐시기 이전 로그 인 072d4ac 로 리셋하면 된다\ngit reset --hard 072d4ac\n\n\n실전 사용시 주의사항\n\n타겟 브랜치에 각 커밋 별로 병합 오류를 해결해야 되는게 많이 리스크\nRebase 이후 push 할때는 git push -f 하여 강제로 해야한다\n\n이게 원격과 로컬의 커밋 로그가 달라 서로 충돌이 나버려서 pull 을 시도하는 순간 병합 커밋이 생겨 버린다\n그래서 로컬 변경사항이 어차피 무조건으로 원하는 방향인거니  강제로 push 하는것이다\n\n그러나 push -f의 고질적 문제인데 누군가 rebase 진행 이전에 커밋을 가지고 있는 상태인 경우 git reset --hard 원격저장소/브랜치로 원격 브랜치로 reset 시켜야 한다\n또한, 만약 그 사이 누군가가 해당 브랜치에 커밋했을경우 그 커밋은 씹히는 문제가 있으니 이때는 rebase 취소하고 pull 받은 후 다시하자\n\n\n그러니 master 브랜치나 다른사람랑 같이쓰는 브랜치에서는 되도록 쓰지말고 나만 쓰는 브랜치에서만 쓰는게 맞는듯 하다\n\n\n\nRebase 사용할 때\n\nsub 브랜치에서 master 브랜치 변경사항 병합할 때는 Rebase\nmaster 에서 Rebase 병합된  sub 브랜치를 병합할때는 Merge\n\nSquash Marge와 함께 사용 권장\n\nsub 브랜치에서 master 브랜치를 Rebase 하기전 master 을 Squash Merge 으로 병합 후 Rebase 하기\n이러고 Rebase시 발생하는 병합오류는 모두 sub 브랜치 껄로 변경사항 저장하면 모든 문제 해결이다\n일반 merge로 하면 병합커밋 이전에 병합도 처리해야 하니 좀 많이 복잡해짐\n\nsub 브랜치에서 수정한 파일이 master에는 삭제됬을때\n\n일단 병합 하라고 뜨는데 이런 파일은 보통 수정이 안된다\n\ncli로 작업할때도 충돌 났다고 뜨지만 막상 파일을 열어보면 별다른 충돌 해더가  없다\n\n\n이때 그냥 그걸 스테이징 하고 변경사항 유지 클릭하면 됨\n\ncli에서는 그냥 git rebase --skip\n\n\n\n3-way merge\n\n이게 일반적으로 부르는 병합\n\nfast-forward\n\nmain 브랜치는 가만히 있는데 sub 브랜치에만 커밋이 있는경우\n일반 merge 처럼 커밋하나 만들어서 병합하는게 아니라 Rebase 됨\ngit merge --no-ff로 일반 병합으로 하게 할 수도 있음\n\nsquash merge\n\n커밋은 병합하지 말고 변경사항만 가져온다\n그러고 새로 수정하여 커밋하는 방식\n그렇기에 git에서는 이 브랜치가 병합되었다는 사실이 남지 않는다\n그냥 일반 커밋으로 올라 오게되니까\n\n브랜치 간 파일 구조 동일하게 하기\n문제상황\n\n예를들어 main 에서 a 파일만 건드린 상태고 sub 에서는 a, b 파일을 건드린 상태\n\n이 상황에서 sub 가 main을 병합 한다면 충돌이 나지 않는 b 파일은 문제 없이 커밋 됨\n근데 b 파일은 쓸모 없기 때문에  b 파일은 커밋하면 안되는 상황이라 main 브랜치와 파일 상태를 동일하게 맞출 필요가 있는거\nreset 이나 rebase 로 처리한다면 커밋 로그가 달라지기에 나중에 문제가 생기기 때문\n\n\n\n해결\n\nsub 브랜치의 파일 구조를 main 과 동일 하게\n# main브랜치와 병합\n# 모든 충돌을 sub 브랜치 변경사항으로 유지 시키기\ngit merge --strategy=ours main\n \n# main 브랜치에 파일 구조를 덮어쓰기\ngit checkout main -- .\n\n"},"스터디/Github-Actions-정리":{"title":"Github Actions 정리","links":[],"tags":[],"content":"\n\n                  \n                  참고 \n                  \n                \n\n\nActions 마켓\n깃허브의 CI툴인 Actions의 문법 간단 정리\n\n\n\n기본 문법 정리\n병렬처리\njobs:\n  build-docker:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        platform:\n          - linux/amd64\n          - linux/arm/v7\n          - linux/arm64/v8\n \n===이렇게 변수로 참조===\n${{ matrix.platform }}\n\n이렇게 수행하면 platform 내부 변수들을 통해 github actions 가 각각 Runner를 할당 받아서 빌드한다\n\nSPA 배포시\n\n404.html 다운로드 후 index.html과 같은 폴더에\n메인 index.html head 테그에 해당 구문 추가\n\n&lt;script type=&quot;text/javascript&quot;&gt;\n\t// Single Page Apps for GitHub Pages\n\t// MIT License\n\t// github.com/rafgraph/spa-github-pages\n\t// This script checks to see if a redirect is present in the query string,\n\t// converts it back into the correct url and adds it to the\n\t// browser&#039;s history using window.history.replaceState(...),\n\t// which won&#039;t cause the browser to attempt to load the new url.\n\t// When the single page app is loaded further down in this file,\n\t// the correct url will be waiting in the browser&#039;s history for\n\t// the single page app to route accordingly.\n\t(function (l) {\n\t\tif (l.search[1] === &quot;/&quot;) {\n\t\t\tvar decoded = l.search\n\t\t\t\t.slice(1)\n\t\t\t\t.split(&quot;&amp;&quot;)\n\t\t\t\t.map(function (s) {\n\t\t\t\t\treturn s.replace(/~and~/g, &quot;&amp;&quot;);\n\t\t\t\t})\n\t\t\t\t.join(&quot;?&quot;);\n\t\t\twindow.history.replaceState(null, null, l.pathname.slice(0, -1) + decoded + l.hash);\n\t\t}\n\t})(window.location);\n&lt;/script&gt;\n\n추가설정\n\nreact-router 사용시\ncreateBrowserRouter([...각종라우팅], {basename: process.env.PUBLIC_URL})\n\n이외에 url을 /a/b 이런식으로 따로 참조하고 있는경우 앞에 process.env.PUBLIC_URL/a/b\nhtml은 %PUBLIC_URL% 로\npackage.json에 해당 구문 추가\n&quot;homepage&quot;: &quot;[원본 도메인주소]&quot;,\n\n따로 커스텀 도메인을 설정했다면 404.html 에 pathSegmentsToKeep 값을 0로 설정 (디폴트)\n\n\n\n릴리스 생성시 작동하는 job\n해당하는 job 에 해당 구문 추가\n - `if: startsWith(github.ref, ‘refs/tags/’)“\n릴리스 생성시 결과물 릴리스에 배포법\n\naction-gh-release 사용\n\nGithub Actions 릴리스(태그) 생성시에만 작동시키기\non:\n    push:\n\t\ttags:\n\t\t\t- &quot;**&quot;\nGithub Page에 커스텀도메인 적용\n\ndig [원래 페이지 도메인] +noall +answer -t A\n\n리눅스:\n그냥 저렇게 입력, 없으면 apt로 설치\n윈도우:\nwww.isc.org/download/ 에서 BIND9 설치 (Current-Stable 로)\n\n\n거기서 나온 ip 전부를 dns에 등록\nGithub Page 사이트에서 Custom domain에 등록한 domain 입력\n\n\n적용에는 1시간 이상 걸릴 수 있음\n\nGithub Runner\n\n원래Actions 자체에서 VM 을 할당하는데 이를 내 자체 서버를 사용 할수있음\n이를 통해 커밋 이후에 내 서버에 뭔 갈 하거나 이런게 가능해짐\n\nRunner를 사용하지 않다면 actions에서 ssh 연결을 통해 조작할 수도 있음\n\n\n\nGitHub 조직 Dcoker 컨테이너 패키지 배포\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n컨테이너 레지스트리 작업\n\n\n\n조직 토큰 설정하기\n\ngithub.com/organizations/fit-pin/settings/personal-access-tokens\n\n개인 토큰 발행\n\ngithub.com/settings/tokens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n범위설명필요한 권한read:packagesGitHub Packages에서 패키지 다운로드 및 설치읽기write:packagesGitHub Packages에 패키지 업로드 및 게시쓰기delete:packagesGitHub Packages에서 패키지 삭제관리자repo패키지 업로드 및 삭제(write:packages 또는 delete:packages 포함)쓰기 또는 관리자\n해당 개인 토큰으로 Docker에 로그인 하고 push 하기\necho &lt;개인토큰&gt; | docker login ghcr.io -u Lseoksee --password-stdin\ndocker buildx build --push -t ghcr.io/&lt;조직명&gt;/&lt;이미지명&gt; .\n기타\n\n보니까 Github ghcr.io 라는 Docker Hub 와 같은 이미지 공유 사이트를 운영하는듯\n개인 리포지토리나 조직에 이미지를 ghcr.io로 Push 하는 구조를 가짐\n\nGitHub Actions 사용 예제\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nGitHub Actions를 사용하여 패키지 게시 및 설치\n\n\n\n\n단일 플렛폼\nname: Publish Docker image\n \non:\n  push:\n    branches: \n        - main\n  workflow_dispatch:\n \njobs:\n  # fitpin-ar-backend 빌드\n  publish_fitpin-ar-backend:\n    runs-on: ubuntu-latest\n    env:\n      PLAT_FORM: linux/amd64,linux/arm64\n      IMAGE_NAME: fitpin-ar-backend\n      FILE_LOCATION: ./AR-BackEnd/\n    permissions:\n      contents: read\n      packages: write\n      attestations: write\n      id-token: write\n \n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4.2.1\n \n      - name: Log in to the Container registry\n        uses: docker/login-action@v3.3.0\n        with:\n          registry: ghcr.io\n          username: ${{ github.actor }}\n          password: ${{ secrets.ACTIONS_TOKEN  }}\n \n      - name: Docker Setup Buildx\n        uses: docker/setup-buildx-action@v3.7.1\n        with:\n          driver: docker-container\n          driver-opts: |\n            image=moby/buildkit:master\n          platforms: ${{ env.PLAT_FORM }}\n        \n      - name: Build and push ${{env.IMAGE_NAME}} Docker image\n        id: push\n        uses: docker/build-push-action@v6.9.0\n        with:\n          platforms: ${{ env.PLAT_FORM }}\n          context: ${{ env.FILE_LOCATION }}\n          push: true\n          tags: ghcr.io/fit-pin/${{ env.IMAGE_NAME }}:latest\n          cache-from: type=registry,ref=ghcr.io/fit-pin/${{ env.IMAGE_NAME }}:latest\n          cache-to: type=inline\n \n      - name: Generate artifact attestation\n        uses: actions/attest-build-provenance@v1\n        with:\n          subject-name: ghcr.io/fit-pin/${{ env.IMAGE_NAME}}\n          subject-digest: ${{ steps.push.outputs.digest }}\n          push-to-registry: true\n\n멀티 플렛폼 + 병렬처리\n기본 제공 Runner의 용량 문제로 병렬로 할당 받아 쓰는것이 유리함\n"},"스터디/HTTP-쿠키--and--세션":{"title":"HTTP 쿠키 & 세션","links":[],"tags":[],"content":"쿠키\n\n서버에서 어떤 데이터를 브라우저에 저장하고, 다시 활용하는 데이터\n\n서버는 클라이언트에 응답을 할때 Set-Cookie 해더를 설정할 수 있다\nSet-Cookie: &lt;cookie-name&gt;=&lt;cookie-value&gt;; Domain=&lt;domain-value&gt;; Secure; HttpOnly\n\n그럼 응답 받은 브라우저는 자신의 Cookie 저장소에 저장하게 된다\n향후 Http 요청을 할 때 Cookie 해더를 설정하여 설정된 모든 Cookie를 해더에 담아서 요청을 하게 된다.\nCookie: yummy_cookie=choco; tasty_cookie=strawberry\n\n종류\n\nSession Cookie\n\n만료 시간을 명시하지 않으면 브라우저 종료될 때 삭제됨\n\n\nPersistent Cookie:\n\nExpires, Max-Age 속성에 명시된 기간이 지나면 삭제됨, 이건 클라이언트가 처리하므로 클라이언트 시간이 기준\n\n\n\n속성\n\nHttpOnly:\n\n클라이언트가 Document.cookie 를 통해서 접근할 수 없고 요청 시 해더에서만  cookie 값에 접속할 수 있도록 설정한 쿠키\n\n\nSameSite:\n\nNone: 도메인을 검증하지 않아 다른 사이트 모두가 해당 쿠키에 접근이 가능 (https 만)\nLax: 브라우저 상 이동 (a 테그 등) 에서만 접근가능 (fetch 나 이런걸로는 안됨)\nStrict: SameSite 도메인만 허용 가능\n\n\nDomain:\n\n해당 도메인에 접속된 사이트인 경우에만 전송하도록\n만약 서버에서 설정된 Set-Cookie가 클라이언트와 다른경우 서드파티 쿠키로 분류된다\n이걸 이용해 특정 페이지에 접속한 유저 에 대해 이벤트 쿠폰을 뿌리거나 하는게 가능하다\n\n\nPath:\n\n보통은 / 로 설정되어 하위 경로에도 전부 쿠키가 포함되지만 이걸 하위로만 제한 하는것도 가능하다\n\n\nSecure:\n\nHTTPS 에서만 쿠키를 포함하겠다는 뜻\n\n\n\n특징\n\n최대 4kb 저장 가능\n\n로컬 스토리지와 차이점\n\n로컬 스토리지는 기본적으로 브라우저 클라이언트가 값을 쓰고 읽기위해 만들어 졌고\n쿠키는 서버가 브라우저에 값을 쓰고 읽기위해 만들어 졌다.\n\n물론 로컬 스토리지와, 쿠키 모두 반대 용도로도 사용할 수 있는 방법은 있지만 핵심 설계는 이렇다.\n특히 쿠키는 단순히 클라이언트에서만 사용할 용도면 로컬스토리지로 사용이 권해지는데 이유는 HTTP 요청 시 불필요한 쿠키 해더가 설정되는 꼴이기 때문이다\nHTTP 에서는 서드파티 쿠키 설정은 불가하다\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n사이트 간 요청 위조 (CSRF)\nMDN 문서\n브라우저 쿠키와 SameSite 속성\n\n\n\n정확히 말하면 클라이언트와 서버가 다른 호스트에 존재한다면 서버에서 쿠키설정은 불가하다\nCSRP 공격이라고 해서 쿠키를 탈취해서 사용자 정보를 얻을 수 있으므로 브라우저 측에서 서드파트 쿠키(웹 페이지와 다른 호스트) 를 차단한다\n해결 방법은 HTTPS 를 할당 받아서 SameSite 값을 None 으로 설정할 수 밖에 없다\n세션\n\n쿠키에 특성을 사용하여 서버측에서 일련의 상태를 유지시키는 기술\n\n주로 로그인 유지 &amp; 관리에 사용 된다\n과정\n\n\n먼저 세션을 사용할 주소에 접속을하면 서버는 고유한 session ID 를 생성한 뒤\n자동로그인을 구현하고 싶다면 Persistent Cookie로 쿠키를 설정하고 아니면\nSession Cookie로 설정하여 응답을 보낸다\n(이때 session ID를 PK로 사용하여 식별하고 싶은 값을 테이블등에 보관한다)\n그 후 다른 페이지들에 접속이나 API 요청을 보낼 때  cookie 해더에 session ID를포함하여 요청이 진행된다\n요청받은 서버는 session ID가 테이블에 있는지 판단하고 있으면 정상응답을 보낻다\n만약 session ID가 존재하지 않는 경우 로그아웃이나, 재발급을 하게 설계가 된다\n\n단점\n대용량 세션 처리\n모든 세션에 session ID 테이블 형태로 저장하는데 이게 양이 적을때는 인덱싱이 크게 느리지 않지만 대규모 사이트에 경우 인덱싱에 속도가 느려질 수 있다\n세션 탈취\n어떠한 방법으로든 session ID 만 확보하면 사실상 로그인 된 것과 다름이 없다\n어치피 검증은 session ID 만 하니까\n\n대응책\n\n한 유저에 대해서 session ID를 하나만 사용하도록 즉 다중 로그인에 대한 제한을 둠\nSession ID 재발급 주기를 빠르게 설정함 (탈취해도 많은 작업은 못하도록)\n브라우저나 접속된 OS 등이 담겨있는 User-agent 해더를 검증하여 Session ID를 사용하고 있는 원래 사용자에 User-agent와 같은지를 검증\n\n\n"},"스터디/IPC-통신":{"title":"IPC 통신","links":[],"tags":[],"content":"dar0m.tistory.com/233\n\n프로세스와 프로세스간 데이터를 주고받을 수 있는 기술\n\n\n한마디로 로컬에서 작동하는 Rest API, 소켓 통신 이라고 볼 수 있다\n\n사용하는 이유\n\n로컬에서도 어차피 Rest API 로 통신해도 되지만 태생이 웹 통신이라 비효율 적인 요소가 많다 (오버해드 문제)\n보통에 로컬 통신에 작동하는 API(윈도우 API같은)는 해당  API 개발 언어에 맞춰 프로그램을 작성 해야 하는 문제가 있다.\n\n종류\n\n명명된 파이프 (Named Pipe)\n\n특수 정의된 파일을 이용한 통신 방식\n\n\n공유 메모리\n\n프로세스 간 메모리 영역을 공유하는 방식\n가장 빠르지만 구현이 어렵고 각 언어별 최적화가 뒤죽박죽\n그리고 요청/응답 구조 처리를 할 수 없음\n\n\n메시지 큐\n소켓\n\n일반적인 TCP 소켓 통신 또는 UDS 통신\n\n\nRPC\nProcess Creation | Child Process\n\n메인 프로그램에서 하위 프로그램을 실행하여 부모-자식 프로세스간 통신\n\n\n\n\n가장 쓸만 한건 소켓이랑Named PIPE 인듯\n\n사용 예\n\nDBMS 에서 서버PC 와 DB 엔진 간 통신\n웹서버와 WAS 와의 통신\n마이크로서비스 아키텍처:\n\n모든 서버와 서버를 독립적으로 배포하는 구조\n근데 이제 하나의 PC에서 실행 하는걸 곁들인\n\n\n\n개념\nFile Descriptor\n\n\n                  \n                  참고자료 \n                  \n                \n\n\ndev-ahn.tistory.com/96\n\n\n\n\n\n프로세스가 파일을 open 하면 부여받는 0이 아닌 정수값\n\n프로세스에서 열린 파일의 목록을 관리하는 테이블의 인덱스\n\n\n기본 할당 File Descriptor:\n\n\n0: STDIN_FILENO = 표준 입력\n\n\n1: STDIN_FILENO = 표준 출력\n\n\n2: STDIN_FILENO = 표준 애러\n\n즉 뭔 소리냐 콘솔에 입/출력을 할 수 있는게 이 File Descripor 가 담당하기 때문인 것이다\n\n\n\n\nFile Descriptor 의 Index는 OS 가 관리한다\nFile Table 은 각각의 프로세스가 관리하게 된다.\n\nFile Descriptor Index 는 프로세스에서 동일하지만, 거기에 연결되는 Table 은 달라서 같은 File Descriptor Index 라도 다르게 작동하는 것이다\n\n\n\nUnix domain socket (UDS)\n\n유닉스, 리눅스 계열 운영체제 사용가능\n\n\nIPC 통신 소켓\n서버, 클라이언트 형태로 구성되며 해당 언어가 UDS 통신 지원하면 뭘로 구성하든 상관 X\n일반 네트워크 소켓을 사용해도 구현 가능하나 네트워크 스택, 경로 결정 과정 등을 패스 즉 최적화가 좋음\n\n작동방식\n\n기본적인 사용법은 네트워크 소켓과 비슷\nTCP 소켓에 경우 IP 를 이용하여 통신하지만 UDS에 경우 특수 파일 하나를 만들고 그 파일에 접근 하는 프로세스끼리 연결 시켜서 하는 통신\n서버 생성시 .sock같은 파일로 커넥션이 이루어지기 때문에 만일 통신이 종료된 이후 해당 파일을 지우지 않으면 오류걸림\n\n파이프 통신(Pipe)\n\nIPC 통신에서 가장 기초적으로 쓰이는 단방향 통신\n\n\n\n한 프로세스의 Output 이 다른쪽에 Input 으로 쓰이거나 그 반대로 쓰는\n양방향 통신을 하려면 위 사진처럼 파이프를 2개 생성해야함\n\n익명 파이프 (Anonymous Pipe)\n\n일반적인 파이프\n\n\n통신할 프로세스가 명확해서 이름이 필요 없는 경우에 사용하는\n주로 아래 나오는 프로세스 생성 (Process Creation) 같은 부모-자식 간 통신에 쓰인다\n\n망명된 파이프 (Named Pipe)\n\n윈도우 및 여러 운영체제 사용가능\n\n\n커널 에서 특수 정의된 파일을 이용한 통신 방식\n파일 입출력에 로우레벨에서 작동되는 것 (File Descriptor)\n연결을 파일 시스템에서 관리된다\n이름이 있기 떄문에 전혀 관계 없는 프로세스끼리의 통신도 가능하다\n\n작동방식\n\n\nUDS와 통신 방식은 유사하다. 서버가 특정 파일을 만들고 클라이언트가 해당 파일에 접근하면 커넥션이 이루어지고 실제 통신은 메모리를 통해 통신하게 된다.\n\n\n서버:\n\nNamed Pipe 생성\n클라이언트 연결 대기\n데이터 전송 및 수신\n파이프 닫기\n\n\n\n클라이언트:\n\nNamed Pipe에 연결\n데이터 전송 및 수신\n파이프 닫기\n\n\n\n이때 File Descriptor 가 사용되는데\n\n서버쪽에서 할당한 파일을 역 추적해서 해당 파일을 사용하고 있는 프로세스 찾고\n있으면 해당 프로세스와 연결 해주는 식\n\n\n\n프로세스 생성 (Process Creation)\n\n다른 말로 자식 프로세스 (Child Process) 라고도 함\n\n\n상위 프로그램에서 다른 외부 프로그램을 실행하여 해당 프로그램과 통신하는\nFile Descriptor 와 파이프 통신에 기초한다\n\n작동원리\n\n기본적으로 실행하는 프로그램은 열린파일로 간주된다 즉 File Descriptor 가 할당 된다는 소리\n\nFile Descriptor 상속\n\n메인 프로세스는 파이프를 열고\n서브 프로세스의 나의 File Descriptor를 상속시킨다\n즉 서브 프로세스가 나의 File Table을 참조할 수 있게 끔 해서 입/출력이 메인 프로세스에서 가능하도록 설계한다\n\nI/O 통신\n\n메인 프로그램이 서브 프로세스에 데이터를 전달하려면, 파이프의 쓰기 디스크립터에 데이터를 쓰고\n서브 프로세스는 해당 파이프의 읽기 디스크립터를 통해 읽고\n"},"스터디/Kubernetes-(K8S)-스터디":{"title":"Kubernetes (K8S) 스터디","links":["스터디/Docker-스터디"],"tags":[],"content":"\nDocker 와 같은 가상 컨테이너 기술 + 컨테이너들을 배포하는 환경\n\n\n\n                  \n                  같이보기 \n                  \n                \n\n\nDocker 스터디\n공식문서 (한글 문서는 오래된 것이므로 영어로 보자)\n\n\n\n\n수 많은 가상 컨테이너 들을 관리하고 자동화(오케스트레이션) 할 필요가 있어짐\n컨테이너 생성 + 자동화 + 배포 모든 것을 합친거라고 보면 됨\n\n사용하는 이유\n\nDocker 같은 경우에는 컨테이너를 생성 하는 것 까지는 어느정도 자동화 가능하긴 한데 딱 거기 까지밖에 불가 하다\nk8s에 경우 컨테이너 내부에 시스템을 업데이트 하거나 롤백 하는 것을 편하게 할 수 있다\n또한 한쪽에 컨테이너의 트래픽이 몰렸을 경우 이를 적절히 분산처리 하는것도 가능하다\n\n구조\n클러스터 (Cluster)\n\n클러스터란 실행중인 컨테이너의 집합을 의미 한다\n\n\n재대로 정의하면 컨테이너화된 프로그램을 실행하는 노드의 집합이다\nk8s를 실행중이라는 의미는 클러스터를 실행 중 이다 이렇게도 표현 가능\n\n\n\n컨트롤 플레인 영역 (Master Node): 클러스터의 제어영역\n노드 (Worker Node): 컨테이너 내부 프로그램을 실행하는 역할\n\n컨트롤 플레인 (Master Node)\n\n클러스터의 상태를 관리하고 제어\n\n\n전체적인 구조는 이런 느낌이다\n\n클라이언트 요청을 처리하는 API 서버 ←&gt; 그 API 가 클러스터의 상태를 저장하는 DB\n\n\n\nscheduler\n\n서비스 담당\n\n\nAPI 서버와 통신하는 컴포넌트\n각각의 노드의 시스템 자원을 관리한다\n새로 생성된 pod 를 감지하고 적절한 노드에게 배포하는 기능\n\n여기서 pod란 컨테이너 애플리케이션의 최소 단위\n\n\n\ncontroller-manager\n\n클라이언트 담당\n\n\n컨트롤 프로세스 통합 관리\n예가 클러스터 관리자 페이지 느낌이라고 생각하면 됨\nCloud 컨트롤러 매니저: AWS, GCP 등 클라우드 서비스와 통신하는\nkube 컨트롤러 매니저:  api server 와 통신하여 현재 scheduler 정보와 etcd 에 저장된 정보를 대조하여 클러스터  상태 갱신\n\nkube api server\n\nAPI 서버 담당\n\n\netcd, scheduler 데이터 불러와서 controller-manager 에 적절한 응답을 반환\n\netcd\n\nDB 담당\n\n\n클러스터 상태 저장\n\n노드 (Worker Node)\n\n컨테이너를 포함한 각종 관리 시스템을 추가\n\nkubelet\n\napi server에 노드에 리소스 상태를 보고\n\nCRI(Container Runtime Interface)\n\nkubelet 이 다양한 컨테이너 런타임과 통신할 수 있도록 설계한 인터페이스\n컨테이너 구현체가 Docker 뿐이 아니라 다양하게 존재 하기 때문에 모든 컨테이너 구현제가 kubelet 와 통신 할 수 있도록 인터페이스를 만든 것\n\nkube-proxy\n\n네트워크 프록시 &amp; 로드벨런싱 담당\n\nPod\n\nk8s에서 배포할 수 있는 가장 작은단위\n한개 이상의 컨테이너, 스토리지, 네트워크를 가진다\nDocker 가 컨테이너 단위 라면, k8s 는 이 Pod 단위 이다\n\n워크로드\n\nk8s를 통해 배포할때 가장 작은 단위인 pod 와 이를 실행하는구성요소들의 추상화\n\nReplicaSet\n\n실행되는 Pod를 지정된 수만큼 복제하여 실행시킨다\n일반적으로 Deployment 를 정의하면 자동으로 관리\n노드가 어떤 문제가 발생하면 여기에 저장된 복제본을 pop 하고 새로운 복제본을 생성하는 작업을 한다\n\nDeployment\n\n실행되는 Pod 를 모니터링하는 프로세싱\nDeployment 를 정의하여 새로운 ReplicaSets을 만들거나 기존 Deployment를 제거\n\n서비스\n\nPod 집합에서 실행중인 에플리케이션을 외부로 노출 시키는 방법\n쉽게 말해서 외부IP 주소를 할당한다\n\n주요 개념\n컨테이너 런타임\n\n쿠버네티스 자체의 컨테이너 런타임 가지고 있지 않는다\nDocker Engine 이나 기타 런타임을 설치 해야한다\n\nDesired State\n\n관리자가 서버를 배포할 때 원하는 상태를 선언 하는 방식\n\n\n\nDesired State: 원하는 상태\n\n구체적으로 웹서버 몇게 띄울지, 포트 몇번 쓸지 결정\n\n\n\nKubernetes Object Spec\n\n모든 k8s 의 Object를 정의하는 YML 파일\n대충 이런느낌\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot;]\n        args: [&quot;while true; do sleep 30; done;&quot;]\n        ports:\n        - containerPort: 80\n설치 에디션 (배포 도구)\nkubeadm\n\n아마 기본 설치 방법인듯\n\n\n쿠버네티스 공식 클러스터 관리 도구\n\nKubespray\n\n상용서비스에 적합한 오픈소스 프로잭트\n\nkops\n\n향후 kubeadm 을 대체 하기위한 프로젝트 인듯?\n"},"스터디/NGINX-스터디":{"title":"NGINX 스터디","links":["스터디/네트워크-통신"],"tags":[],"content":"\n리버스 프록시 지원 웹 서버\n\n구성파일\n\n/etc/nginx/nginx.conf:\n\nnginx 서버에 서버 설정파일\nTLS 나 각종 웹서버 설정 구성\n\n\n/etc/nginx/sites-enabled/default:\n\n\n실제 서버에 굴러갈 파일이나 리버스 프록시를 구성 하는 기본설정 파일\n\n\nnginx.conf에 import 되어 굴러가게 된다\n# nginx.conf 예\ninclude /etc/nginx/sites-enabled/*;\n\n\n\n\n설명\n\n기본적으로 설정 파일은 nginx.conf 이거 하나다\n하지만 웹서버 자체에 서버 설정과, 프로젝트를 구분 하기위해 sites-enabled 에 따로 파일을 분리하게 된다\n따라서 프로젝트를 추가하거나 프록시 구성은 sites-enabled에 파일을 추가해서 하자\n\nnginx.conf 설정\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n공식문서\nNginx 예시/샘플 설정 공유 및 설정 시의 주의사항\n\n\n\nHTTP\n\n최상위 블록으로 해당 블록에서 설정한 값은 server 블록에 상속된다\n즉 HTTP, server 블록에 설정값은 상호 호환 된다\n\nclient_max_body_size &lt;크기&gt;\n\n\n최대 Body로 보낼 수 있는 사이즈\n\n\n파일 업로드 할 때 해당 크기 설정을 고려해야 함\nhttp {\n    client_max_body_size 5M;\n}\n\n\ntimeOut 설정\n\nngix 에 기본 timeout 값은 120초 이다 따라서 바꾸고 싶으면 설정하자\n\n\n\n일: d, 시: h, 분: m, 초: s\n# 최대 읽기 시간\nproxy_read_timeout 10m;\n# 최대 쓰기 시간\nproxy_send_timeout 10m;\n# 최대 연결 시간\nproxy_connect_timeout 10m;\n\n\nserver\n\n하나의 서비스를 정의하는 블록\nserver 블록은 중복해서 사용가능하다\n\nlisten 80 &amp; listen [::]:80\n\n해당 서비스를 80 포트로 호스팅 하겠다는 뜻\nlisten [::]:80에 경우 IPv6 이다\n다른 server 블록과 포트가 동일한 경우에도 server_name 이나 location 값이 다르면 상관 없다\n\ndefault_server\n\n만약 listen 80 default_server 이렇게 지정하면\n정의되지 않은 location 이나 server_name 으로 접속하는 모든 요청은 여기서 처리된다\n\nroot\n\n프로젝트 디렉토리를 구성한다\n만약 웹서버로 사용할 시 로드될 HTML 과 CSS 등에 프로젝트 경로를 정의한다\n\nserver_name\n\n해당 server에 접속 도메인을 정의한다\n이걸로 도메인 리버스 프록시를 구성 할 수 있다\n\nlocation &lt;경로&gt;\n\n해당 server에 접속 URL을 정의한다\n\n내부 속성\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n엔진엑스 프록시 모듈\n\n\n\n\nproxy_pass &lt;URL | IP&gt;: 해당 경로로 리버스 프록시를 설정한다\nproxy_set_header &lt;요청해더&gt;: proxy_pass 쪽 서버에 요청해더를 설정한다.\nadd_header &lt;응답해더&gt;: proxy_pass 쪽 서버에서 응답을 리턴한 경우 추가적인 해더를 추가한다\n\n주의 사항\n\n홈 경로인 / 로 설정하지 않는경우 즉 다른 경로로 지정하는경우, proxy_pass 주소와, location 주소 끝에는 반드시 /를 붙여야 한다.\n안그러면 proxy_pass 가 위치한 서버에서 uri에 홈주소를  /주소/ 로 인식해 버린다\n\nlocation /주소/ {\n\tproxy_pass 주소:8080/;\n}\n예약 변수\n\n$host: 도메인 주소\n$request_uri: 요청한 uri (/abc/def)\n\n리버스 프록시 구성\nURL 라우팅\n\n\nserver 블록 내부에 location 부분을 수정한다\nlocation / {\n\tproxy_pass http://172.168.0.2;\n}\n \nlocation /api {\n\tproxy_pass http://172.168.0.3;\n}\n\n/로 접속하면 http://172.168.0.2로 가지고\n/api로 접속하면 http://172.168.0.3 으로 가진다\n\n\n\n도메인 라우팅\n\n\n분활 하려는 도메인 만큼 server 블록을 추가하고 이렇게 수정\nserver {\n\tlisten 80;\n\t#ipv6\n\tlisten [::]:80;\n \n\t# 이 server_name 이 핵심!!!\n\tserver_name domain1.kro.kr;\n\tlocation / {\n\t\tproxy_pass http://172.168.0.2:8080; \n\t}\n}\nserver {\n\tlisten 80;\n\t#ipv6\n\tlisten [::]:80;\n \n\t# 이 server_name 이 핵심!!!\n\tserver_name domain2.kro.kr;\n\tlocation / {\n\t\tproxy_pass http://172.168.0.4:8080; \n\t}\n}\n\ndomain1.kro.kr 로 접속하면 http://172.168.0.2:8080 로 가지고\ndomain2.kro.kr 로 접속하면 http://172.168.0.4:8080 로 가진다\n만약 따로 처리안하면 IP로 접속하는 요청은 재일 위에 선언한 http://172.168.0.2:8080\n\n\n\n구성 팁\nIP 직접 접근 차단하기\n\n도메인 리버스 프록시를 설정한 경우 도메인으로만 접근하게 하고 싶을 때 사용\n이렇게 하면은 무작위 IP 스켄을 통한 DDOS 공격을 어느정도 차단 가능하다\n참고로 return 444 는 서버가 아예 없는거 처럼 어떤것도 반환하지 않는다.\n# ip 직접 접근 차단\nserver {\n    listen 80 default_server;\n\t#ipv6\n\tlisten [::]:80 default_server;\n \n\t# 응답 ERR_EMPTY_RESPONSE\n    return 444;\n}\n\n\nWAF (Web Application Firewall) 설정하기\n\n\n                  \n                  구성법 \n                  \n                \n\n\ndocs.nginx.com/nginx/admin-guide/dynamic-modules/nginx-waf\n\n\n\n\n포트 개방으로 인해 DDOS 공격이나 RCE 공격을 막기위해 nginx 자체적으로 방화벽을 제공해준다\n\nWebSocket 허용하기\n\n\nlocation 블록에 다음 구문을 추가한다\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection &quot;upgrade&quot;;\n \n# nginx 에서 websocket이 1분 간격으로 커넥션을 중단함으로 이걸 30일로 설정함\nproxy_read_timeout 30d;\nproxy_send_timeout 30d;\n\n\n로그 보기\n\n접속로그: var/log/nginx/access.log\n애러로그: /var/log/nginx/error.log\n만약 위치를 바꾸고 싶다면 nginx.conf 에 error_log 부분과 access_log 부분 수정\n\n서비스 제어\nsystemctl 사용\n# 시작\nsudo systemctl start nginx\n \n# 중지\nsudo systemctl stop nginx\n \n# 재시작\nsudo systemctl restart nginx\nnginx CLI 사용\n\nDocker 환경같이 systemctl가 기본제공되지 않는경우\n\n# 시작\nnginx\n \n# 중지\nnginx -s stop\n \n# 재시작\nnginx -s reload\nHTTPS 인증받기\n\n\n                  \n                  참고 \n                  \n                \n\n\nHTTPS\n\n\n\n알아둘 점\n\n인증서는 도메인 마다 각각의 인증서를 발급받아야 한다.\n\n인증서 발급하기\n\n대부분에 기업에서는 DigiCert 인증서를 구매해서 사용하지만 나는 아주 영세하기 때문에 무료 발급 기관인 Let’s Encrypt에 인증서를 사용할꺼임\n해당 발급 기관에서는 Certbot이라는 자동 발급툴을 제공하고 있어서 그걸 사용할꺼\n\nCertBot 설치 및 발급\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nNginx+certbot으로 https 요청 설정하기\n\n\n\n\nLet’s Encrypt 인증서는 대략 2달간 유효 하다\n그래서 재발급 하는 스케줄링을 추가로 구성해 주는것이 좋다\n\nsudo apt install python3-certbot-nginx\n \nsudo certbot --nginx -d &lt;도메인 이름&gt;\nnginx.conf 구성\n\n\n발급을 마치면 발급한 &lt;도메인 이름&gt; 과 Server 블록 server_name 이 일치하는 곳에 인증서및 SSL 설정이 자동으로 구성된다\n\n\n그걸 적절하게 바꿔서 사용하자\n\n\n적절히 바꾼 예\nserver {\n    listen 443 ssl;\n\t#ipv6\n    listen [::]:443 ssl;\n \n\tserver_name &lt;도메인&gt;;\n\tlocation / {\n\t\tproxy_pass http://172.168.0.3:8080;\n\t}\n \n\t# HTTPS 인증 부분\n    ssl_certificate /etc/letsencrypt/live/fitpin-web-back.kro.kr/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/fitpin-web-back.kro.kr/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n}\n \n \n# HTTP 로 접근시 HTTPS 로 이동하게\nserver {\n\tlisten 80;\n\tlisten [::]:80;\n\t\n\tserver_name &lt;도메인&gt;;\n \n\treturn 301 &lt;도메인&gt;$request_uri;\n}\n\n\ncrontab 재발급 스케줄링 등록\n\n\n매달 30일 3시에 재발급\n0 3 30 * * certbot renew --renew-hook=&quot;sudo systemctl restart nginx&quot;\n\n\n\n인증서 관리\n\n인증서 확인\n\nsudo certbot certificates\n\n\n\n인증서 삭제\nsudo certbot delete\n\n만약 삭제 후 재발급 하려면 위에 nginx.conf에 HTTPS 인증과 관련된 부분을 말끔하게 삭제해야 함 안그러면 발급할때 오류난다\n"},"스터디/React-Native-정리":{"title":"React-Native 정리","links":[],"tags":[],"content":"시작하기\naidenarea.tistory.com/entry/React-Native-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0-Windows\n안드로이드\n\n안드로이드 SDK 설치\n해당경로로 환경변수 설정\nC:\\Users\\[유저이름]\\AppData\\Local\\Android\\Sdk\\platform-tools\n\n\n/android 폴더에 local.properties 파일 만들고 해당 내용 입력\nsdk.dir = C:\\\\Users\\\\[유저이름]\\\\AppData\\\\Local\\\\Android\\\\Sdk\n\n\n\n실제 기기로 테스트 시 포트오류 나면\nadb devices\nadb reverse tcp:8081 tcp:8081\n\n자바 버전 변경\n\n가끔 특정 라이브러리 쓸 때 자바 버전 안 맞아서 빌드 오류 나는 경우가 있음\n다른 자바를 설치하고 gradle.properties에 설치된 경로로 해당 구문 추가\n\n# 자바 버전 변경\norg.gradle.java.home=C:\\\\Program Files\\\\Java\\\\jdk-17\nHTTP 요청 시 주의사항\n\nReact-Native 내부 모듈은 Node.js와 구현이 다르다\n(React-Native에서 Node 내부 모듈을 따로 구현한 거라)\nurl로 요청할 때 그 주소가 redirect 되는 주소라면 Node.js와는 다르게 redirect를 따라가지 못해 커넥션이 종료된다.\n만일 서버에서 URL 구성 시 끝에 / 가 붙어있으면 요청 시 / 까지 붙어야 redirect 하지 않는데 이점 주의해서 요청해야 한다\n\n// 주소 매핑이 / 로 되어있을 떄 \nfetch(&quot;http://localhost/&quot;);\nMultipart/form-data 요청\nconst formData = new FormData();\n// formData 파일 전송\nformData.append(&#039;form-data 키값&#039;, {\n\turi: &quot;파일 uri&quot;,\n\tname: &quot;파일이름&quot;,\n\ttype: &quot;MME 타입&quot;,\n});\n \n// formData 일반 텍스트\n formData.append(&#039;form-data 키값&#039;, &#039;값&#039;);\n\n이렇게 만든 formData 를 fetch body 값에 넣어야 한다\n"},"스터디/SSE-(Server-Sent-Event)":{"title":"SSE (Server Sent Event)","links":[],"tags":[],"content":"\n서버 → 클라이언트 통신\n\n특징\n\n서버쪽에서 클라이언트에게 지속적으로 스트리밍 하는 기술\nHTTP 1.1 에 Persistent Connections Chunked Transfer Encoding, 기술 이용\n\nPersistent Connections:\n\n지속연결기능 (Connection: keep-alive 해더)\n\n\nChunked Transfer Encoding:\n\n서버에서 데이터를 여러번 걸쳐서 보낼 수 있게 해주는 기술\n\n\n\n\n\nWebSocket 대비 장 단점\n\n결론적으로 SSE는 최적화, 안정성, 베터리 소모 부분에서 우월함을 가진다.\n따라서 굳이 구현하려는게 서버쪽에서만 데이터를 보내줘야하는 서비스는 SSE 로 구현 하자\n\n단점\n\n서버 쪽 단방향 통신\n\n요청 시 데이터를 보내는건 가능\n\n\n바이너리 데이터 전송 불가 (Base64로 변환 해야함)\n\n장점\n\nWebSocket 은 전용 서버를 따로 구축해야한다는 문제가 있는데\nSSE는 해더 설정 한번이면 끝\n서버와 연결이 끊길이 3초마다 재접속 시도\n최적화 배터리 소모량 부분에서 좋음\n\n구현\n서버\n\n보낼 때 아예 응답을 보내서 커넥션을 종료하면 안되고\n일부 데이터 스트림을 보내주는 함수를 쓰자\n\n해더 구성\n\nContent-Type을 text/event-stream 으로 설정 하기만 하면됨\nGET /SSE HTTP/1.1\nContent-Type: text/event-stream\nCache-Control: no-cache\nConnection: keep-alive\n\nConnection: keep-alive: 커넥션 유지\nCache-Control: no-cache 브라우저에서 케싱 하지 않게끔\n\n\n\n메시지 구조\nevent: message\\n\\n\n \ndata: 보낼데이터\\n\\n\n \n \n\nevent: 기본값 (message)\n\n클라이언트에 보낼 이벤트 지정\n클라이언트는 해당 이벤트에 맞춰 이벤트 리스너를 구성\n\n\ndata:\n\n해당 부분에 보낼 메시지 입력하면됨\nUTF-8 만 지원\n\n\n\\n\\n:\n\n줄바꿈을 두 번 한걸 표현하기 위해서 둠\n실제 코드짤때 표현 한 것처럼 \\n\\n 으로 줄바꿈 두번 해야함\n\n\n\n클라이언트\n\n여기선 js로 구현\n\n// SSE 클라이언트 객체 생성\nconst eventSource = new EventSource(`/sse`);\n \n// 서버로부터 데이터가 오면\neventSource.addEventListener(&quot;message&quot;, (e) =&gt; {\n    console.log(e.data);\n});\n \n// 만일 서버쪽에서 event: notification 으로 설정하면 해당 리스너에서 받을 수 있다\n/* eventSource.addEventListener(&quot;notification&quot;, (e) =&gt; {\n    console.log(e.data);\n}); */\n \n// connection되면\neventSource.addEventListener(&quot;open&quot;, (e) =&gt; {\n    console.log(&quot;SSE 서버연동&quot;);\n});\n \n// error 나면\neventSource.addEventListener(&quot;error&quot;, () =&gt; {\n    if (e.readyState === EventSource.CLOSED) {\n        console.log(&quot;서버 중단&quot;);\n    }\n});\n사용 예\n\n주식 실시간 호가\n\n클라이언트 쪽에서는 종목 코드 하나만 보내주면 따로 보낼 데이터는 없기에\n\n\n실시간 채팅\n\n채팅방 아이디를 식별자로 가져서 요청하여 모든 메시지를 서버쪽에서 스트리밍 받게 설계\n\n\n"},"스터디/SSH-(Secure-Shell-Protocol)":{"title":"SSH (Secure Shell Protocol)","links":["스터디/암호화","스터디/네트워크-통신","리눅스/리눅스-명령어-정리"],"tags":[],"content":"\n원격자 PC 셀 에 접근하기 위해 사용되는 프로토콜\n\n\n텔넷에 키 교환(암호화) 알고리즘을 도입한\n\n용도\n\n데이터 전송:\n\ngithub에 push 작업을 할 때 SSH 를 통해 파일을 전송하게 된다.\n\n\n원격제어:\n\nLinux 같은 shell로 돌아가는 pc의 원격제어\n\n\n\nSSH 포트포워딩 (터널링)\n\nSSH로 하는 터널링 기능\n\n\n22 포트를 통해 SSH로 접속을 하여 해당 PC의 내트워크 망으로 접속 할 수있게되는 기능\n서버쪽에서는 22번 포트만 열어놓고 DB나 기타 서비스 포트는 외부 접속을 차단한체 SSH 포트 포워딩을 통해서만 접근하게 하여 보안적인 이득을 챙길 수 있다\n\nLocal Port Forwarding\n\n특정 포트로 구동되는 서비스를 localhost 포트로 매핑시키는\n\nssh -L 1234:Host:8080 user@Host\n\nSSH 호스트에 8080 포트로 서비스 되고있는 프로그램이 있을 때\n이를 localhost:1234 에 메핑 시킨다, 즉 나는 localhost:1234 에 요청하면 SSH 가 포트 포워딩을 통해 Host:8080 포트에 전달 되게 한다\nHost 본인 한테 굴러가는 서비스 뿐만 아니라 해당 네트워크망에 있는 모든 서비스를 포트포워딩 하는것도 가능하다\n\n포트포워딩 하려는 서비스가 다른 pc여도 상관 없다는 것(Host PC 망 안에 포함만 되있으면 됨)\n\n\nDBMS 에 SSH 터널 기능이 바로 이것이다\n\nRemote Port Forwarding\n\nSSH 연결하려는 서버를 클라이언트로 사용해서 연결하는\n\n\n22번 포트를 포함한 모든 포트가 막혀있어도 나가는 연결이 막힌게 아니기에\n서버가 클라이언트가 되면 해결 된다는 것이다\n즉 클라이언트에서 SSH 데몬이 실행중 이고, 클라이언트에 포트가 막히지 않는다면 이다.\n\nServer\n ssh -R 8585:127.0.0.1:80 192.168.1.200\n\n192.168.1.200는 클라이언트 ip 주소\n127.0.0.1:80 에서 실행중인 서비스를 클라이언트는 8585 로 매칭 시켜준다\n\nClinet\n# localhost:8585 에 get 요청 날리는 스크립트\ncurl localhost:8585\n\n서버가 연결한 경우 localhost:8585 요청하면 서버의 127.0.0.1:80 포트로 요청이 가게 된다\n\nDynamic port forwarding\n\nSSH를 아예 프록시 서버로 쓰는\n\nssh -D 9090 -N -f user@host\n\n9090 포트에 SOCKS 프록시를 생성해준다.\nSOCKS 프록시를 사용하는 모든 응용 프로그램이 SSH 서버에 연결\n아예 해당 네트워크 망에 1:1로 메핑 시켜준다.\n완전히 해당 네트워크에 접속시켜주는 VPN 같이 아예 외부 ip 주소까지 해당 네트워크로 바뀐다\n이걸통해 해당 서버 공유기 설정페이지 가는게 가능\n실제 구현은 여기서\n"},"스터디/VPN-서버-구성--and--비교":{"title":"VPN 서버 구성 & 비교","links":["API-키-목록"],"tags":[],"content":"프로토콜 비교\nOpenVPN\n\n가장 보편적으로 많이쓰는 프로토콜\n\n\n구성은 IKEv2 보다 간단하여 사용하기 쉬움\n별도 앱을 설치하여야 함\nWireGuard 보다 속도 및 안정성에서 성능이 더 나쁨\n\nVPN 터널에서에 TCP-on-TCP 사용 때문인듯\n\n\n\nWireGuard\n\n최근에 유행하는 프로토콜\n\n\n구성은 OpenVPN 보다는 좀까다롭긴 함\nOpenVPN 과 마찬가지로 별도 앱 필요\nVPN 터널에서 UDP만 사용하여 호환성 문제를 야기할 수 있지만 터널 자체에서 TCP 패킷을 UDP로 다시 캡슐링하기 때문에 HTTP 같은 TCP 통신에도 문제는 없음\n문제점 중 하나가 클라이언트 구성 파일을 돌려 쓰면 ip가 중복됨, 이 때문에 같은 구성 파일로 접속한 두 클라이언트간 통신이 안되는 문제가 있음 클라이언트 각각에 구성파일을 만들어 줘야함\n어떤 프로토콜 보다 속도 및 안정성에서 우월함\n\nIKEv2 (네이티브)\n\n안드로이드 및 윈도우에서 네이티브로 사용가능한 프로토콜\n\n\n별도 앱을 설치할 필요는 없어서 간편하긴 함\n다만 초기 구성이 매우 까다롭고 Docker 에서는 잘 작동안함\n다운로드 속도는 WireGuard 와 거의 동일하거나 조금 나쁨, 근데 업속도가 개판임\n\n설치\nOpenVPN\n\nUDP 1194 포트로 포트포워딩 필요\n\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nOpenVPN Server Auto Setup Script\n\n\n\n\n다음 스크립트로 openvpn 설치\nwget git.io/vpn -O openvpn-install.sh\n./openvpn-install.sh\n\n저장된 .ovpn 파일을 연결하려는 클라이언트들에게 복사\n\nroot/ 폴더에 저장되서 권한 잘 해서 해야함\n\n\n\nWireGuard\n서버 구성\n\n\n                  \n                  참고 \n                  \n                \n\n\nDocker 사용\n\n\n\n\nUDP 51820 포트로 포트포워딩 필요\n\n\n\nroot 사용자로 변경\nsudo su\n\n\n패키지 설치\napt install wireguard\n\n\n키 생성\n# 서버 측\nwg genkey | sudo tee /etc/wireguard/privatekey_server | wg pubkey | sudo tee /etc/wireguard/publickey_server\n \n# 클라이언트 측\nwg genkey | sudo tee /etc/wireguard/privatekey_client | wg pubkey | sudo tee /etc/wireguard/publickey_client\n\n\n기본 NIC(네트워크 인터페이스) ID 확인\nip -o -4 route show to default | awk &#039;{print $5}&#039;\n\n\n/etc/wireguard/wg0.conf 파일 생성 후 아래 내용 입력\n[Interface]\nAddress = 10.0.0.1/24 # VPN NIC 용 CIDR\nSaveConfig = true\nPostUp = iptables -t nat -A POSTROUTING -o &lt;확인된 NIC ID&gt; -j MASQUERADE\nPostDown = iptables -t nat -D POSTROUTING -o &lt;확인된 NIC ID&gt; -j MASQUERADE\nListenPort = 51820\nPrivateKey = 서버 비밀키 (/etc/wireguard/privatekey_server 에서 확인)\n\n[Peer]\nPublicKey = 클라이언트 공개키 (/etc/wireguard/publickey_client 에서 확인)\nAllowedIPs = 10.0.0.2/32  # 클라이언트와 동일하게\n\n\n\nIP 포워딩 활성화\n\n\n/etc/sysctl.conf 파일 맨 아래 추가\nnet.ipv4.ip_forward=1\n\n\n\n적용\nsysctl -p\n\n\n\n\nVPN 서버 시작\n# 시작\nsudo wg-quick up wg0\n \n# 서비스 자동시작 등록\nsudo systemctl enable wg-quick@wg0\n\n\n(추가) 서비스 관리\n# VPN 시작\nsudo wg-quick up wg0\n \n# VPN 종료\nsudo wg-quick down wg0\n \n# VPN 상태 확인\nsudo wg\n \n# ufw 사용\nsudo systemctl stop wg-quick@wg0\n\n\nufw 사용 중 이라면\n\n\n/etc/sysctl.conf 에 PostUp, PostDown 부분을 이렇게 수정\nPostUp = ufw route allow in on wg0 out on &lt;확인된 NIC ID&gt;; iptables -t nat -A POSTROUTING -o &lt;확인된 NIC ID&gt; -j MASQUERADE\nPostDown = ufw route delete allow in on wg0 out on &lt;확인된 NIC ID&gt;; iptables -t nat -D POSTROUTING -o &lt;확인된 NIC ID&gt; -j MASQUERADE\n\n\n\n51820 포트 열기\nufw allow 51820/udp\n \n# 적용\nufw disable\nufw enable\n\n\n클라이언트 구성\n\n\n프로그램 설치 (Windows)\n\n\n클라이언트 구성\n[Interface]\nPrivateKey = 클라이언트 비밀키 (/etc/wireguard/privatekey_client 에서 확인)\nAddress = 10.0.0.2/32\nDNS = &lt;DNS 서버1&gt;, &lt;DNS 서버2&gt;\n\n[Peer]\nPublicKey = 서버 공개키 (/etc/wireguard/publickey_server 에서 확인)\nAllowedIPs = 0.0.0.0/0\nEndpoint = &lt;서버 IP주소&gt;:51820\n\n\n\n(참고) 클라이언트 추가 방법\n\n단순 연결은 문제가 없지만 하나의 클라이언트 구성으로 여러 클라이언트가 접속하면 IP가 똑같아서 연결된 클라이언트 간에 통신에 장애가 발생함\n즉 클라이언트 별로 새로운 구성을 생성해야 이런 문제가 없음\n\n\n\n서버:\n\n\n기존 서비스 종료\nsudo wg-quick down wg0\n\n\n추가 클라이언트 키 발급\nwg genkey | sudo tee privatekey | wg pubkey | sudo tee publickey\n\n\n/etc/wireguard/wg0.conf 파일 수정\n===추가===\n[Peer]\nPublicKey = 위에서 생성한 publickey 내용\nAllowedIPs = 10.0.0.3/32  # 이전과 다른 IP로 (대역은 똑같이)\n\n\n\n서비스 다시시작\nsudo wg-quick up wg0\n \n# 파일 정리\nrm -rf privatekey publickey\n\n\n\n\n클라이언트:\n\n\nInterface → PrivateKey 부분만 수정 (나머지 부분은 이전에 생성한것과 똑같이)\n[Interface]\nPrivateKey = 서버에서 생성한 privatekey 내용\n===다른값 똑같이===\n\n\n\n\n\nIKEv2\n\nUDP 500,4500 포트로 포트포워딩 필요\n\n서버 구성\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nHow to Set Up an IKEv2 VPN Server with StrongSwan on Ubuntu 22.04\n\n\n\n\n\n필요 패키지 설치\nsudo apt install strongswan strongswan-pki libcharon-extra-plugins libcharon-extauth-plugins libstrongswan-extra-plugins libtss2-tcti-tabrmd0\n\n\n인증서 발급\n# 인증서 생성 폴더 생성\nmkdir -p ~/pki/{cacerts,certs,private}\n \n# ca-key 생성\npki --gen --type rsa --size 4096 --outform pem &gt; ~/pki/private/ca-key.pem\n \n# ca-key 인증 생성\npki --self --ca --lifetime 3650 --in ~/pki/private/ca-key.pem \\\n    --type rsa --dn &quot;CN=&lt;인증서 이름&gt;&quot; --outform pem &gt; ~/pki/cacerts/ca-cert.pem\n \n# server-key 생성\npki --gen --type rsa --size 4096 --outform pem &gt; ~/pki/private/server-key.pem\n \n# server-key 인증 생성\npki --pub --in ~/pki/private/server-key.pem --type rsa \\\n    | pki --issue --lifetime 1825 \\\n        --cacert ~/pki/cacerts/ca-cert.pem \\\n        --cakey ~/pki/private/ca-key.pem \\\n        --dn &quot;CN=&lt;서버 도메인 또는 IP 주소&gt;&quot; --san &lt;서버 도메인 또는 IP 주소&gt; \\\n        --flag serverAuth --flag ikeIntermediate --outform pem \\\n    &gt;  ~/pki/certs/server-cert.pem\n \n# 결과물 옮기기\nsudo cp -r ~/pki/* /etc/ipsec.d/\nsudo mv /etc/ipsec.conf{,.original}\n\n\nStrongSwan 수정하기\n\n\n/etc/ipsec.conf 수정 (추가 설명은 위 참고자료 보자)\nconfig setup\n    charondebug=&quot;ike 1, knl 1, cfg 0&quot;\n    uniqueids=no\n\nconn ikev2-vpn\n    auto=add\n    compress=no\n    type=tunnel\n    keyexchange=ikev2\n    fragmentation=yes\n    forceencaps=yes\n    dpdaction=clear\n    dpddelay=300s\n    rekey=no\n    left=%any\n    leftid=@&lt;서버 도메인 OR ip&gt;\n    leftcert=server-cert.pem\n    leftsendcert=always\n    leftsubnet=0.0.0.0/0\n    right=%any\n    rightid=%any\n    rightauth=eap-mschapv2\n    rightsourceip=10.10.10.0/24\n    rightdns=&lt;DNS 주소1&gt;,&lt;DNS 주소2&gt;\n    rightsendcert=never\n    eap_identity=%identity\n    ike=chacha20poly1305-sha512-curve25519-prfsha512,aes256gcm16-sha384-prfsha384-ecp384,aes256-sha1-modp1024,aes128-sha1-modp1024,3des-sha1-modp1024!\n    esp=chacha20poly1305-sha512,aes256gcm16-ecp384,aes256-sha256,aes256-sha1,3des-sha1!\n\n\nleftid: 도메인 주소가 아닌 IP 주소인경우 앞에 @ 없에야함\n\n\n\n/etc/ipsec.secrets 수정:\n: RSA &quot;server-key.pem&quot;\n&lt;사용자 이름&gt; : EAP &quot;&lt;암호&gt;&quot;\n\n\n\n\n\nstrongswan-starter 재시작\nsudo systemctl restart strongswan-starter\n\n\nufw 사용 중 이라면\n\n\n포트 허용\nsudo ufw allow OpenSSH\nsudo ufw allow 500,4500/udp\n\n\n기본 NIC(네트워크 인터페이스) ID 확인\nip -o -4 route show to default | awk &#039;{print $5}&#039;\n\n\n/etc/ufw/before.rules 수정\n===*filter 위쪽에 추가===\n*nat\n-A POSTROUTING -s 10.10.10.0/24 -o &lt;확인된 NIC ID&gt; -m policy --pol ipsec --dir out -j ACCEPT\n-A POSTROUTING -s 10.10.10.0/24 -o &lt;확인된 NIC ID&gt; -j MASQUERADE\nCOMMIT\n\n*mangle\n-A FORWARD --match policy --pol ipsec --dir in -s 10.10.10.0/24 -o &lt;확인된 NIC ID&gt; -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1361:1536 -j TCPMSS --set-mss 1360\nCOMMIT\n\n\n===*filter 내용 아래쪽에 추가===\n-A ufw-before-forward --match policy --pol ipsec --dir in --proto esp -s 10.10.10.0/24 -j ACCEPT\n-A ufw-before-forward --match policy --pol ipsec --dir out --proto esp -d 10.10.10.0/24 -j ACCEPT\n\n\n\n/etc/ufw/sysctl.conf 수정\n===맨 아래 추가===\nnet/ipv4/ip_forward=1\nnet/ipv4/conf/all/accept_redirects=0\nnet/ipv4/conf/all/send_redirects=0\nnet/ipv4/ip_no_pmtu_disc=1\n\n\n\n적용\nsudo ufw disable\nsudo ufw enable\n\n\n클라이언트 구성\n\n\n윈도우:\n\n구성된 서버에서 /etc/ipsec.d/cacerts/ca-cert.pem 파일 내용을 복사하여 ca-cert.pem 파일을 만듬\n윈도우 검색에서 컴퓨터 인증서 관리를 검색 해서 들어가기\n왼쪽 항목에서 신뢰할 수 있는 루트 인증 기관 우클릭\n\n모든작업 → 가져오기 → 파일 선택 → 모든파일로 변경 하여 ca-cert.pem 파일을 선택하여 추가\n\n\n이후 VPN 연결 설정 가서 구성한 대로 입력해 주면 됨 (이때 VPN종류는 자동으로)\n\n\n\n안드로이드\n\n안드로이드 연결 시 StrongSwan 앱을 쓸 필요는 없음\n특이하게 이쪽은 인증서 없이도 잘 작동하더라\n\n\nVPN 종류: IKEv2/IPSec MSCHAPv2\nIPSec 식별자: 사용안함으로 그대로 두기\nIPSec CA 인증서: 서버 인증 안 함 으로두기\nIPSec 서버 인증서: 서버로 부터 수신\n사용자 이름 &amp; 비밀번호: 설정한 걸로 잘 수정하기\n\n\n\n(추가) No-ip DDNS 설정하기\nno-ip 도메인 발급\n\nno-ip 홈페이지에서 도메인 발급\nDDNS Key 생성\n생성한 DDNS Key 잘 간직하기\n\nno-ip ddns 등록\n\n\n                  \n                  참고 \n                  \n                \n\n\n공식가이드\n이미 발급한 DDNS 정보\n\n\n\n\n\n서버에 DCU 설치 (동적으로 도메인 바꿀 서버를 생성)\nwget --content-disposition www.noip.com/download/linux/latest\ntar xf noip-duc_3.1.1.tar.gz\ncd /home/&lt;유저&gt;/noip-duc_3.1.1/binaries &amp;&amp; sudo apt install ./noip-duc_3.1.1_arm64.deb\n\n\n설치가 완료되면 해당명령으로 DDNS 서버 활성화 해보기\n\n스크립트가 포그라운드로 활성화 되는거라 서비스 등록해야함\n\nnoip-duc -g all.ddnskey.com --username &lt;DDNS Key username&gt; --password &lt;DDNS Key passwd&gt;\n\n\n서비스 등록하기\n\n\n/etc/systemd/system 에 서비스 파일 만들기\ncd /etc/systemd/system\nsudo nano\n\n\n해당 파일에 이렇게 작성하고 no-ip-ddns.service로 저장\n[Unit]\nDescription=no-ip-ddns\n\n[Service]\nExecStart=noip-duc -g all.ddnskey.com --username &lt;DDNS Key username&gt; --password &lt;DDNS Key passwd&gt;\nType=simple\nRestart=always\n\n[Install]\nWantedBy=default.target\n\n\n\n\n\n서비스 시작하기\n# 서비스 파일 리로드\nsudo systemctl daemon-reload\n# 부팅시 서비스 자동시작\nsudo systemctl enable no-ip-ddns.service\n# 서비스 시작\nsudo systemctl start no-ip-ddns.service\n# 로그확인\nsudo systemctl status no-ip-ddns.service\n\n\n서버 설정하기\n\n설정파일 위치: /etc/openvpn/server/server.conf\n설정적용하기\n sudo systemctl stop openvpn-server@server.service\n sudo systemctl stop openvpn.service\n \n sudo systemctl start openvpn-server@server.service\n sudo systemctl start openvpn.service\n\n"},"스터디/Wireshark-스터디":{"title":"Wireshark 스터디","links":[],"tags":[],"content":"TLS 복호화\nwww.comparitech.com/net-admin/decrypt-ssl-with-wireshark/\n\nSet a Windows environment variable, Configure Wireshark to decrypt SSL 부분 참고\n\nEttercap\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nblog.naver.com/taeyoun795/220766607479\n\n\n\n\nARP 스푸핑 공격/탐지 프로그램\n\n원리\n자신의 MAC 주소를 게이트웨이 MAC 주소로 위조하여 다른 피해자들이 아웃바운드 패킷을 위하여 ARP를 수행한 경우 공격자 PC의 MAC 주소를 반환하게 하여 패킷이 공격자에게 올 수 있도록 함\n일종의 ARP 프록시 인건데 악용하면 스푸핑인거고\nFake ARP Reply Attack\n위와는 반대로 인바운드 패킷에 대한 스푸핑인데\n송신 패킷이 라우터에 도착한 경우 송신 받을 목적지 MAC 주소를 찾기위해\n내부 망에 대한 ARP를 수행한 경우\n공격자PC가 먼저 ARP Reple을 보내 패킷을 가로챔"},"스터디/gRPC-통신":{"title":"gRPC 통신","links":[],"tags":[],"content":"cla9.tistory.com/177\n\n원격 프로시저 호출 시스템\n\nA 서버에서 만든 함수를 B 클라이언트 에서 사용하는\n\n\n단방향 / 양방향 모두 지원한다\nHTTP/2 기반\n\nRPC (Remote Procedure Call)\n\n원격 프로시저 호출\n\n\n다른 주소나 공간의 함수나 프로시저를 원격으로 실행할 수 있게 하는 기술\n\n함수: 입력에 따른 아웃풋이 발생 즉 Return 값을 반환 하는\n프로시저: 아웃풋이 없이 그저 수행에 초점을 맞추는\n\n\nIPC 통신의 일종이라 볼 수 있다\n사실상 REST API 서버도 웬만한 웹 프레임워크는 API 호출 범위를 함수로 잡기 때문에 수행 동작에는 큰 차이가 없지만 내부 로직이 다르다\n\n\ngRPC는 RPC에 대표적인 구현체 인것이다\n\nREST 와 차이점\n\n\n클라이언트가 여러개의 API 요청을 서버에 보낼 수 있다.\n\nREST 와 달리 여러 요청을 한번에 보낼 수 있다는 것\n따라서 여러개의 응답이 발생 할 수 있다\n\n이게 무슨 소리냐면 스트리밍을 생각하면 된다\n\n\n\n\n요청 방식\n\ngRPC에 경우 일반적인 함수 호출하듯이 호출한다\n\n서비스 지향 설계\n\n\nREST 에 경우 제한된 HTTP 명령구문으로 이루어진다\n\n엔티티 지향 설계\n\n\n\n\n서버와 클라이언트에 긴말성\n양방향 스트리밍 기능 제공\n\n단일 연결에 대하여 클라이언트 서버 모두 여러 요청과 응답을 동시에 처리 가능 하다\n\n\n\n프로토콜 버퍼\n\ngRPC 에서에 JSON 느낌\n메시지 구조를 짤 때 .proto 파일을 통해 프로토콜 버퍼로 구조화 한다\n.proto 파일로 구조를 정의하고 실제 데이터를 보낼때는 json 처럼 키, 값 형태가 아닌 그냥 값 데이터만 보내 바이트 크기를 줄일 수 있다\n\n구조\n\nprotobuf.dev/programming-guides/proto2/\n\n정의 (.proto)\npackage test;\nsyntax = &quot;proto3&quot;; //ver3 사용을 위해 명시\n \nmessage Person{\n\tstring name = 1;\n\tint32 age = 2;\n\t// 타입 | 필드명 | 필드테그\n}\n\n약간 JAVA 클래스 처럼 쓰는느낌\nJSON 처럼 키 값으로 정의 하는 느낌이 아니라 그냥 데이터구조 잡는 느낌\n\n직렬화\n\nname=seok, age=22\n[string바이트]seok\n[int바이트]22\n\n위에서 .proto 파일로 정의 이후 실제 전송되는 데이터\n실제로는 바이너리로 직렬화 된다\n받는 클라이언트는 위에 정의된 proto 데로 해석 하기만 하면 됨\n\n컴파일\n\n.proto 파일은 그냥 구조만 작성하는 거다 실제 정의된 데로 값을 작성하려면\ngithub.com/protocolbuffers/protobuf/releases\n해당 페이지에서 프로토콜 버퍼 컴파일러를 설치 해야 한다\n.proto 파일을 내가 개발하고 있는 언어로 컴파일 하면 해당 언어로 객체를 만들어 주게 된다\nex) python\n\n컴파일 전 proto 파일\nsyntax = &quot;proto3&quot;;\n \npackage com.terry.proto;\n \nmessage Person{\n  string name = 1;\n  int32 age=2;\n  string email=3;\n}\n\npython 으로 컴파일된 proto 파일\nimport address_pb2\n \nperson = address_pb2.Person()\nperson.name = &#039;Terry&#039;\nperson.age = 42\nperson.email = &#039;terry@mycompany.com&#039;\n\n\n\n컴파일하여 address_pb2 파일이 만들어지고 해당 객체에 데이터를 입력하게된다\n\n결론\n\n그냥 값만 보낼테니 내가 구조 정의한데로 해석하셈 이런 느낌임\n서버와 클라이언트는 동일한 proto 를 정의해야한다\n\n그래야 데이터를 해석할 수 있으니\n데이터 전송시 서버가 정의한 proto를 넘겨주면 되는거 아님? 이라 생각 할 수 있지만 그럼 프로토콜 버퍼를 쓰는 이유가 사라진다.\n\n\n이렇게 함으로 데이터 송수신시 발생하는 바이트 크기를 극단적으로 줄일 수 있어 많은 양의 데이터가 오갈 경우 성능이 좋다\n\n사용법\n\ngrpc.io/docs/languages/\n공식적으로 지원되는 언어는 위 페이지와 같다\n아마 필요한 패키지나 설정할게 까다로워 REST와 다르게 지원 언어가 적다\n\n.proto 정의\nsyntax = &quot;proto3&quot;; // 이건 불변\npackage grpc; //패키지 정의, 클라이언트 서버 모두 일치 해야함\n \n// 함수 인자 타입  지정\nmessage HelloRequest {\n  string firstName = 1;\n  string lastName = 2;\n}\n \n// 반환 타입 지정\nmessage HelloResponse {\n  string greeting = 1;\n}\n \n// hello 함수에 인자는 HelloRequest 로, 반환타입을 HelloResponse로 하겠다는거\nservice HelloService {\n  rpc hello(HelloRequest) returns (HelloResponse);\n}\n스트리밍 &amp; 양방향 통신\n\ngRPC는 한번에 커넥션으로 버퍼를 여러번 나눠서 전송하는게 가능하다.\n이는 클라이언트, 서버 모두 가능하여 마치 소켓과 같은 양방향 통신을 구현하는게 가능하다\nREST 라면 패킷을 보낼때 마다 서버에 요청을 하기 때문에 오버해드 문제가 생기는데 이건 커넥션을 유지시키고 패킷을 보내는거기에 최적화가 좋다\n\n\n작동방식\n서버 스트리밍\n\n\nproto 정의\n\nrpc 함수의 return에 stream 을 붇인다\n\nservice StreamingService {\n\trpc serverStream(google.protobuf.Empty) returns (stream StreamingResponse); \n}\n\n\n클라이언트에서는 패킷을 받는 리스너를 등록하기만 하면 된다.\n\n\n서버가 패킷을 보낼때 마다 해당 리스너가 작동할것이다\n\n\n서버는 데이터 전송이 끝나면 전송종료 메시지를 보내고 클라이언트와 연결을 종료한다.\n\n\n\n사실상 서버와 커넥션을 무한정 유지하며 데이터를 받을 수 있게 됨으로 SSE와 작동 방식이 유사해진다.\n\n클라이언트 스트리밍\n\n\nproto 정의\n\nrpc 함수의 파리미터의 stream 을 붇인다\n\nservice StreamingService {\n\t  rpc interactiveStream(stream StreamingReqests) returns (StreamingResponse);\n}\n\n\n\n나머지는 서버 스트리밍과 반대라고 생각하면 된다\n\n\n양방향 스트리밍\n\n사실상 소켓과 유사하게 작동한다\n\n\n\nproto 정의\n\nrpc 함수의 파리미터 와 return에 stream 을 붇인다\n\nservice StreamingService {\n\trpc interactiveStream(stream StreamingReqests) returns (stream StreamingResponse); \n}\n\n\n\n클라이언트 서버 모두 패킷을 받는 리스너를 등록하면 된다\n\n\n멀티쓰레딩 이나 비동기를 사용하여 데이터를 받는 로직과, 데이터를 보내는 로직을 분리시켜 블로킹이 생기지 않게 끔 하는것이 좋다\n\n\ngRPC 서버에서 REST API 구성\n\ngRPC proto를 수정해서 REST API를 만드는게 가능한거 같다\n\n참고사항\n\ncloud.google.com/endpoints/docs/grpc/transcoding\ndeepbaksu.github.io/2021/05/01/how-to-REST-from-gRPC/\n\n사용 시기\n\n웹에서는 사용하기 힘든게 js에 지원이 구대기인듯…\n아마 MSA에 쓰일듯\n\n\n고성능 시스템\n많은 양의 데이터 로드가 필요한 경우\n실시간 스트리밍 애플리케이션\n"},"스터디/개발-용어-정리":{"title":"개발 용어 정리","links":[],"tags":[],"content":"CS (Computer Science)\n\n보통 CS 지식 이라고 불리는데 정말 단순히 컴퓨터 지식이다\n개발자 면접에서 해당 기술을 얼마나 딥하게 알고있는지, 그것과 연개되는 것들이 뭔지를 설명\n그래서 어설프게 알고있는거 괜히 말하지 말고 확실히 말할 수 있는 기술을 설명하자\n\n인터페이스 (Interface)\n\n서로 별게인 두 시스템 끼리 서로 통신하기 위한 접점 또는 규약\n\n\n인터페이스는 단순 선언이다.\n다른 두 시스템이 의도한 기능을 하나의 접점을 두고 기능이 맞게 작동하도록 각자 구현하여 접점에 전달하거나, 전달 받는 것으로 구현이 이루어 진다\n\n보통 그 구현 을 부르는 말을 그냥 구현 이라고 부르거나 구현체 라고도 한다\n\n\n\n하드웨어 인터페이스\n\n예를들어 리모콘에 볼륨 버튼을 누르면 볼륨이 조절 되야 하는건 상식이다\n그니까 리모컨과 TV 사이 서로 통신 규약을 설정한 것이다\n\nAPI (Applicaion Programming Interface)\n\n응용 프로그램 인터페이스\n\n\n사실 인터페이스는 포괄적인 개념이고 소프트웨어에서는 그냥 API다\n서로 물리적 또는 논리적으로 떨어진 두 프로그램간의 소통을 위해 규약을 정한것\n\nDevOps (Development Operations)\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n데브옵스(DevOps)란?\n\n\n\n\n소프트웨어 개발과 운영을 통합하여 효율성, 협력, 속도, 안정성을 개선 하는 개발 방법론\n특정 용어라기 보다는 개발 철학에 가까움으로 하나로 정의하긴 좀 힘든\n\n\n\n소프트웨어 개발부터, 운영, 모니터링 까지 전체적인 생명주기를 관리\n이를 담당하는 사람을 DevOps 엔지니어 라고 한다\n방법론으로는 스크럼, 칸반, 애자일 등이 존재한다\n\n5가지 철학\n\n문화: 아래 사항에 따라 적절한 설계방식을 구상한다\n\n사람, 일, 서비스, 자원, 시간\n\n\n자동화: 아래 사항을 모두 고려 하여 자동화 시스템을 구축 하게 된다\n\n인프라 및 보안, 언어 및 도구, CI/CD, 모니터링\n\n\n측정:\n\n변경 사항이 발생 시 항상 측정하고\n성능과 개발 속도를 모니터링 한다\n\n\n공유:\n\n언제든지 접근 가능한 투명한 데이터\n지식을 공유하는 마인드를 가지고\n문제 발생시 함께 해결해 나가야하고\n일에 가속도가 늘어야 한다\n\n\n축적:\n\n성공과 실패의 결과는 항상 축적되야 한다\n\n\n\nCI/CD\n\n애플리케이션 개발 단계부터 배포까지의 모든 단계를 자동화 하는것을 말함\n\n\nDevOps의 핵심 업무 이기도 하다\n\nCI (Continuous Integration)\n\n지속적인 통합\n\n\ngit 같은 형성관리툴에서 main repo에 통합되는 것을 의미\n많은 양의 커밋을 한번에 통합 하려고 한다면 분명히 많은 충돌을 야기 할 수 있다\n\n그렇기에 작은 단위 커밋을 작성하고 지속적으로 통합하는 것이 중요하다\n\n\n이렇게 통합한 프로젝트를 빌드하고 버그가 있는지 테스트 해보는 것 까지를 의미 한다.\n\nCI 의 자동화\n\n대표적으로 github actions prettier\n\n\n브랜치를 통합하는 과정에서 버그가 있는지 코드가 적절히 포매팅 되있는지 확인하는 것은 여간 귀찮은 것이다\nCI 과정을 자동화하여 통합만 진행하고 나머지는 CI툴이 자동으로 빌드, 테스트 를 진행 하게 된다\n\nCD (Continuous Delivery)\n\n지속적인 제공\n\n\nCI 단계에서 모든 테스트를 마치고 문제 없는지 한번 더 확인하여 배포를 진행하게 된다\n이것으로 CI/CD 절차가 끝나는 것이다\n\nCD 의 자동화\n\n대표적으로 github actions gh-pages\n\n\n이것을 자동화 하여 배포를 진행하는것을 지속적인 배포(Continuous Deployment)\n\n대표적인 CI/CD 툴\n\n잰킨즈(Jenkins)\nGithub Actions\nGitLab\n\n로드 밸런싱 (Load Balancing)\n\n애플리케이션이 위한 모든 리소스에 적절히 네트워크 트레픽을 분산 하는 방법\n\n서비스 디스커버리 (Service Discover)\n\nMSA 를 구성한다고 할때 각 서비스들에 대한 IP와 Port 번호를 저장하고 관리하는 방법\n\nURL &amp; URI\n\nURL: github.com/lseoksee/abc\nURI: /lseoksee/abc\n\n클라우드\n\n내부 컴퓨터의 데이터를 인터넷을 통해 중앙 컴퓨터에 보관하는 공간\n\n\n클라우드 컴퓨팅: 정보를 자신의 pc 가 아니라 클라우드에 연결된 중앙 pc로 처리하는 것\n\n유닉스\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nwww.geeksforgeeks.org/introduction-to-unix-system/\nen.wikipedia.org/wiki/Unix_filesystem\n\n\n\n리눅스"},"스터디/네트워크-통신":{"title":"네트워크 통신","links":[],"tags":[],"content":"기본적인 용어 정리\n네트워크\n\n컴퓨터, 라우터, 스우치 등 여러 장비들이 서로 유/무선 으로 연결되어 데이터를 공유하는 시스템\n네트워크는 단순히 내부 망(이더넷) 끼리의 통신이나, 외부 네트워크 끼리 통신하는 것 모두를 포함\n\n이더넷\n\n네트워크 기술의 일종.\n공유기, 허브에 연결되어 여러 시스템들이 통신 가능한 네트워크 구조\n\n인터넷\n\n전 세계 네트워크가 서로 연결 되있는 전체 네트워크에 집합\n따라서 외부 네트워크와의 통신 그 자체를 말함\n\nWAN\n\n광역 네트워크\n\n\n인터넷 게이트웨이로 라우팅되는 네트워크를 보통 WAN이라고 부른다\n보통 인터넷 통신을 위한 UTP 인터넷 케이블을 WAN 케이블 또는 업링크 케이블 이라 부르기도 한다\n\n네트워크 인터페이스\n\n물리적인 렌카드 (사실상 하나의 PC)\n보통은 렌포트가 하나인데 요즘 좋은것들은 2개씩 존재하는것도 있다\n\n백본 망\n\n소형 네트워크들을 묶어 대규모 파이프라인을 통해 다른 네트워크들의 집합과 연결 한 네트워크\n해저 케이블을 비롯한 광케이블을 등을 사용하여 극도에 대역폭으로 전송이 이루어 진다\n전세계 네트워크에 최상단의 위치하는 Tier 1 네트워크도 이런 형태로 구성되어있다\n\nTCP/IP 계층\n\n모든 네트워크 장비가 실제 물리적인 데이터 전송이 되기 전까지의 과정들을 추상화 한것\n\n\n이론상 존재하는 OSI 7계층을 실제 구현한 것\n보통은 컴퓨터 네트워크 통신에서는 TCP/IP로 많이 설명하는데 다른 네트워크 장비들은 설명하기 쉽다고 OSI 7으로 설명되는 경우가 많다\n\n구조\n\n응용계층\n\n설명: 여긴 그냥 개발자가 개발하기 나름\n프로토콜: HTTP, FTP, SSH\n\n\n전송계층\n\n설명: 데이터 전송할 수 있도록 해주는 계층\n프로토콜: TCP, UDP\n\n\n인터넷 계층\n\n설명: IP 주소를 설정하여 목적지를 지정\n프로토콜: IP, Port\n\n\n네트워크 인터페이스 계층 (NIC 계층)\n\n설명:\n\n실제 물리적인 전송을 담당하는 계층\nPC에 경우 보통은 공유기와 연결됨으로 공유기의 MAC 주소를 추가해준다\n\n\n프로토콜: MAC, 이더넷\n\n\n\n오해하면 큰일나는 것\n\n각 계층에 역할(구현)은 네트워크 장비마다 다르다\n\n보통 설명할때 전송하는 PC 입장에서 설명해놓은 경우가 많다\n모든 네트워크 장비가 TCP/IP 계층을 따르긴 하는데 아래 설명한것 처럼 각각이 어떤역할을 하는지는 컴퓨터 라는 네트워크 장비를 예를 들어 설명한거다\n\n\n예를들어 라우터는 TCP/IP 모델에 L2(인터넷) 계층에서 작동한다\n이 말은 라우터는 TCP/IP 계층 구조에서 인터넷 계층 까지만 구현이 되어있는거다\n\nPC가 NIC 계층에서 물리적으로 전송되어 그 정보를 라우터가 받았다 치자\n네트워크 인터페이스 계층:\n\n라우터는 PC NIC계층에서 온 프레임해더에 MAC 주소와 자신의 MAC 주소를 비교하게된다 여기서 동일한 MAC 주소가 나오면\n\n\n인터넷 계층:\n\n디캡술레이션 과정을 거쳐 최종 목적지IP 주소를 확인하게 된다\n그 IP 주소를 라우팅 테이블을 참조하여 다음 목적지 주소를 확인 하고\n\n\n네트워크 인터페이스 계층:\n\n다음 목적지 MAC 주소를 결정하여 다시 프레임으로 캡슐레이션 해서 전송하게 된다\n\n\n\n\n\nOSI 7계층 | TCP/IP 계층\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#TCP/IP역할OSI 7전송단위L1네트워크 인터페이스Mac, RS-232C물리프레임L2데이터 링크L3인터넷라우팅, IP, ICMP, ARP, RARP네트워크패킷L4전송TCP, UDP, Port전송세그먼트L5세션L6표현L7응용HTTP, SSH, FTP, RIP, OSPF, RIP응용\n수신자PC 입장에서 본 TCP/IP 전송 구조 &amp; 단위\n\n인캡술레이션: 데이터 전송시 각계층을 지날때 마다 각자 해더가 하나씩 붙어가는것\n디캡술레이션: 데이터 수신시 각 계층을 지날때 마다 캡슐화된 데이터를 복원하는 것\n\n\n소켓 스트림 (응용계층)\n\n아래 전송계층단은 OS 커널영역 인데 이걸 파일형태로 추상화 해서 개발자가 사용할수 있게끔 인터페이스화 한게 소켓이다\n우리가 쓰는 WinSocket 이나 이런건 그 인터페이스의 구현체 인거고\n사실상 모든 네트워크 데이터는 소켓스트림 이다\n\n세그먼트 (TCP 계층)\n\n소켓에서 날라온 데이터가 일정크기 만큼 잘려서 TCP 단에 들어오게 된다\n(세그먼트화)\n그럼 그 잘린 데이터에 TCP 해더를 붙이게 되면 이게 세그먼트다\n\n세그먼트 관점에서는 TCP해더 쪽이 해더, 데이터 영역이 페이로드\n\n\n그럼 만들어진 세그먼트를 IP단에 전송하면 여기 작업은 끝난다\n\nMSS (Maximum Segment Size)\n\n최대 세그먼트 사이즈\n\n\n해당 사이즈는 패킷에 크기 에 기초에서 잘려지는데 당연하지만 패킷에 크기보다 작음\nIP 해더 20바이트, TCP 해더 20바이트 해서 MSS의 최대 크기는\n패킷크기-40 이다\n\n패킷 (IP 단)\n\n실제 전송될 데이터 덩어리\n패킷은 세그먼트를 인터넷에서 전송가능한 형태로 변환한게 패킷이다\n\n\nTCP 단에서 전달받은 세그먼트+IP 해더 이것이 패킷이다\nIP 해더 = 해더, 세그먼트 = 페이로드\n페킷을 TCP 단으로 보내는 경우 세그먼트를 디켑슐레이션 과정을 통해 TCP해더를 얻게된다\n즉 세그먼트=패킷에 페이로드 영역이다 라고 볼 수 있다\n\nMTU (Maximum Transmission Unit)\n\n패킷에 최대 크기\n\n\n별일없으면 MTU=1500바이트\n그니까 하나의 패킷으로 전송가능한 최대 크기는 1500바이트 라는 뜻\n\n사실상 해더 제외하면 실제 순수데이터 만으로는 1460바이트가된다\n\n\n더 큰 사이즈를 전송하고 싶으면 위 세그먼트 단계에서 잘게잘게 잘려서 여러 패킷으로 나오게 되어 전송이 된다\n\n프레임 (MAC 주소단)\n\n네트워크가 실제로 데이터를 전송할때 쓰는 단위\n이더넷 프레임 이라고 하기도 함\n\n\nIP 단에서 전달받은 패킷+ 프레임 해더\n프레임 해더 = 해더, 패킷 = 페이로드\n목적지, 소스 실제 MAC 주소를 담고 있다\n\n여기서 MAC 주소는 최종 전달자가 아니다\n만약 PC가 공유기(라우터)와 연결되어있다 치면 해당 공유기의 MAC 주소인거다\n\n\nFCS: 프레임의 무결성을 확인하기 위한 오류 검출 코드\n여기서 IPv4, IPv6 판단이 이루어 진다\n\n소켓 통신\n\n\n네트워크들을 연결하는 하나의 접점을 제공하는 기술.\n전송계층단은 OS 커널영역 인데 이걸 파일형태로 추상화 해서 개발자가 사용할수 있게끔 인터페이스화 한게 소켓이다\n모든 네트워크 통신에 기반\n자체 규격 만드는거 아니면 어떤 통신을 하건 소켓으로 하게 됨\nHTTP, FTP, SSH 등도 다 내부적으로는 소켓으로 구현\n\n소켓으로 데이터를 보낼 때 JSON 처럼 규격있는 메시지를 보낸다고 생각하면 됨\n\n\n\n작동 원리\n\n소켓은 파일이다 보통 그래서 똑같이 파일처럼 스트림을 쓰는데 이때 쓰는게 소켓 스트림\n전송계층과 응용계층 사이를 연결하여 데이터 전송 을 당당 즉\n그 중간에 작동한다 볼 수 있다\n\n에초에 HTTP, FTP, SSH 이것들 전부가 소켓통신 기반\n즉 소켓으로도 규격에 맞는 데이터만 보낸다면 잘 작동함\n\n\n\n구현\n\nWindows api 같은 OS API를 사용하여 구현이 가능하다\n사실상 개발자가 구현할 수 있는 가장 밑바닥\n\n소켓 기반 프로토콜이 나온 이유?\n\n그냥 소켓으로도 모든 통신이 가능한데 왜 HTTP, FTP, SSH 이런 것들이 나왔는지에 대해 생각해 보자면\n\n\n페이지 또는 웹 브라우저 별로 각자 소켓으로 주고받는 메시지가 다르다면?\n누군가는 요청/응답 동작만 하는 통신에 양방향으로 설정하고 누구는 단방향으로 설정한다면?\n핼파티 날께 뻔하므로 규칙을 정한 듯\n\nTCP가 UDP보다 느린 이유\n패킷의 무결성 확인 과정의 대기\n\n서버가 1, 2번패킷을 서버가 보냈다\n클라이언트가 1,2 번 패킷을 받은 경우 ACK 3 을 보내 “3번부터 보내라” 라는 전송완료 메시지를 보낸다\n근데 이 과정에서 서버는 ACK3 응답이 올때까지 대기 하게 된다\n그래서 딜레이가 생기기 때문에 느려진다\n\nTCP 버퍼 윈도우 공간 문제\n\nTCP 버퍼란 서버에 전송완료된 데이터를 소켓이 Read 하는 임시공간\n서버에서 아주 빈도 높게 데이터를 전송하는 경우 소켓이 이를 즉시\nRead하지 못 할수있기 때문에 만들어놓은 임시공간이라고 볼 수 있다\n\n왜 문제가 되는가\n\n서버가 ACK패킷을 응답받으면 서버는 다음 패킷을 보내야 하는 상황이다\n수신측 TCP 버퍼쪽에 남은 윈도우 공간이 없으면 보내지 않는다\n그럼 이때도 또 클라이언트가 TCP 버퍼를 Socket이 읽을때 까지 또 대기타야 한다\n\n해결 방법은?\n\n송, 수신 버퍼를 동일하게 잡는다.\nNetwork 수신속도 &lt; 데이터 읽는 Read 속도\n이게 성립해야 한다\n\n사실상 네트워크 I/O보다 파일I/O 속도가 느리다면 이건 프로그램을 잘못 짠거라고 생각된다\n즉 앱을 개발하는데 있어서 네트워크 속도보다 입출력 속도가 더 빠른지 체크하면서 해야한다\n\n\n\nHTTP 통신\n\n소켓통신에서 양방향 통신을 버린 TCP 단방향 통신\n\nHTTP 2.0 와서 지원하기 시작함\n\n\n요청/응답 프로세싱을 지님\n클라이언트가 서버에 요청하면 서버가 해당 작업 후 연결 종료\nURL을 통한 요청 분리 가능\n\n구현\n\nHTTP 도 결국은 소켓으로 통신하는 것\n\n클라이언트\nPOST /userAccount/login HTTP/1.1 Host: swiftapi.rubypaper.co.kr:2029 Content-Type: application/x-www-form-urlencoded account=swift%40swift.com&amp;password=1234&amp;grant_type=password\n\n\n이런 메시지를 소켓 데이터에 담아서 보내면 잘 전달됨\n응답 후 연결 종료, 단방향 통신 등 이런것만 설정해주면 HTTP Reqeust API 완성\n\n서버\n\n클라이언트가 위 데이터를 보내면 해당 구문을 parse 하는것으로 가능\n/userAccount/login 이 부분으로 구분해서 Spring 이나 Nest.js는 어노테이션으로 파싱되고 Express.js같은 경우 Callback으로 전달 되는 구조인 듯\n즉 HTTP 요청 구문을 파싱해서 데이터를 받고 다시 응답 데이터를 구성해서 보내게 된다\n\nHTTPS\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nHTTPS 통신 원리 쉽게 이해하기 (Feat. SSL Handshake, SSL 인증서)\nWireshark 패킷 분석을 통해 http와 https의 차이점 알아보기\n\n\n\n\n기존 HTTP 에TLS 암호화 프로토콜이 추가된 방식\n\n요약하자면 클라이언트-서버 사이 사용할 대칭키를 안전하게 교환하기위한 프로토콜\n통신 과정에서 중간자 공격을 보호한다\nTLS 프로토콜 과정\n\n\nClient Hello\n\n클라이언트가 서버에 연결시도 패킷을 전송\n클라이언트에서 사용가능한 암호 프로토콜 목록 등과, 인증방식 등을 서버에 전송한다.\n패킷 내부에 Server Name Indication extension 이라는 값이 있는데\n이게 현재 인터넷 검열에 사용되는 SNI 필드다.\n\n\nServer Hello\n\n클라이언트에 대한 응답 패킷을 전송\n클라이언트가 사용가능한 암호 프로토콜 중 하나를 선택하여 클라이언트에 전송\n\n\nCertificate, Server Key Exchange, Server Hello Done\n\n서버는 비밀키-공개키 키 쌍을 생성하고, 공개키를 포함한 SSL 인증서를 구성하여 클라이언트에게 전송한다\n인증서에는 발급기관, 만료기간 등을 포함한다.\n\n\nClient Key Exchange, Change Ciper Spec\n\n클라이언트는 실제 데이터 전송을 암호화 하기 위한 대칭키를 생성한다.\n생성한 대칭키를 서버에서 전달받은 공개키로 암호화를 진행 하여 서버에 전달한다\n\n\nServer / Client SSL Handshake Finished\n\n서버는 전달 받은 대칭키를 서버에 비밀키로 복호화 하여 통신할 준비를 마친다\n클라이언트-서버는 생성된 대칭키 기반으로 데이터를 암호화 하여 송-수신 하게된다.\n\n\n\nTLS에 키 교환에 대해\n보통 비 대칭키에 암호화는 서버-클라이언트 모두 공개키-개인키 쌍을 생성하여 서버는 클라이언트에 자신에 공개키를 전달하고, 클라이언트는 자신에 공개키를 서버에 전송하여 각각에 비밀키로 복호화 하는것이였다.\n하지만 속도에 문제로 모든 데이터를 비 대칭키로 암-복호화 하는것이 매우 비효율적이여서 TLS 에서는 서버에 개인키-공개키 만 사용한다\n\n서버는 개인키-공개키 키 쌍을 생성하여 공개키를 클라이언트에 전송한다.\n클라이언트는 임의의 키를 생성하고 생성된 대칭키를 서버에 공개키로 암호화 하여 서버에 전송한다.\n서버는 암호된 임의의 키를 서버에 비밀키로 복호화 한다\n서버-클라이인트 모두 해당 대칭키 기반으로 동일한 세션키를 생성하여 (보통은 AES 프로토콜이 사용됨)\n해당 세션키로 데이터를 암호화하여 통신이 이루어진다.\n\n즉 데이터에 대한 암호화는 대칭키 방식으로 이루어진다. TLS는 그래서 대칭키를 암호화 하기 위한 프로토콜로 보는것이 합당하며, 이렇게 하므로 효율과 안전성 모두를 챙길 수 있다\n다만 TLS에 옵션중 Mutual Authentication 라는것을 활성화 하면 서버 클라이언트 모두 비 대칭키를 사용하는 인증방식을 사용할 수 있다.\nSSL과 TLS\n\n결론부터 말하면 SSL에 후속 버전이 TLS 이며 SSL은 현재 사용되지 않는다.\n\nSSL 과 TLS는 현시점에서 용어가 혼용되는 경우가 많은데 그냥 TLS 만 사용한다 생각하면 된다\nSSL 인증서에 경우 TLS 인증서 라고 생각하면 된다\n인증서와 발급기관\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nnamu.wiki/w/TLS#s-1.1.1\n\n\n\n\nX.509 규격에 전자 서명 인증서를 사용하여 서버 또는 클라이언트를 인증한다\n\n인증서 발급은 openssl과 같은걸로 누구나 할 수 있기 떄문에 HTTPS 과정중 유일하게 중간자 공격이 가능한부분이 Certificate 과정인데 공격자가 자신이 위조한 인증서를 바꿔치기 한다면 공격자는 위조한 인증서에 발급된 비밀키 정보를 사용하여 피해자에 모든 패킷을 복호화 하는게 가능하다.\n때문에 각 브라우저들은 신뢰할 수 있는 인증서 발급 기관(CA)를 지정하여 해당 CA 들이 가진 공개키를 관리하고 있다.\n서버가 인증서를 발급 시 서명 정보에는 도메인 주소와 같은 고유한 정보로 만들어진 해시값을 CA에 개인 키로 암호한 정보가 저장되어있다.\n클라이언트가 서버에 인증서를 전달 받으면 CA에 공개키로 복호화 하면 하면\nCA가 구한 해시값을 얻을 수 있다.\n클라이언트는 서버가 했던 방식으로 해시값을 생성하여 두 해시값이 같은 경우 위조되지 않았다 판단 할 수 있다.\n인증서 종류\n\nDV: 도메인의 소유권만을 검증\nOV: 인증서 발급 대상의 실체까지 검증\nEV: 인증서 발급 대상 조직 심사, 검증, 도메인 승인을 모두 검증\n\nDV 인증서가 제일 싸고 EV 인증서가 제일 비싸다\n무료로 발급 가능한 Let&#039;s Encrypt의 경우 DV의 해당 한다\n물론 EV 가 가장 안전하긴 하겠지만 사실 여기 보안에 그렇게 많은 힘을 쏟아봤자 가격대비 별 의미가 없다.\nTCP 연결/해제\n3-Way Handshake\n\nTCP 연결 과정\n\n\n\n클라이언트 → 서버에 접속요청하는 SYN 패킷보냄\n서버 → 클라이언트에 요청을 수락한다는 의미로 ACK+SYN 패킷을 보내어\n클라이언트가 ACK 패킷을 응답하길 기다림 (이것을 Receive 상태)\n클라이언트 → 서버에 ACK 패킷을 보내어 데이터 송/수신\n\n4-Way Handshake\n\nTCP 연결 해제\n\n\n\n클라이언트 → 서버에 연결 종료하겠다는 FIN 플래그 전송\n서버 → 클라이언트 ACK 패킷을 먼저 전송하고 통신이 완전히 끝날때 까지\n기다림 (이때 서버는 TIME_WAIT)\n서버 → 클라이언트 통신이 완전히 끝나면 FIN 플래그 전송\n클라이언트 → 서버에 종료 확인으로 ACK 전송\n\n서브넷\n\nIPv4 주소의 소멸을 막기위해 하나의 네트워크를 한번 더 쪼갠 네트워크\n\n\n\n할당 받은 IP 192.168.0.1/24에 호스트 부분 0~255를 쪼게서 192.168.0.127/25 로 네트워크 대역을 추가적으로 만들어 내는 것\n개인 네트워크에서 쓸 일은 잘 없고 기업이나 클라우드 컴퓨팅에 애용된다\nISP 에서 할당받는 외부IP 도 상위 네트워크에서 서브넷으로 쪼개서 나오는 것이다\n\n특징\n\n\n192.168.1.0/24 를 4개에 서브넷으로 분활한다면\n\n4개니까 4를 표현할 수 있는 2비트를 호스트에서 빼옴\n즉 192.168.1.0/26 서브넷마스크는 255.255.255.192\n이때 나올수 있는 가지 수는 1번:00,  2번:01, 3번:10, 4번:11 이다.\n(순서는 맨 마지막 비트부터 1씩 더해 가서 나온 순서 대로 이다)\n\n1번 네트워크 (00): 192.168.1.0~63,\n2번 네트워크 (01): 192.168.1.64~127,\n3번 네트워크 (10): 192.168.1.128~191\n4번 네트워크 (10): 192.168.1.192~255\n\n\n이렇게 서브넷이 분활된다\n\n\n\n많은 서브넷을 생성하려면:\n\n최상위 서브넷에 네트워크ID 부분이 적어야한다.\n분활되는 서브넷에 호스트 부분은 적어야 한다.\n최상위 서브넷은 58.0.0.0/8 같이 적은 CIDR 값을, 분활된 서브넷은 58.0.1.0/24 같이 큰 CIDR 값을 사용해야 네트워크를 더 많이 분할 할 수 있다\n\n\n\n왜 사용할까?\n\n그냥 처음부터 네트워크ID 부분을 적게 할당하고 호스트 부분을 크게 늘려서 모든 네트워크 장치를 해당 네트워크에 하위로 넣으면 되지 않을까?\n\n\n문제는 브로드케스팅과 보안에 있다.\n네트워크 통신 과정에서 ARP는 필수적이며 해당 과정으로 인해 필현적으로\n브로드 케스팅이 이루어 지진다\n만약 연결되는 호스트가 1백만게 라고 치면 패킷 하나 들어올때 마다 1백만게 장치들한테 브로드 케스팅이 되는거다\n이건 정말 비효율적이며, 보안에도 문제가 많다\n\n오해 하지 말 것\n\n공유기와 공유기 연결은 서브넷이 아니다\n\n공유기A (58.0.0.0/8) 에 공유기B 를 연결한다고 치자, 그렇다면 공유기A 에 호스트로\n공유기 B 가 할당 되는거다\n(1) 만일 공유기B 가 새로운 서브넷인 192.168.0.0/24를 생성한다면\n이것을 서브넷을 분할한다고 볼 수 없다.\n(2) 하지만 만약 공유기A 가 가지고있는 호스트 범위, 58.0.0.0 을 쪼갠 서브넷 58.0.1.0 을 만들어서 공유기B에 새로운 서브넷을 생성하지 않고\n연결한다면 이것은 서브넷을 분할 볼 수 있다\n정리하자면 1번은 서브넷이 아니라 서로 다른 네트워크를 구성하는거 오해하지 말자\n\n\n\nCIDR\n\n서브넷에 IP를 할당할때 사용하는 방법\n\n203.76.25.1/24 라면 앞에 8+8+8 비트를 네트워크 ID로 쓴다는 뜻\n만약 203.76.25.1/24를 203.76.25.1/26 으로 서브넷을 분활한다면\n네트워크 부분은 26비트, 호스트 부분은 6비트가 되버린다\n이때 서브넷 마스크는 255.255.255.192 가된다.\n그러면 최대로 가질 수 있는 네트워크는 4개, 각 네트워크 당 호스트는\n255-192=63개 이다\n이제는 거의 클래스 방식을 쓰지 않는다\n재일 중요한 점이 IP 클래스(A, B, C)와 CIDR 가 다른거라고 오해 할 수 있다\nCIDR 는 기존 IP 클래스 할당 방식을 대체 하기 위해 나온것이다\n따라서 현재 IP 할당에 대부분은 CIDR 이다\n다만 개념은 아직까지도 쓰여서 IP 주소를 알면 대강에 서브넷 마스크를 알 수 있다\n서브넷 마스크\n\n서브넷을 정의할때 사용되는 값, 어떤부분이 네트워크인 부분인지, 호스트 부분인지를 나눔\n\n\n이게 사실 공인 IP도 하나의 네트워크에서 파생된 서브넷 이라고 볼수 있어서\n모든 IP주소 할당은 서브넷 마스크를 사용하게 된다\n가령 서브넷 마스크가 255.255.255.0 이라면\n\n3바이트 네트워크, 1바이트 호스트 부분으로 결정된다\n192.168.0.x 이렇게 IP 주소가 결정될 수 있다\n\n\n\nProxy Server\n\n클라이언트가 자신(Proxy)을 거쳐 다른 네트워크에 접속 할 수 있고록 하는 중간자 서버\n즉 일종의 터널링 서버 이다\n\nForward Proxy\n\n일반적으로 말하는 프록시 서버\n\n\n설명\n\n클라이언트와 접근하려는 리소스 사이에 위치한다\n웹에 요청 시 해당요청을 내부PC 에서 하는게 아나라 프록시 서버에서 요청하여 그 결과를 리턴해준다 즉, 요청을 프록시 서버가 케치하는\n\n즉 서버쪽에서는 PC의 IP 정보가 아닌 프록시 서버의 IP로 요청을 보낸다\n\n\n위 사진에는 내부망 이라고 표현되었지만 프록시 서버에 직접적으로 접근 가능한 경우 외부망도 접근 가능하다\n\n사용이유\n\n케시 사용:\n\n프록시 서버로 요청한 내용 중 일부를 케싱 하여 재요청시 실제 서버에 요청 되는 트레픽을 줄일 수 있다\n\n\n클라이언트 보안목적:\n\n클라이언트 쪽에서 실제 서버에 IP 주소를 노출 시키지 않기 때문에 클라이언트 보안에도 좋다\n학교나 기관 같은경우 프록시 서버에 룰을 도입하여 인터넷 사용을 제한하기도 한다\n\n\n우회기능:\n\n위에서 설명 하듯이 외부 망에서 프록시 서버에 접근하여 프록시를 적용한 경우 네트워크 요청이 모두 프록시 서버를 통해 진행하기 때문에 VPN 같은 효과를 누릴 수 있다.\n\n\n\n점프서버\n\n서비스들의 외부 접근은 전부 차단하고 같은 네트워크 망에 Proxy Server나 터널링 서버의 외부 접근만 허용한다\n각 클라이언트는 Proxy Server나 터널링 과정을 반드시 거치도록 설계한 서버\n\n\n일종의 외부 클라이언트의 게이트웨이 역할을 한다\n예를들면 MySql 의 포트 3306의 외부 포트를 차단시키고 SSH 포트 22번만 오픈하여 각 클라이언트는 SSH 터널링을 통해서만 접근 가능하도록 설계하는\n\nRevers Proxy\n\nForward Proxy가 클라이언트 쪽 Proxy 라면\nRevers Proxy는 서버 쪽 Proxy\n\n\n설명\n\n프록시를 서버에 적용한 방식\n클라이언트는 평소처럼 웹에 접속을 하지만 해당 요청은 Revers Proxy 전달되고 해당 요청사항을 WAS나 웹 서버에 다시 전송하게 된다.\n이 경우 본 서버의 IP 주소를 감추는 보안적 효과를 얻는다\n\n사용이유\n\n웹 서비스 통합:\n\n하나의 서버의 여러개의 웹 에플리케이션을 배포하려면 서로 포트를 다르게 해야한다\n문제는 HTTP, HTTPS 포트는 80, 433 두개로 표준이 되어있다\nProxy 서버 하나를 80 포트 또는 433 으로 호스팅하고\n요청된 엔드포인트가api/1 일경우 해당하는 서버에 전달하고 다른 엔드포인트가 오면 또 해당하는 서버에 전달하는 식으로 구성하여 해결한다.\n\n\n트래픽 관리:\n\n본 서버를 여려개를 둔 경우(약간 메이플 처럼) 프록시 서버쪽에서 원할한 서버를 찾아 연결시켜 주어 분산화 작업이 가능하다.\n또 클라이언트에 지역을 파악하여 가장빠른 서버로 매칭 시켜줄 수도 있다\n\n\n서버 보안목적:\n\n실제 서버를 감출 수 있기에 프록시 서버가 해킹 당해도 본서버만 멀정하면 복구가 수월하다\n\n\n\nSOCKS5\n\n프록시 서버 ←&gt; 서버 ←&gt;클라이언트 의 사용되는 프로토콜\n\n\nHTTP 그 아레단 에서 작동한다\nTCP, UDP를 통해 네트워크 패킷을 전달하는 프로토콜\n\nQoS (Quality of Service)\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nQoS (Quality of Service) 보장기술\n\n\n\n\n한정된 네트워크 자원 내 사용자 관점에 네트워크 서비스 품질 향상을 위한 기술\n\n우선순위에 따라 트레픽과 대역폭을 관리하게 된다.\n에를들어 특정 서비스에 네트워크 품질을 향상 시키려면 다른 서비스나 OS상에서 돌고있는 서비스들에 우선순위를 낮추고 해당 서비스의 우선순위를 높혀서 먼저 처리할 수 있게 끔 설정한다.\n즉 QoS를 설정한 서비스 외에 모든 서비스의 네트워크 품질이 떨어진다는 해석을 할 수 있어서 신중하게 설정해야 한다.\n작동 원리\n보통 패킷이나 프레임에 특수 해더를 내포하여 이 패킷이 QoS 패킷이라는 정보를 내포 하도록 설계 됨\n목적지 까지 거치는 라우터들이 QoS를 지원하면 QoS패킷을 가장 상위 우선순위 대역폭에 할당하여 빠르게 처리할 수 있도록 지원하게 됨\n외부 네트워크에서는 ISP가 QoS를 제대로 지원하지 않는 경우가 있고, 있더라도 데이터 센터에서 오는 트래픽을 우선 처리해주는 경우도 있어서\n상황에 따라 크게 지연 속도가 향상되기도 하지만 반대로 크게 상관없는 경우도 있다.\n내부 네트워크/외부 네트워크\n내부 네트워크에서의 외부 IP\n\n공유기에 묶인 모든 장치의 외부 IP 주소는 같다.\n나가는 통신의 경우 아래 NAT 기능이 있어서 문제가 없는데\n여기서 가장 큰 문제가 바로 서버를 구성할 경우인데\n\n외부 IP 주소는 222.253.0.1 이라고 가정한다\n각각 같은 공유기에 묶인 PC 2대가 80포트로 서버를 실행한다 치자\n그럼 외부에서 222.253.0.1:80 포트로 접속을 시도하면 어떤 서버로 들어 가져야 하는 것일까?\n\n\n해결 방법중 하나는 공유기 포트포워딩을 하는것이다\n\nPC1의 IP를 192.0.0.1, PC2의 IP를 192.0.0.2 로 가정한다\n외부포트 100을 192.0.0.1 에 내부포트 80 으로\n외부포트 101을 192.0.0.2 에 내부포트 80 으로\n\n\n그러나 이 방법은 외부 접속시 포트가 달라지는 거라 그게 싫다면 리버스 프록시를 고려 할 수 있다\n\nDHCP (Dynamic Host Configuration Protocol)\n\nTCP/IP 주소와 각종 프로토콜 설정을 자동적으로 재공해주는 프로토콜\nIP주소를 유동적으로 관리해 준다\n\n\n주로 내부 내트워크 안에있는 기기들의 ip 주소를 관리해 준다\n\n외부 망에서도 사용하긴 하는데 이건 ISP들이 관리하는거라 논외다\n\n\n\n원리\n\n클라이언트에게 ip 할당 요청이 들어오면 ip 를 부여해 준다\nIP 주소 할당은 임대 기간이라는것을 정해놓고 그기간동안 ip를 할당한다\n\n주의점\n\nIP 주소를 유동적으로 관리하기 때문에 서버를 굴릴꺼면 DHCP 를 끄는것이 좋다\n\nGateway (게이트웨이)\n\n외부 네트워크와 통신하기 위에서 반드시 거쳐야 하는 지점\n\n\n하드웨어 형태로 존재하지만 보통 공유기에 내장\n192.0.0.1 같이 끝에 1비트만 1로 두는 형태\n공유기에 경우 보통 관리자 페이지가 서비스 되고 있다\n공유기에 연결된 장비가 직접적으로 외부랑 통신하는게 아니라 게이트웨이를 거쳐서 진행된다 즉 서로와 서로의 게이트웨이 끼리에 통신을 하는것이다.\n\nNAT (Network Address Translation) &amp; 게이트웨이\n\n하나의 외부 IP 주소를 가지고 여러 내부 네트워크 통신을 컨트롤 하는 기술\n\n\n공유기를 사용하는경우 위처럼 외부 IP 주소는 딱 하나다.\nNAT 은 모든 내부 장치들이 하나의 외부 IP를 가지고도 통신할수 있게 하는 기술이다\nIPv4 주소가 소멸하지 않는 이유는 이 NAT 의 존재 덕분 이라고 할 수 있다\n\n작동원리\n\n외부로 나가는 통신:\n\n특정 장치가 패킷을 전송하면 공유기가 해당 내부IP 주소와 포트를 기억\n공유기는 해당 정보를 NAT 테이블에 저장했다가 응답이 돌아오면 다시 원래의 내부 주소와 포트로 변환한다.\n즉 전송 포트번호를 다르게 하여 구분한다는 것이다\n\n\n외부에서 들어오는 통신:\n\n목적지 IP 주소와 포트번호를 공유기가 받아서 NAT 테이블울 참조하여 적절한 장치에 반환시킨다.\n\n\n\n인터넷 게이트웨이\n\n기본적인 네트워크와 인터넷 간의 연결을 제공\n\n\n클라우드 컴퓨팅의 VPC 환경이나, 공유기를 사용하지 않고 ISP 에 직접적으로 IP 주소를 할당받는 경우\n\nVPC 자체의 NAT 게이트웨이를 두지 않고 EC2 같은 단일 서비스에 공인 IP 주소를 할당 받는 상황\n\n\n\n외부 IP 주소 고정\n\n결론부터 말하면 공짜로는 못한다\n\n\n공인 IP는 ISP 업체로부터 IP를 할당 받는 방식이라 맘대로 설정할 수 없다\nDDNS를 쓰든 따로 연락해서 구매하여 써야 한다.\n근데 나의 경우 일정 기간동안 쓰니까 몇년째 IP가 고정되었다…\n\n라우팅 &amp; 포워딩\n\n라우팅은 L3 (네트워크 계층), 포워딩은 L2 (데이터 링크 계층)\n\n라우팅\n\nPC가 NIC 계층에서 물리적으로 전송되어 그 정보를 라우터가 받았다 치자\n라우터는 PC NIC계층에서 온 프레임해더에 MAC 주소와 자신의 MAC 주소를 비교하게된다 여기서 동일한 MAC 주소가 나오면\n디캡슐링 과정을 통해 IP를 추출하게되어 실제 라우팅 과정이 진행 된다\n\nOSI 7계층 | TCP/IP 계층\n\n패킷이 출발지로 부터 도착지 까지의 경로로 설정하는 과정\n\n라우팅 테이블\n\n목적지 까지 갈수있는 경로들을 모아놓은 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n목적지넷마스크게이트웨이인터페이스192.168.1.0255.255.255.0192.168.2.1eth010.0.0.0255.255.0.010.0.1.1eth10.0.0.00.0.0.0192.168.2.1eth0예시: 192.168.1.0 으로 패킷이 나가야 되면 192.168.2.1 로 이동해야함\n\n정적 라우팅:\n\n직접 라우팅 테이블을 건드려서 아예 다른 네트워크를 거치거나\n아예 목적지 주소를 바꿔 버릴 수 있음\n\n\n동적 라우팅:\n\n라우터들끼리 알아서 정보를 교환하여 자동으로 최단 라우팅을 설정하는 방법\n이때 OSPF, BGP, RIP 같은 라우팅 테이블 생성알고리즘이 작동하게 된다\n\n\n\n포워딩 (스위칭)\n\n포트포워딩과 다른 개념이니까 오해하지말자\n\n\n포워딩은 라우팅이 경로를 설정한다면 포워딩은 실제로 패킷이 이동하는 작업이다.\n이 작업은 스위치가 진행한다. 보통은 라우터 안에 스위치 기능이 포함되어 라우팅 이후에 스위칭이 진행된다\n\n포워딩 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAC 주소포트타이머00:1A:2B:3C:4D:5E1300s00:1F:2E:3D:4C:5B2200s00:1B:2A:3E:4F:5D3150s\n\n라우터가(네트워크 계층) 경로를 결정하면 ARP 과정을 통해 IP 주소를 MAC 주소로 변경한다\nMAC 주소가 L2스위치(데이터링크 계층)로 전달되고 포워딩테이블을 참조하여 해당 MAC 주소에 패킷을 전달한다\n\nARP\n\nIP 주소를 실제 MAC 주소로 바꾸는 프로토콜\n\nWho has 58.232.252.10? Tell 58.232.252.1\n\n송신자는 58.232.252.10를 가진자가 누구 인지를 찾기위해 ARP Call을 브로드케스팅함\n이때 자신에 네트워크로 판단하면 게이트웨이 IP 주소를 반환해줌\nARP 패킷에 수신측 MAC 주소는 FF:FF:FF:FF:FF:FF:FF로 설정되는데 해당 패킷을 받은 L2스위치나, 라우터는 연결된 모든 장치에 브로드케스팅을 하게 된다.\n주의사항\n내 PC에서 나가는 아웃바운드 패킷에 경우 ARP 케싱을 하기 때문에 무조건 ARP Call을 요청하는건 아님!!!\n다만 인바운드에 경우 모뎀이나 공유기가 따로 케싱하지 않는이상 대부분 CALL 하게 되어있다\n\n\nARP 케싱 확인하는 법\narp -a\n\n\nIP forwarding\n\n리눅스 커널 자체에서 라우팅을 할 수 있는 기능\n포워딩은 원래 데이터 링크 계층이라 MAC 주소로 작동하는데 이걸 IP 주소로 포워딩 하게 설계했다 보면됨\n\n\n\n\n해당 구조는 Node1, 2 3 모두 하나의 공유기에 물려있다고 생각하자\n\n\nNode01 에 패킷이 들어오면 IP forwarding 으로 옆에 있는 Node02 에 패킷을 전달하는 역할을 한다\n\n\n그니까 PC를 하나의 라우터로 사용하는 방식인 것이다\n\n\n응용\n\nPC1 에서 VPN 서버를 설치, PC1 은 대한민국IP, PC2는 일본IP\n모종의 이유로 PC2에 VPN 서버를 구성할 수 없다치자\n그럼 PC2가 PC1에 VPN 클라이언트로 연결하여 PC1의 내부망으로 들어간다\n실제 VPN을 사용하려는 클라이언트가 PC1 VPN으로 들어올 경우 PC2 의 네트워크망 즉, 일본 으로 라우팅 되야하는경우 이 기능을 사용해 볼수 있다\n\n포트포워딩\n\n게이트웨이를 통과하는동안 외부에서 접속가능하도록 주소를 변환하는 것\n\n\n\n공유기로 물린 모든 장치의 외부 IP 주소는 같다\n그걸 접속포트로 구분하여 매핑시켜준다\n\nDNS (Domain Name System) 서버\n\nDNS 는 도메인 → IP 주소로 바꾸는 역할\n\n\nDNS 서버는 도메인 주소를 요청하면 DNS서버쪽 테이블에서 해당 도메인 주소에 맞는 IP 주소를 응답해주는 서버를 뜻함\n바로 DNS 서버를 거치는건 아니고 단계가 있다\n\nOS의 Hosts 파일\nDNS 케시\n그때 DNS 요청\n\n\n국내3사 통신사, 구글, cloudflare 등이 DNS 서버를 가짐\n\n국내 이통사 DNS\n인터넷 검열\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nSNI(Server Name Indication)\n\n\n\n\n우리나라는 인터넷 검열로 음지 사이트 도메인주소는 전부 Warning.or.kr의 IP 주소를 반환하게 설계했다\n그래서 DNS 주소를 구글(8.8.8.8) cloudflare(1.1.1.1) 로 바꾸면 검열된 사이트를 볼 수 있게 됬었다\n근데 2019년 인터넷 검열이후로  HTTPS 해더의 Host 부분인 SNI 필드로 검열한다더라\n\nSKT DNS를 쓰면 인터넷이 개느린 이유\n\n답은 간단하게 그냥 평균 IP 반환 속도가 개느리다\n\n\n모든 광고 및 어떤 리소스를 불러와도 인터넷에서는 거의 도메인 주소를 쓴다\n이때 DNS 서버가 느린경우 도메인 → IP 주소로 변환하는 시간이 느려지면\n그 만큼 리소스를 불러오는 속도가 늦어지니까 이다\n파일 다운로드 하는 경우에도 그렇다. 보통 HTTP 요청할떄 bytes 해더를 써서\n바이씩 끊어서 요청하게 될텐데 그럼 그만큼 DNS서버에 IP 반환 요청도 그만큼 할태니 거기서 딜레이가 생기기 때문\n\nAdguard DNS\n\n이친구들 DNS 서버는 아예 광고로 파악된 모든 도메인 주소를 쓰레기IP 주소를 반환 하게 설계해서\n모든 광고를 차단하게 된다\n\nVPN (Virtual Private Network)\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n개발자는 알아야 할 VPN 작동원리 (영상)\n\n\n\nPrivate Network란?\n\n공유기 등에 묶여 별도에 인바운드 트레픽이 뚫려있지 않은 사설망(LAN) 환경 그 자체\n\nVirtual 이라는 것에서 알 수 있듯이 VPN등장 이전에 일반적인 Private Network는 외부 망에 누군가가 내부 Private Network에 보안 접속을 하기 위해서는 진짜 물리적으로 통신사에 연락해서 사설망과 연결하려는 장치 간에 전용회선에 독점적 권한을 사야 됬다.\n당연하지만 거리가 멀어질 수록 가격이 기하급수적으로 상승하였고 속도 또한 미친듯이 느렸다.\n민감한 정보로 인해 외부로 포트를 노출 시킨다고 하면 중간자 공격에 매우 취약하기에\n이걸 물리적으로 처리하지 않고 소프트웨어를 사용해 해결해 보자고 한 게 VPN 이다.\nVPN은 사용자 인증을 담당하는 VPN 서버와, TLS 와 같은 비대칭 암호화를 결합 시켜 외부로 포트를 노출 시켜 외부 접속자와 통신을 하더라도 중간자 스푸핑 공격을 방어하기 위해 개발된 것이다.\nVPN의 원리\n\n내부 망에 서비스는 건드리지 않고 VPN 서버를 추가하여 해당 서버에 포트만 공개한다\n이 과정에서 VPN 서버와 연결하려는 클라이언트 모두 VPN 트레픽이 오갈 가상의 네트워크 인터페이스 드라이버를 설치하게 된다.\n클라이언트가 VPN 서버에 접속하기 위해 일종의 인증을 거치고 연결이 되어 두 네트워크 사이 패킷을 이동할 수 있으면 이게 터널링 이다.\n일반적으로 PC에 가상 네트워크 인터페이스를 설치하는 경우가 3, 4계층 에서 작동하는 VPN이고, 아예 VPN이 하드웨어 형태로 나온게 있는데 이게 2계층에서 작동하는 VPN 이다\n암호화 과정\n\n클라이언트가 서버에 패킷을 보낸다 가정하면 먼저 패킷은 VPN 터널의 네트워크 인터페이스로 가게 된다.\n여기서 출발지 IP 주소는 자신의 VPN IP 주소가 되고 목적지는 해당 VPN 서버 내부 망에 존재하는 서비스 IP 주소일 것이다.\n가장 핵심은 이 패킷을 VPN 터널은 싸그리다 암호화 시켜버려서 하나의 패킷으로 다시 생성한다\n해당 패킷을 원래 물리 네트워크 인터페이스에 전달하는데 이때 출발지 IP 주소는 실제 사용되는 내부 IP 주소이며  목적지 IP 주소는 VPN 서버의 게이트웨이가 된다\n패킷이 게이트웨이에 도착을 하면 VPN 서버는 겉에 IP 해더를 제외한 나머지 부분이 암호화 되어있다는 것을 알고 있으니까 사전에 공유된 공개키로 복호화를 하게 된다.\n그럼 이 패킷이 도착할 원래 IP 주소를 목적지 IP 주소로, 자신의 실제 내부 IP주소를 출발지 IP 주소로 하여 해당 IP로 패킷을 전송하게 된다.\n즉 인터넷에 보내지는 패킷에 입장에서 모든 패킷에 목적지 IP 주소는 VPN 게이트웨이가 될 것 이며 내부에 데이터는 암호화 되어 보내지게 되는것이다.\n라우팅\nVPN 터널로 오는 모든 트래픽을 VPN 인터페이스가 담당을 하는데, 이게 무엇을 의미하냐면 VPN 서버 내부에 호스팅 되는 모든 서비스는 실제 물리 네트워크 인터페이스 상에서 돌아간다\n\n실제 물리 LAN 주소: 192.168.0.2\nVPN 주소 주소: 10.0.0.2\n\n위 예시로 PC에서 8080으로 호스팅 하고있는 서비스는 192.168.0.0 네트워크만 접속이 가능하다\nVPN 클라이언트로 연결된 두 장치에 IP 는 다음과 같다고 하자\n\nPC1: 10.0.0.2\nPC2: 10.0.0.3\n\n그럼 PC1과 PC2 두 서비스간 자유롭게 통신이 가능하다\n즉 원래는 해당 망에 접속 된 클라이언트 끼리 통신을 하기 위해 설계가 된 것이다.\n하지만 여기서 서버에 라우팅을 수정하여 VPN 인터페이스로 도착한 패킷을 기존에 사용하는 물리 네트워크 인터페이스에 패킷을 라우팅 하는경우\n우리가 일반적으로 사용되는 IP 우회가 가능해져서 해당 IP로 인터넷 사용도 가능해지고 192.168.0.0 대역에서 호스팅되는 서비스에도 접근 가능하다.\n해외 VPN을 사용하면 해외 망 이용속도가 빨라지는 이유\n보통의 VPN 서버들은 클라우드 서비스에서 작동한다.\nAWS 같은 대형 클라우드 서비스는 해당 지역과 직통 회선을 뚫어 놓는 경우가 있으며 통신사가 QoS 를 보장하고 있기 때문에 지연속도가 빨라질 수있다.\n즉 일반 가정용 인터넷은 라우팅 과정에서 한참을 해맬수 있기 때문이다.\n그럼 이런 결론이 나오는데 클라우드 서비스를 사용하지 않고 자체 서버를 해외에 구축 한 VPN 서버에 경우 속도향상은 개뿔 VPN 터널로 인한 오버해드만 무진장 발생할 수있다.\n프록시랑 차이\n\n프록시와는 작동 계층이 다르다\n\n프록시는 L7 즉 응용계층에서 작동하여 웹 브라우저나 프록시를 설정할 수 있게 설계된 프로그램에서만 동작을 한다.\nVPN은 운영체제 내부에 가상 드라이버 형태로 작동하여 모든 트레픽을 다른 서버를 거치게끔 하는게 가능하지만 프록시는 해당 프로그램이 지원하냐에 달려있다.\n케스팅\n유니케스팅 (Unicasting)\n\n하나의 송신자가 하나의 수신자에게 통신\n서버-클라이언트 통신 또는 PC1-PC2 통신\n\n\n\n대부분에 인터넷 통신에서 사용되며 별일 없으면 이 방식으로 통신한다\n같은 서브넷에서의 통신은 별도에 공유기나, 라우터를 거치지 않고 다이렉트로 통신한다\n\n브로드캐스팅 (Broadcasting)\n\n하나의 송신자가 연결된 모든 장치에 통신\n1:N 통신\n\n\n\n하나의 송신자가 연결된 모든 장치들에 데이터를 전송한다\n당연하게도 네트워크에 연결된 장치가 많아지면 많아질 수록 성능이 저하된다\n인터넷(외부IP) 에서는 작동하지 않으며 별도에 공유기나, 라우터를 거치지 않는다\n라우터가 ARP 를 사용하거나 DHCP 할당에 쓰인다\n\n멀티캐스팅 (Multicasting)\n\n그룹 가입이라는 시슽템을 두어 그룹에 있는 장치와 통신\n\n\n\n브로드캐스팅과는 다르게 연결된 장치들 중에서 그룹 가입을 한 장치들 한테만 전달한다\n인터넷(외부IP) 에서도 작동하며 같은 서브넷에서의 통신은 별도에 공유기나, 라우터를 거치지 않고 다이렉트로 통신한다\nIPTV 에 사용되는 IGMP 프로토콜이 해당 방식을 사용한다\n\n애니케스팅 (Anycasting) (IPv6)\n\n가장 가까운 장치에게 전송\nIPv4 에서는 베타 지원\n\n\n\n요즘 많이 사용하는 CDN 서버에 핵심 기술로 로드벨런싱 같은 분산 처리할때 많이 쓰인다\n\nGSLB (Global Server Load Balancing) &amp; CDN (Content Delivery Network)\nGSLB\n\n여러 서버의 트래픽을 전세계로 분산시키기 위한 기술\n\n\n이렇게 함으로서 각 사용자들에게 적절한 서버를 선택하여 응답을 주거나\n상태가 어떠한지 Health Check 를 진행한다\n\nCDN\n\n물리적으로 멀리 떨어저 있는 사용자가에게 서비스를 빠르게 제공하려고 고안된 시스템 방법론\n\n\n본 서버 1개(CDN DNS 서버)랑 전세계에 분산시켜놓은 케시서버 여러대를 설치한다\nCDN은 하나의 기술을 특정하는 용어가 아니라 지리적 분산, 캐싱, 부하 분산, 라우팅 같은 개념을 적용한 시스템을 의미하는 것이다\n\n그러니까 어떤 기술을 쓰건 CDN에 부합하게 결과가 나오면 그게 CDN 인것이다\n\n\n\n동작 원리\n\n클라이언트가 접속을 하면 DNS 서버에 요청을할텐데\nDNS 서버는 CDN의 DNS 서버 IP 주소를 반환한다\n\n원래 DNS 서버와 같은 느낌에 서버가 아니라 실제 물리 서버임\n\n\nCDN DNS 서버는 사용자 IP 주소를 파악해서 가장 가까운 서버의 주소로 리다이렉션을 시켜준다\n\n정리\n\nGSLB는 CDN 방법론의 핵심 기술인 것이다, 즉 CDN의 부분집합 관계인 것이다\nGSLB로 트래픽이 높거나 장애가 생긴 서버를 피하고 다른 CDN 기술들로 그 중에서 가장 사용자와 가까운 서버를 찾게 되는것이다\n\nL1 관점에서 본 네트워크 신호 전송 원리\n상위 계층에서 프레임을 전송하면 해당 프레임이 랜카드에 전송되면 랜카드는\n해당 프레임을 2진법 신호로 변환하여 UTP케이블등을 통해 전류를 흘려보낼 것이다.\n여기서는 어떻게 신호로 변환되며 신호를 어떻게 처리하는지를 서술한다\nEncoding (부호화)\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n전송과 패킷\n\n\n\n이진법 데이터를 아날로그 신호로 바꾸는 방법에 대해 서술한다\n\n\n맨체스터 인코딩\n\n예를 들어 하나의 비트를 표현하려면 1초 시간 내의 5V 에 신호를 전송해야 한다고\n치자\n0을 전송하고 싶다면 0.5초 동안 5V에 전압으로 전송하고 남은 0.5초는 0V 로 복귀 시킨다\n1을 전송하고 싶다면 0.5초를 0V 전압으로 대기 한 뒤 0.5동안 5V 전압을 전송하게 되된다.\n해당 방법은 10Mbps급 이더넷에 사용되었던 방식이라 지금은 쓰지 않고 있다.\n이외에도 여러 방법들이 있다.\n시간/공간 분활 (다중화 기술)\n하나의 공유기에 100개의 장치가 연결되어있고 100개의 장치 모두 동시에 인터넷 통신을 한다고 가정해보자.\n그럼 100개의 패킷을 받은 공유기는 다음 목적지 라우터에 하나의 회선으로 보내야 될텐데 문제는 그냥 동시에 보내버리면 신호의 간섭이 생겨버린다.\n이걸 처리하는 방법이 시간/공간 분활에 대한 문제이다.\n진폭 변조/주파수 변조\n부호화에서 5V 전압으로 예시를 들었는데\n이 전압을 각 송신자에 따라 다르게 부여 하는것이다.\n예를들어 1번은 2V로 2번은 5V로 해서 받는 수신자는 해당 전압을 판단해서 어떤 송신자인지 할 수 있게 된다.\n이것을 진폭 변조라고 한다.\n또한 부호화 예시에서 하나의 비트를 표현하는 시간을 1초로 예시를 들었는데\n이것에 시간을 다르게 하여 송신자를 구분하는 것이 주파수 변조 이다\nTDM (시분활 다중화)\n\n시간에 따라 채널을 구분하는 것\n\n이것은 하나의 회선에서 하나의 신호만 전송할 수 있게 재한 하는 방식이다.\n송-수신 측이 하나의 신호를 받는 시간을 정하고 정해진 시간 만큼 신호를 보내게 된다.\n전송이 끝나면 다음 사이클에서 또다른 송신자에 신호를 전송하고 다시 넘기고 이렇게 흘러간다.\nFDM (주파수 분활 다중화)\n\n앞서 설명한 주파수 변조를 사용하여 여러개의 채널을 구분하는 것\n\n송신자들에 주파수를 다르게 설정하여 하나의 회선에 동시에 보내도 문제 없게 끔 만들어 준다.\nCDM (코드분할 다중화)\n\nTDM+FDM\n\n현재 네트워크에서는 두가지 방식을 합친 CDM을 사용한다\n전이중 통신\n\n송-수신 회선을 각각 따로 사용하는 것\n\n요즘 네트워크는 대부분 전이중 통신으로 송-수신 신호 간에 통신에는 간섭 문제가 발생하지 않는다.\n또한 아예 송-수신 회선을 더 많이 뚫어서 물리적으로 동시 전송을 가능하게 하기도 한다.\nCSMA/CD\n\n다중화 기술과 전이중 통신이 나오기 이전에 쓰였던 초기 이더넷에 신호 간섭 문제 해결 프로토콜\n\n현재는 사용하지 않으므로 크게 중요하진 않다.\n과정\n통신하고 싶은 PC는 먼저 네트워크 상에 통신이 일어나는지를 확힌한다 (Carrier Sense)\n네트워크 통신이 일어나고 있으면 (캐리어가 감지되면) 데이터를 보내지 않고 기다리고\n네트워크 통신이 일어나고 있지 않을 때 (캐리어가 감지되지 않으면) 데이터를 보낸다.\n만일 케리어가 감지되지 않을때 두PC가 동시에 데이터를 전송한다면 충돌이 일어난다. 이것을 Multiple Access라고 한다\n두 PC는 다시 랜덤한 시간동안 기다렸다 다시 데이터를 전송한다\n네트워크 장비\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nL1, L2, L3, L4, L5, L6, L7 스위치란?\n\n\n\nL1\n허브\n\n\n단순히 컴퓨터와 컴퓨터를 이더넷으로 이어주는 장치\n\n허브는 스위치랑 연결 방식은 비슷하지만 라우팅과 게이트웨이라는 개념이 존재하지 않는다.\n따라서 연결된 모든 장치는 특정 장치가 패킷을 받을경우 허브는 그냥 그걸 모두에게 브로드 케스팅 해버린다. (목적지가 누구인지를 모르니까)\n그래서 허브는 단순히 전기 신호를 전달해주는 역할이다.\n사실상 이런 문제로 인해 스위치에 밀려서 거의 사용되지 않는다\n리피터\n리피터는 사실상 허브에서 분배 기능을 제외하였다고 봐도 무방하다.\n하나의 네트워크에서 나온 신호를 증폭해주는 역할을 한다.\n모뎀 (최근 L2, L3)\n\nISP로부터 받은 아날로그 데이터를 디지털로 변환 또는 그 반대 역할을 수행한다.\n\n외부 네트워크 특성상 내부에서 사용하는 UTP 케이블이 아닌 광케이블을 사용하게 된다.\n여기서 UTP는 전기신호를 매게로 하고 광케이블은 빛의 신호를 메게로 한다.\n즉 디지털 신호로 통신하는 UTP, 아날로그 빛의 신호로 통신하는 광케이블 둘 간에 신호 변환을 담당한다\n근데 최근 나오는 통신사 모뎀들은 라우터 기능까지 달려있는 경우가 대부분 이지만\n일단 모뎀 자체의 기능은 L1에 가깝기 때문에 여기다 서술하였다.\nL2\nL2 스위치 (스위치 허브)\n\n허브와 달리 스위치는 MAC 주소까지 디캡슐레이션을 하여 해당 MAC주소로 패킷을 전달해 주는 장비이다.\n\nL3, L7 스위치도 존재하지만 보통 스위치라 함은 L2 스위치를 의미한다.\n기본적인 기능은 ARP테이블을 관리하고 해당 테이블 기반으로 패킷전달을 한다.\n라우터와 공유기와는 다르게 게이트웨이(L3) 개념은 존재하지 않아\n자체적으로 IP를 할당받지 못하며\n인터넷 WAN 케이블을 그대로 스위치에 보통은 연결 하면 단 하나의 장치만\n인터넷 사용이 가능하다.\n(단 ISP가 모든 장치에 공인 IP를 할당 해 줄수있는 상황이면 가능하다)\n\n따라서 보통 스위치는 위에 처럼 구성한다.\n오해 하면 안될것이 스위치에 연결된 모든 장치들은 각각의 다른 IP주소를 가질 수 있다.\n보통 연결된 각 장치가 IP할당 요청을 하는 구조라 스위치는 해당 요청만 재대로 전달하면 공유기가 IP할당을 해서 MAC주소로 관리되기 때문이다.\n또한 스위치는 사실상 스위치 허브랑 동일한 용어다.\n원래는 허브를 부르는 말이 스위치 허브였는데 지금은 그냥 스위치로 의미가 바뀐거 같다.\n브릿지 (사실상 스위치)\n\n사실상 스위치의 하위 호환이라고 봐도 무방하다.\n\n브릿지는 일반적으로 2개의 포트를 가지며 네트워크를 연결하는 간단한 기능만 제공하고 있다.\n사실상 스위치=브릿지 라고 생각해도 무방하다\nL3\n라우터\n\n\nIP 기반으로 패킷이 가야될 최적에 경로를 지정하고, 데이터 패킷을 전달하는 장치\n\nL3 이므로 라우터 본인이 IP를 할당받을 수 있으며 해당 IP를 연결된 LAN 장치들은 NAT을 통해 하나의 외부 IP를 할당 받아 사용할 수 있다.\n연결방식과 작동구조는 L2스위치와 유사하지만 조금 더 많은 기능을 제공하는 장치이다.\n라우팅과 기타 부가 서비스 (VPN 연결, QoS 등)기능들을 제공된다.\n사실 L2스위치 없이 라우터만 있어도 라우터가 스위칭 까지 하므로\n스위치를 살빠엔 라우터 사는게 맞지 않냐라고 할 수 있는데\n내부 망 포트 확장 목적으로 라우터를 사봤자 상위 계층까지 디캡슐레이션 해야되므로 오버헤드만 증가하니 이럴 땐 L2스위치만 사는게 좀 더 빠르다.\nL3 스위치 (사실상 라우터)\n\nL2 스위치 기반으로 라우팅 기술을 추가한 장비\n\n현재는 L3 스위치를 라우터로 봐도 무방하다\n근데 라우터에서 기능을 좀 제거하여 그냥 라우팅만 하는 친구로 만든게 L3 스위치이다.\n그래서 라우터 보다 좀 더 저렴하고 라우팅과 스위칭에 특화된 기술만 들어간 친구가\nL3 스위치 였는데… 지금은 그냥 똑같아 졌다.\n다나와 제품 검색 결과 라우터쪽이 포트가 더 적고, 스위치 쪽은 포트가 좀 더 많이 존재하는 차이가 있는거 같다\n공유기 (라우터에 기능 추가한거)\n\n라우터에서 가정용으로 쓰기 좋은 기능을 추가하고 라우팅 기능을 간소화 한 것\n\n라우팅 설정은 간편화 하고, 가정이나 소규모 네트워크에서 쓸만한 기능들 좀 더 추가하여 만든 라우터가 공유기 이다.\n지금은 무선 Wi-Fi를 지원하면 공유기로 보고, 아니면 라우터로 보는 이런 추세 인듯하다.\nL4~L7\nL4/L7 스위치\n\n서버에 부하 분산이나, 하드웨어 방화벽 역할을 함\n\nIP + 포트번호 기반 참조 지원 시 L4로\nHTTPS/HTTPS등과 같은 프로토콜 기반까지 같이 참조하면 L7\n서버 앞단에 성능저하 없이 보안수준을 끌어 올리거나 로드벨런싱 할때 사용하는데\n예시로 통신사가 SNI 필드 검증을 하여 인터넷 검열을 수행할 때 해당 장비를 사용한다.\n기본 가격이 100만원은 훌쩍넘어서 개인이 사용하기에는 꽤나 부담스럽다\nHFC / FTTH\n\nISP의 인터넷의 전송방식\n\nHFC (Hybrid Fiber Coaxia)\n\n비대칭 인터넷 (유사광랜)\n\n\n구형 인터넷 구축 방식, 광케이블을 아파트나, 주택 지하에 구내 증폭기에 연결하여 구내 증폭기는 분배기를 이용하여 각 세대에 인터넷 케이블을 보급하는 구조\n인터넷 사용 시 주파수 다운로드 채널을 업로드 채널보다 더 많이 배치하여\n비대칭 인터넷 문제로 인해 논란이 많은 친구다.\n2000년대 이전 아파트는 해당 방식일 가능성이 높다.\nFTTX (Fiber To The x)\n\n대칭 인터넷 (광랜)\n\n\n광케이블을 어디까지 보내느냐의 대한 기술\n이것을 보통 부르는 광랜 이라고 함\n\nFTTN:\n전신주나 길가에 광케이블이 깔리고 집안까지는 랜선이 들어오는구조\nFTTB:\n건물 까지만 광케이블이 들어가있는 구조, 아파트 지하나, 관리사무소 쪽에 통신 배전반이 설치되고 해당 배전반에서 랜선으로 보급 해주는 방식\nFTTH:\n직접 광케이블을 가정내에 보급하는구조, 최근 건설된 아파트나, 1G 이상의 기가인터넷에서 많이 쓰인다.\n\n대부분의 구축 아파트가 FTTB 수준인데 보급되는 LAN선이 CAT.5 수준밖에 안되는 경우가 있다. 이럴 경우 각 가정에서 증폭 광모뎀을 설치하여 500 Mbps 기가 라이트 인터넷 정도는 이용할 수 있게 된다.\n\n우리 아파트도 이런구조인듯 하다\n\n기타\nWinodws 공용 네트워크 문제\nWinodws내 네트워크 설정이 공용 네트워크인 경우 포트를 열어도 접근이 불가하다\n이게 공용 네트워크랑 개인 네트워크랑 방화벽 포트 설정이 달라서 그렇다\n따라서 개인 네트워크로 변경해야 함\n\n설정 → 네트워크 및 이더넷  → 네트워크 프로필\n"},"스터디/데이터-베이스-정규화":{"title":"데이터 베이스 정규화","links":[],"tags":[],"content":"\n각 단계별 기본조건은 이전 단계 조건들을 전부 만족해야 한다\n실제로는 BCNF 정규형 까지 정도만 지켜줘도 OK 이다\n\n\n1정규형 (1NF)\n\n모든 속성의 도메인은 원자값으로 구성되야 한다\n순서가 보장되야 하는 작업에 경우에는 행 순서를 사용하지 않고 별도의 속성을 사용해야한다\n반복그룹은 허용하지 않는다\n기본키가 없는 테이블을 갖는건 허용하지 않는다\n\n1번째\n\n모든 속성의 도메인은 원자값으로 구성되야 한다\n튜플에 키, 몸무게 이런식으로 여러값을 집어넣지 말라는거\n\n예시\n\n문제: 토마토를 가진 사람을 찾으시오\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템player1사과player2감자player3사과, 감자, 토마토\n\n이 경우 like 연산자를 쓸 순 있지만 로직이 복잡해지고 성능이 좋지 못하다\n또 하나는 삽입할 때 인데  일단은 update 연산을 써야한다 근데 그럴려면 기존에  player3 의 아이템 값을 불러와서 거기에 추가를 시키고 업데이트 해야 하는 극악의 효율을 보여준다\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템player1사과player2감자player3사과player3감자player3토마토\n\n그래서 각 값들로 행을 분리해 줘야 한다\n\n2번째\n\n순서가 보장되야 하는 작업에 경우에는 행 순서를 사용하지 않고 별도의 속성을 사용해야한다\n\n예시\n\n문제: 나이순으로 오름차순으로 정렬하시오.\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n맴버혜인하니민지다니엘혜린\n\nDB에서 튜플의 순서는 어떤 의미를 지니고 있질 않다.\n즉, 순서대로 저장했다고 해서 순서대로 반환 해줄거라는 기대를 하면 안된다\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n맴버나이혜인16하니19민지20다니엘19혜린18\n\n이렇게 수정하면 나이를 기준으로 정렬하거나 하는게 가능하다\n\n3번째\n\n반복 그룹은 허용하지 않는다\n\n예시\n\n문제: 플래이어5에 새로운 아이템을 추가한다\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템1아이템2아이템3player1사과감자player2감자토마토player3사과player4감자player5사과감자토마토\n\n이 경우 다음 테이블인 아이템4 속성이 없는지 판단해서 있으면 해당 속성에 추가하고 없으면 속성을 만들고 거기다 값을 업데이트 한다\n이런 미친 비효율성\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템player1사과player1감자player2감자player2토마토player3사과player4감자player5사과player5감자player5토마토player5옥수수\n\n그럼 새로운 아이템이 추가되면 그냥 insert 문 한번이면 끝난다.\n순서가 필요하면 속성 하나추가해서 부여해주면 끝이기에\n\n2정규형 (2NF)\n\n기본키가 아닌 모든 속성에 대하여 기본키에 종속적이어야 한다\n\n\n인벤토리 테이블에 갑자기 플레이어 랭크 속성이 있으면 안된다.\n\n예시\n\n문제: player1에 랭크값을 업데이트 하라\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템플레이어 랭크player1사과브론즈player2감자실버player3사과다이아player1감자브론즈\n\n해당 플레이어가 모든 아이템을 잃으면 자연스럽게 해당 플레이어의 랭크값이 사라진다 (삭제이상)\n만일 랭크에 업데이트가 잘못되어 사과 가 있는 튜플만 실버가 됬다고 치자 그럼  player1 은 브론즈 이면서 실버라는 모순이 생겨 버린다 (갱신이상)\n\n또한 아이템이 막 100개라고 모든 값에 대해 업데이트 해줘야 한다, 즉 오버해드가 장난아니다\n\n\n만일 새로운 플레이어가 가입했다고 치자 그럼 자연스럽게 등급은 브론즈 로 부여되야 한다, 하지만 당연하게도 인벤토리에 어떤 아이템도 없는 상태일 것이다\n즉 삽입이 불가능한 모순이 생겨 버린다 (삽입이상)\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어아이템player1사과player2감자player3사과player1감자\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어랭크player1브론즈player2실버player3다이아\n\n두 속성이 서로 종속되지 않도록 플레이어 테이블과 랭크 테이블을 분리 시킨다\n\n3정규형 (3NF)\n\n기본키가 아닌 모든 속성은 기본키에만 의존해야한다\n\n\n기본키를 제외한 속성간에 이행 족속성을 허용하지 않는다\n\n예시\n\n스킬레벨: 14=브론즈, 59=실버, 10~15=다이아 라고 가정\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어랭크스킬레벨player1브론즈4player2실버9player3다이아14\n\n만일 player1 이 레벨을 올려 5레벨을 달성했다 치자 그러면 랭크도 실버로 바꿔야 하는데 문제가 생겨 업데이트가 안됐다면, 레벨은 5인데 브론즈라는 모순이 생긴다\n문제가 발생한 부분은 랭크가 스킬레벨에 종속적이라는 것이다\n\n플레이어 → 랭크 → 스킬레벨 의 관계가 성립한다\n즉 랭크와 스킬레벨은 분명 플레이어 에 종속적이지만 랭크 ←&gt; 스킬레벨 의 관계가 성립하기 때문이다\n\n\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n플레이어스킬레벨player14player29player314\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n스킬레벨랭크1브론즈2브론즈3브론즈4브론즈5실버6실버7실버8실버9실버10다이아\n\n즉 미리 스킬레벨에 따른 랭크 테이블을 만들어 두고\n플레이어 레벨이 올리면 레벨만 업데이트 해주고 랭크가 필요할땐 두 테이블을 Join 해서 사용한다\n\nBCNF\n\n모든 결정자가 후보키가 되도록 테이블을 분리한다\n\n예시\n\n학생은 여러 교수의 수업을 들을 수 있고 한 교수당 하나의 과목만 맞는다\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n학번과목명담당교수100C123P1100C234P2200C123P1300C234P3400C234P4\n\n과목명 이 바뀌면 무조건 담당교수가 바뀌거나 담당교수가 바뀌면 과목명이 달라져야 하는것은 아니라 3NF를 만족한다고 볼 수 있다\n여기서 문제는 학번을 담당교수가 결정한다라고 볼 수 없다 즉, 후보키가 될 수 없다\n새로운 교수가 와서 다른 과목이 추가되야 될 상황인데 문제는 수강생이 없는 상태라 이 상태에서는 추가 할 수 없다 (삽입이상)\n만약 100번 학생이 C234 과목을 취소한다면 P2가 C234를 담당한다는 정보도 같이 사라진다 (삭제이상)\n만일 과목명, 또는 담당교수가 변경되면 모든 행을 찾아 변경해줘야 하는데 이과정에서 오버해드가 장난 아닌게 발생한다 (갱신이상)\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n과목명담당교수C123P1C234P2C234P3C234P4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n학번과목명100C123100C234200C123300C234400C234\n\n새과목이 추가되면 담당교수 테이블에만 추가해두고\n수강취소를 하면 수강신청 테이블에서만 지우면되고\n과목명 수정의 경우 수강신청 테이블에 과목명은 담당교수 테이블에 과목명에 외레키로 참조 해두고 과목명이 변경되면 담당교수 과목명만 수정하면 그만이다\n\n4정규형 (4NF)\n\n다치 종속성을 제거한다\n\n예시\n\n색상과 스토리지 조합은 일치해야한다\n다만 핑크 색상은 512GB 모델에만 존재한다\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델색상스토리지S24화이트512GBS24화이트1TBS24블루512GBS24블루1TBS24핑크512GBS24+화이트512GBS24+화이트1TBS24+핑크512GB\n\n만약 S24+에 블루 색상이 추가된다고 치자, 그럼 512GB 모델  1TB 모델 둘다 지원해야 되니 행을 두게나 추가 해야한다.\n극단적인 예시로 만약 스토리지 종류가 100가지가 추가 되있다면 색상하나 추가 될때마다 100가지 행을 추가해야 되는 미친 상황이 나타난다\n분명 스토리지와 색상은 서로 독립적인 관계지만 하나의 속성을 삽입하려면 그것에 종속되어 여러값을 추가해야 하는 다치종속성 관계가 생겨버린다\n다만 예를들어 S24+ 의 512GB 에서만 오렌지 색상이 추가된다고 가정하면 이는 다치종속 관계로 볼 수 없다 즉 해당 정규형 적용이 불가하다\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델스토리지S24512GBS241TBS24+512GBS24+1TB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델색상S24화이트S24블루S24핑크S24+화이트S24+핑크S24+블루\n\n다치 종속되는 두 부분을 분리하여 필요할때 마다 Join 하여 사용한다\n\n5정규형 (5NF)\n\nJoin 종속성 제거\n\n예시\n\n4NF 에서 분리한 테이블을 다시 Join 하였다\n\nBefore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델색상스토리지S24블루1TBS24핑크1TBS24화이트1TBS24블루512GBS24핑크512GBS24화이트512GBS24+블루1TBS24+핑크1TBS24+화이트1TBS24+블루512GBS24+핑크512GBS24+화이트512GB\n\n이전에는 없던 1TB, 핑크 모델이 생겨났다\n스토리지의 색상 조합의 전부 일치해야만 하는 경우에는 상관없지만 일부만 유효한 (핑크색상은 512GB 모델에만 존재하는 것처럼) 데이터는 무시되고 모든 조합이 추가가 된다\n\nAfter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델스토리지S24512GBS241TBS24+512GBS24+1TB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델색상S24화이트S24블루S24핑크S24+화이트S24+핑크S24+블루\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n색상스토리지화이트512GB블루512GB핑크512GB화이트1TB블루1TB\n\n스토리지에 따른 색상을 추가하여 문제를 해결한다\n즉 모든 경우의 대하여 테이블을 분리해준다\n\n용어정리\n키\n\n후보키 (Candidate Key):\n\n기본 키로 쓸 수 있는 속성의 집합을 의미\n테이블의 모든 튜플을 고유하게 식별할 수 있는 속성들의 집합\n\n\n기본키 (Primary Key)\n\n주 키\n\n\n대체키 (Alternate Key)\n\n기본키가 되지못한 나머지 후보키\n\n\n슈퍼키 (Super Key)\n\n속성들의 집합으로 구성된 키 (주민번호 +성명)\n\n\n외래키(Foreign Key)\n\n다른 테이블 기본키를 참조하는 키\n\n\n"},"스터디/문자-인코딩":{"title":"문자 인코딩","links":[],"tags":[],"content":"유니코드와 문자 인코딩\n\n유니코드는 전세계 모든 문자를 2바이트(+4비트)로 구성된 문자 인코딩 표준\n\n\n흔히 유니코드라고 부르는건 표준 그 자체를 말하는 것으로 문자 인코딩이 아니다\n유니코드의 문자 인코딩에는 UTF-8, UTF-16등 이 존재한다\nascii 의 경우 그 자체가 문자열 집합과 인코딩을 포함 하고있는 것\n유니코드 문자 라고 부르는거는 보통 유니코드 포인트 를 의미하거나\n코드 포인트 안에 포함된 문자 그 자체를 의미\n\n문자 데이터 / 바이너리 데이터\n\n문자도 결국은 바이너리\n그러나 웬만한 프로그래밍 언어들에서는 둘을 구분하여 씀\n문자열은 다양한 인코딩이 존재하여 인코딩 마다 바이너리를 처리하는 것이 재각각임\n그래서 다루기 쉽게하기 위해서 바이너리의 문자 인코더를 추가하여 추가적인 API 를 구성하고 있음\n\n문자 인코딩\n\n예제는 java 이지만 다른 언어도 마찬가지\n\n\n\n                  \n                  참고 \n                  \n                \n\n\n한글 문자 인코딩\n\n\n\nString 객체\n\nString 객체 생성을 통한 인코딩의 잘못된 예\n\nString str = new String(&quot;안녕&quot;.getBytes(&quot;UTF-8&quot;), &quot;UTF-8&quot;);\n\n해당 코드는 정말 의미 없는 코드이다\n\n안녕 이라는 글자의 UTF-8 로 인코딩된 byte[] 로 반환 받는다\n그리고 String 객체는 UTF-8로 인코딩된 byte[] 를 UTF-8  로 해석 하여 문자열 안녕 을 얻는다\n\n\n얼핏 봐서는 문자열 안녕 을 UTF-8 로 인코딩 한다고 생각할 수 있지만\nnew String() 이 하는 건 어떤 것으로 인코딩 된 byte[] 문자열을 같은 인코딩으로 해석하여 올바르게 String 으로 불러올 수 있게 하는 것 뿐이다\n\nString str = new String(&quot;안녕&quot;.getBytes(&quot;UTF-8&quot;), &quot;EUC-KR&quot;);\n\n그렇다고 둘을 다르게 설정한다고 의미 있어지는 것도 아니다\nUTF-8 로 인코딩된 byte[] 를 String 객체가 EUC-KR로 해석 하겠다는건데\n이러면 당연히 문자열이 깨지게 된다. 그래서 깨진 문자열 그대로 str 에 저장된다.\n당연하지만 잘못된 인코딩으로 원본 정보가 손실 되어 UTF-8 로 변환한다 해서 복원 할 수 없다\n\n원래 용도\nString str = new String(reader.readLine().getBytes(&quot;EUC-KR&quot;), &quot;EUC-KR&quot;);\n\nInputStream 이나 기타 파일 I/O 객체를 통해 읽은 데이터에 인코딩을 결정하는데 쓰인다\n어떤 파일이 EUC-KR 인코딩 셋을 사용하여 그냥 읽을 경우 문자열이 깨지는데 이걸\nEUC-KR로 읽고 String 이 EUC-KR 로 해석하여 str에 올바른 문자열이 저장되도록 한다\n\n쓰기\noutputStream.write(&quot;안녕&quot;.getBytes(&quot;UTF-8&quot;));\n\n만약 outputStream 의 해당되는 프로그램의 인코딩 셋이 깨지는 경우\nbyte로 변환할때 해당 프로그램과 동일한 인코딩 셋을 적용해야 한다\n\n유니코드 포인트\n\n전세계 모든 유니코드 (149,878개) 를 하나의 특수코드로 표기한 것\nen.wikipedia.org/wiki/List_of_Unicode_characters\n\n\n알려진것과 다르게 실제론 2바이트가 아닌 21 비트 즉 2바이트+4비트\n\n표기법\n\nU+AC00: 가, U+D7A3: 힣\n\n보통 코딩에서 한글인지 판단할 때 가~힣 까지의 문자인지 판단해야하는게\n바로 이 유니코드 포인트 규칙에 있음\n\n\n\nSupplementary Planes\n\n원래 유니코드 포인트는 2바이트로 즉 65535 까지 밖에 존재 할 수 없다\n그래서 문자열을 확장하려고 만든 것\n\n\n추가적으로 4 비트를 확장하여\nU+10000 ~ U+10FFFF 까지의 문자열 범위를 말한다\n\n프로그래밍 에서\n\n보통은 유니코드 문자열을 UTF-16 형태로 유니코드 포인트를 메모리에 저장하여\n원하는 인코딩으로 바이너리를 쓰는 형태임\n대부분에 문자는 2바이트 자료형(Char)로 저장 할 수 있지만 Supplementary Planes의 존재 때문에 해당 범위에 해당하는 문자열은 4바이트 까지 확장할 필요가 있다\n\n유니코드 이스케이프 시퀀스\n\n특정 문자를 명시적으로 표현 할 수 없을때 16진수 형태로 표현하는 방법\n종류: learn.microsoft.com/ko-kr/cpp/c-language/escape-sequences\n\n\n주로 문자열을 \\uC11D이런 식으로 유니코드 포인트로 표기를 할 수 있는\n\n해당 문자는 석 이다\n유니코드 포인트 상에서는 U+C11D\n\n\n\\n, \\t 등 이런 문자열도 포함이다\njava 에서 소스코드 컴파일 시 유니코드가 포함된 문자열은 전부 이스케이프 처리된다\n"},"스터디/비동기와-멀티스레드-차이":{"title":"비동기와 멀티스레드 차이","links":[],"tags":[],"content":"\n\n                  \n                  참고자료 \n                  \n                \n\n\n비동기와 멀티스레딩\n멀티 스레드와 단일 스레드, 동기와 비동기 제대로 알고 쓰시나요? (feat. 성능 개선)\n\n\n\n\n비동기\n실생활 예\n\n나 혼자 방 청소와, 빨래를 한다고 생각해보자.\n비동기적 접근방법은 방청소 10% 만 해놓고 빨래 10% 하러가고 다시 와서 방청소 20 % 하고\n동기적으로 생각하자면 방 청소 100% 완료 후 빨래를 할 것이다.\n\n설명\n1: ThreadPoolTaskExecutor defaultTaskExecutor = new ThreadPoolTaskExecutor();\n2: defaultTaskExecutor.setCorePoolSize(10);\n3: defaultTaskExecutor.setMaxPoolSize(20);\n4: defaultTaskExecutor.setKeepAliveSeconds(3);\n\n원래라면 코드는 1→2→3→4 번 순으로 진행될 것이다\n하지만 비동기에서는 코드기 1→4→3→2 순으로 진행 될 수 있다\n해당 코드의 진행 속도나 이런것에 따라 비동기 라이브러리가 적절하게 작업을 분리한다\n\n중요점\n\n비동기는 단순히 실행 순서가 보장되어있지 못하는 것이다\n\n제일 오해하면 안되는 것이 순서가 보장되어있지 않기 때문에\n자칫하면 코드가 동시에 실행된다고 오해하는 것이다\n\n\n비동기는 작업 속도의 개선을 기대할 수 없다\n\n비동기의 목적은 Non-Blocking 이다\n하나의 오래 걸리는 작업으로 인해 다른 작업들이 실행 되지 못하는 현상을 막고자 하는것이다.\n단일 쓰레드로 작동하는 JS의 경우 Fetch() 와 같은 네트워크 작업 등 에서 발생되는 대기 시간 때문에 Event-Callback을 제대로 처리하지 못하거나 하는 불상사를 막는다\n예를 들어보면, 10개의 작업을 전부 수행 해야만 다음 작업을 수행 할 수 있는 프로그램을 만든다 치자\n\n다음 작업에 동작 속도를 개선 한답시고 10개의 작업을 모두 비동기 처리를 한다고 하더라도\n비동기는 단순히 실행 순서만 바뀌는 것이기 때문에\n최종 작업 속도는 동일하거나 오히려 늦을 수도 있다 (비동기 처리과정에서 오버해드가 발생할 경우)\n\n\n\n\n비동기도 Blocking의 완벽한 무적은 아니다\nconst ayn = new Promise((r) =&gt; {\n    for (let index = 0; index &lt; 100000; index++) {\n        console.log(index);\n    }\n    r();\n})\n \nayn();\n\n예시로 이런 코드가 있을 때 Promise() 로 감싸서 비동기를 유도했지만\n문제는 for-loop 가 너무 빨리 돌아서 비동기 큐에 해당 for-loop의 작업만 저장되있게 된다\n그래서 사실상 for-loop 100000번 다 돌때까지 다른 작업이 거의 실행 되지를 못한다\n사실 근데 예제 자체에 Promise 를 반환하는 코드가 없어서 그냥 동기로 실행 되서 그냥 참고만 하자\n\n\n\n멀티스레드\n실생활 예\n\n이번엔 두 사람이 방 청소와, 빨래를 한다고 생각해보자.\n1번 사람은 청소를 하고, 2번 사람은 빨래를 하는것이다.\n즉 멀티쓰레딩은 그냥 작업자를 2명 투입한다는 느낌인 것이다\n\n설명\n1: ThreadPoolTaskExecutor defaultTaskExecutor = new ThreadPoolTaskExecutor();\n2: defaultTaskExecutor.setCorePoolSize(10);\n3: defaultTaskExecutor.setMaxPoolSize(20);\n4: defaultTaskExecutor.setKeepAliveSeconds(3);\n\n1, 2, 3, 4 번 코드를 각각의 단일 쓰레드로 구성하여 실행한다고 가정\n그럼 1,2,3,4 번 코드는 각각 독립적으로 동시에 실행을 한다\n\n중요점\n\n서로 독립적이여도 되는 작업에서 사용하는것이 좋다\n\n멀티쓰레딩 특성상 다른 작업이랑 연개되야하는 경우 처리가 매우 까다롭기 때문\n\n\n자신의 CPU 쓰레드 수를 초과해서 생성하는 경우\n초과된 쓰레드는 사실상 비동기로 작동한다\n\n나의 CPU는 I7-7700K 인데 쓰레드 수가 8개 이다\n만약 8개를 초과하여 쓰레드를 생성한 경우 CPU 스케줄링(FCFS, SJF, etc…) 에 의해 비동기와 비슷하게 작동하게 된다\n\n컨텍스트 스위칭: n개의 쓰레드 수가 있을때 넘치는 쓰레드에 대해서는 해당 쓰레드를 빠르게 전환하여 처리\n해당 쓰레드가 작업중인경우 잠깐 작업을 중단하고 새 쓰레드 작업 처리하고 이런식임\n\n\n지금은 흔치않지만 단일 쓰레드 CPU에 멀티쓰레딩은 그냥 비동기 처리라 봐도 문제가 없다\n\n\n\n동시성 문제 (Concurrency Issues)\n경쟁 상태 (Race Condition)\n\n두개 이상의 쓰레드가 동일한 자원에 접근하면서 발생하는 문제\n\n\n변수의 값이 0일 때, 두 스레드가 동시에 그 변수를 읽고 1로 증가시키려고 할때\n두 쓰레드가 참조되는 값이 0으로 동일해서 2가 아닌 1이 나오는 현상\n\n데드락 (Deadlock)\n\n동기화 문제를 해결하기위해 뮤텍스와 같은걸 도입할때\n\n\n두 개의 쓰레드가 서로 Lock 을 걸었는데\n둘 다 다른 쓰레드가 오기를 기다리는경우 무한대기 상태에 빠지게되는\n\n동기화 기법\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n종류기능임계 영역(critical section)공유자원에 대해 오직 한 스레드의 접근만 허용  (한 프로세스에 속한 스레드 간에만 사용 가능)뮤텍스(metex)공유 자원에 대해 오직 한 스레드의 접근만 허용  (서로 다른 프로세스에 속한 스레드 간에도 사용 가능)이벤트(event)사건 발생을 알려 대기 중인 스레드를 깨운다.세마포어(semaphore)한정된 개수의 자원에 여러 스레드가 접근할 때, 자원을 사용할 수 있는 스레드 개수를 제한한다.대기 가능 타이머(waitable timer)정해진 시간이 되면 대기 중인 스레드를 깨운다.\n멀티프로세싱\n\n현재 프로그램을 실행하는 프로세스와 완전히 다른 프로세스를 생성하는것\n\n\n멀티쓰레딩에 경우 메인쓰레드와 동일한 Memory-Map 을 할당받는데\n멀티프로세싱은 아예 독립적인 Memory-Map 과 시스템 자원을 할당받는다\n사실상 완전히 병렬로 실행되므로 언어에따라 멀티쓰레딩으로도 멀티코어를 활용하지 못하는 경우 사용하기 좋다\n\n사용할 때\n\n복잡한 계산을 하거나, CPU 사용량이 많을 때\n\n이게 각각에 프로세스 별로 자원이 제한되있는 경우가 있어서\n아예 다른 프로세스로 빼버리면 멀티쓰레딩 보다 훨씬 빠를 수 있다\n\n\n무조건 독립적일 때\n\n이게 프로세스가 진행중일때는 메모리 참조가 안되므로 프로세스가 다\n끝날 경우 IPC로 끌어다 쓰는 느낌으로 가야되서 그렇다\n\n\n\n멀티쓰레드 VS 멀티프로세싱\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n특징멀티쓰레딩멀티프로세싱메모리 사용모든 스레드가 메모리를 공유각 프로세스가 독립된 메모리 공간 사용병렬성CPU 바운드 작업에서 실제 병렬성이 제한됨CPU 코어별로 병렬 처리가 가능I/O 바운드 작업네트워크, 파일 I/O 작업에 적합비효율적일 수 있음CPU 바운드 작업성능 향상이 제한적멀티코어 CPU를 활용한 병렬 처리 가능오버헤드스레드 간 오버헤드가 적음프로세스 생성 및 관리 오버헤드가 큼통신메모리 공유로 스레드 간 통신이 쉬움프로세스 간에는 IPC(Inter-Process Communication)를 통해 통신해야 함동기화 문제동기화 문제 발생 가능 (Deadlock, Race Condition)독립적인 메모리 공간 사용으로 동기화 문제 없음\n참고\n\nProsces=작업, Thread=연산\n하나의 코어는 하나의 작업만 가능하다, 다만 CPU 쪽에서 하이퍼쓰레딩을 지원하는 경우 대부분 코어*2개의 작업까지 동시에 수행 가능하다\n\n코어수(전체쓰레드 수)를 초과하는 작업은 CPU가 스케줄링 하는거라 실제로 동시에 진행 한다고 볼 수는 없다\n\n\n"},"스터디/암호화":{"title":"암호화","links":[],"tags":[],"content":"비대칭 암호\n\n암호화 키와 복호화 키를 서로 다른 키를 사용하는 방식\n\n대칭 암호화에 경우 암호 키(어떤 글자가 뭘로 바꿨는지 정보)만 알면 어떤 평문이든 쉽게 복호화가 된다는 문제가 있다. 즉 암호 키 교환 과정에서 중간자 공격에 매우 취약하다.\n비대칭 암호화는 암호화에 사용되는 Pubilc키 와 복호화에 사용되는 Private키를 따로 사용하여 1번 Pubic키로 암호화한 평문은 반드시 1번 Private키로만 복호화 가능하다\n따라서 암호 키 교환 과정 시 Pubilc키 만 교환하여 중간자 공격을 하더라도 그걸 복호화 할 수 있는 Private키가 없다면 사실상 무용지물이다.\n보통은 공개키로 암호화, 비밀키로 복호화 하지만 그 반대도 가능하다\n\n키-쌍:\n비대칭 키를 생성할 때는 반드시 Pubic키 에 맞는 고유한 Private키를 쌍으로 생성해야 된다. 단순히 Pubic키 따로 Private키 따로 생성하는게 아니라\n\n현재 많이쓰이는 RSA 알고리즘이 대표적이다\n암호화 원리\n암호가 지켜지는 기본적인 원리는 컴퓨터의 소인수 분해에 있다.\n123x123 같은 단순 곱셈 연산에 경우 아무리 큰 수라도 2진법 계산이 가능한 컴퓨터는 전부 계산 할 수 있다.\n하지만 884339를 소인수 분해를 하라라고 했을때 사실상 노가다 말고는 답이없다. 이제 저 주어진 수를 천문학적인 단위로 바꿔버린다면 몇백년이 걸려도 현재 컴퓨터에서는 풀수가 없다… 에초에 소인수 분해라는게 나누는거 밖에는 별다른 규칙이 없어서\n이것이 현재 리만 가설의 주된 논제 중 하나이며 P/NP 문제의 NP(다항 시간내에 판결하지 못하는 문제)에 해당한다.\n암호 키 과정을 간략하게 서술하면\n\n두 소수 p, q 를 지정한다. (p=11, q=37)\np 와 q 를 곱한 수를 구한다 (407)\n공개키에는 407과 특정 수 e 을 지정하여 두 수의 연산을 통하여 암호키를 생성한다\n(이때 e 값은 두 p, q 에 대한 어떤 수식의 의해 결정된다)\n비밀키에는 407과 특정 수 d 값에 대한 정보를 포함하게 되어\n(이때, d 값은 어떤 수식에 의해 결정되는데 위 공개키에서 결정된 값으로 암호화된 구문에 원래값을 찾을 수 있는 값이다)\n암호화된 구문을 복호화 할 수 있도록 한다\n\n비대칭 암호화에서 소인수분해 문제가 나온 것이 복호화를 위한 비밀키가 없을 경우 공개키에 값을 소인수 분해를 통해 복호화가 가능한 취약점이 있기 때문 인 것이다.\n암호화 과정\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nnamu.wiki/w/RSA%20%EC%95%94%ED%98%B8%ED%99%94#s-2.1\n{보안/암호} RSA 암호화 복호화\n\n\n\n두 소수 p와 q를 11과 37이라 하자. 그리고, 10(p-1)과도 36(q-1)과도 서로소인 정수 e을 7로 정하자 (10과 36에 약수에 포함되지 않는\n수). \n이때 공개키에 포함되는 정보는 N = 407(11*37) 과 e = 7이다.\n10(p-1) * 36(q-1) = 360\n7 * d mod 360 = 1 등식을 만족하는 d를 구해야 한다 (mod 는 나머지 연산)\n확장된 유클리드 호제법을 사용하면 빠른 시간 내에 구할 수 있다.\nd=103\n비밀 키에는 d 값과 N 값을 포함 시키게 된다.\n공개키=N(407), e(7)\n개인키=N(407), d(103)\n평문 240을 암호화 해 보자. 240의 7제곱을 407로 나눈 나머지는 235이다. 이제 235를 비밀키를 가지고 있는 사람에게 전달한다\n\n여기서 소인수분해 문제가 생긴다. 얼핏 보면 공개키에 N(407) e(7) 값이 주어졌으니 평문만 미지수 x 라고 가정하면 x^7 mod 407 = 235 라는 등식이 나오므로 공개키만 가지고도 복호화 할 수 있는거 아닌가라고 생각된다. 근데 문제는 저 x 값을 구하는 게 소인수분해 말고는 없기 때문에 사실상 공개키 만으로 복호화가 힘든 것이다.\n\n받은 사람은 N과 d의 값이 각각 407 과 103 임을 알고 있으므로, 전달받은 235의 103제곱을 407로 나누면 나머지가 평문이 된다.\n이게 가능한 이유는 오일러 정리에 나와있다.\n모듈러 연산\n5 mod 3\n5를 3으로 나눈 나머지는? 이라는 소리\n합동(≡)\n5 ≡ 8 mod 3\n5와 8 모두 3으로 나눴을 때 나머지가 2이므로 나머지가 서로 같다는 걸 의미함\n양자 컴퓨터와의 관계\n양자 컴퓨터가 등장하면 큐비트 연산을 통한 쇼어 알고리즘이 사용이 가능하여 이런 비대칭 키가 다항시간 내에 뚫릴수가 있다\n사용 예\n네트워크 암호화 통신\n\n여기선 HTTPS 에 TLS 원리랑은 조금 다르니 주의할 것\n\n\n서버-클라이언트간 양방향 통신 암호화\n\n클라이언트는 비대칭 키-쌍 을 생성하여 서버와 연결 이때 서버에 클라이언트 자신이 생성한 공개키를 서버에 보냄\n서버도 비대칭 키-쌍을 생성하여 연결된 클라이언트에 자신이 생성한 공개키를 클라이언트에 전송한다\n그럼 클라이언트는 서버에 공개키를, 서버는 클라이언트에 공개키를 가지게 된다.\n클라이언트가 데이터를 전송할 시 서버에 공개키를 사용하여 암호화를 진행하여 전송하고, 전송받은 서버는 자신의 비밀키를 사용하여 복호화 한다.\n서버가 데이터를 전송할 시 클라이언트에 공개키를 사용하여 암호화를 진행하여 전송하고, 전송받은 클라이언트는 자신의 비밀키를 사용하여 복호화 한다.\n즉 둘 사이 오간 암호키는 공개키 밖에 없으므로 중간자 공격에서 방어가 되는것\n이다.\n\n전자 서명 (인증서)\n비대칭 키에 특성 상 개인이 소유한 키-쌍 을 가지고 암호화한 정보는 자신만이 볼 수 있어서 누군가가 위 변조할 수 없는 부인 방지의 역할이 있다는 것을 이용하여 인터넷 상에서 서명을 하는 방법\n공인 인증서, 및 다른 민간 인증서가 다 이런 원리이다\n전자서명은 보통 개인키로 암호화 하고, 공개키로 복호화를 진행한다\n이때 키-쌍 은 서명 인증 주체로 부터 발급 받은며\n서명 내용은 자신의 개인정보 등과 발급 받은 공개키를 해 시 한 값이며\n서명을 인증하는 주체는 서명내용에 들어가는 공개키를 자신도 알고있는 상태에서 서명자가 했던과정을 똑같이 하여 두 내용이 같은 해시값을 가지는지를 판단하여 서명 위 변조를 판단한다.\n해시값\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nnamu.wiki/w/%ED%95%B4%EC%8B%9C\n\n\n\n\n특정 값을 식별하는 고정 길이에 값으로 변환하는 함수\n\nSHA-256 0d20951336a0e6a39da2682efff30ab31012e88ea606aea6fdae9b502012fe5\n\nHash 함수 자료구조를 활용한 값 체크 방법\n해시에 특징은 데이터의 길이와 상관 없이 동일한 길이에 데이터가 출력이 된다.\n또한 단 1비트만 달라져도 기존 데이터와는 상관없이 아주 랜덤한 해시값이 나온다.\n해시에 핵심은 이유는 동일한 데이터가 입력될 경우 같은 해시값이 나온다는 건데\n문제는 그 해시값으로 기존에 데이터를 유추하는것이 사상 불가능 하기 때문이다\n다만 해시가 충돌할 수도 있으므로 주의는 해야한다.\n또한 에초에 암호화를 목적으로 나온게 아니라 복호화가 불가능 하지만 같은값은 동일한 값을 출력한다는 특성을 이용하여\n사용자의 입력이 올바른지, 데이터 무결성, 체크섬(오류체크) 등에 쓰인다\n또한 해시에 이런 원리를 이용하여 빠른 Indexing을 구현 할 수 있는데\n예를 들어 자료가 10000개인 데이터가 있다고 할 경우 사용자가 특정 검색어를 검색하면 10000개를 다 순환하면서 검색어와 일치하는 결과를 찾아야 되는데\n이걸 10000 까지 에 수를 가지는 해시함수를 사용하고 10000개의 공간의 배열을 할당 (해시 테이블) 데이터 저장과 검색 시 모두 해시함수를 거치게 끔 하여 시간복잡도를 O(1) 수준으로 만들어 버릴 수 있다.\n\n보통의 Map 자료구조가 대부분 해시인것이다\n\n해시함수\n\nMD5\nSHA\nCRC\n\n사용 예\n\nDB 비번 암호화\n해시값으로 원래값을 유추하지 못한다는 특성이 있어서 DB에서 비번이 유출되더라도 원래 비번을 찾을 수 없어서 보안에 유리하다\n사용자가 비밀번호를 입력할 경우 이 값을 해시 하여 DB에 저장된 해시값과 동일하다면 검증이 완료됬다고 볼 수 있다.\n다만 비밀번호 찾기를 구현할 때 사실상 실제 비번을 알 방법이 없으므로 비밀번호를 그냥 재설정 하도록 유도해야한다.\n"},"스터디/추후-스터디-할-것들-모음":{"title":"추후 스터디 할 것들 모음","links":[],"tags":[],"content":"WebView\n\n대표적인 하이브리드 앱 개발 방식\n\n서버 oauth 인증/인가\n\n백엔드 JWT 보안 인증\n\n디자인 패턴 스터디\n\nMVC 뭐 등등\n\n메모리 버스\n함수형 프로그래밍\n호이스팅\nGCC 컴파일 단계\n\nclearwater92.tistory.com/39\n\n자바lint\n\nwww.reddit.com/r/java/comments/huht46/what_do_you_guys_use_for_linting/\n\nMSA (마이크로 서비스 아키텍쳐) &amp;  모노레포\nVercel &amp; AWS Amplify\n\nnext.js 같은 SSR 호스팅 서비스\n\nRadis\n\n대표적인 in-memory DB\n"},"스터디/컴퓨터-수-표현":{"title":"컴퓨터 수 표현","links":[],"tags":[],"content":"보수\n\n부호가 있는 숫자를 다룰때 컴퓨터가 음수로 표현 할때 쓰는 방법\n즉 부호가 없는 숫자를 다룰때는 노상관\n\n\n예를들어 -3 이란 숫자 컴퓨터에서 표현 하고 싶다라고 칠때, 예를들어\n997 의 1000 에 대한 보수는 3 이라고 표현 하는것이다\n그니까 1000 의 기준에서 997은 3이 모자란 것이다\n컴퓨터는 음수를 표현할 때 기준 점이 되는 수가 있으면 얼만큼 부족한지를 계산 함으로 음수를 표현 할 수 있게 되는 것이다\n그리고 맨 앞비트를 부호 를 나타내는 비트로 사용하여 0이면 양수, 1이면 음수 이렇게 구분하게 되어\n\n0 이면 양수니까 보수를 사용하지 않고 해석하고\n1 이면 음수니까 보수를 사용하여 해석할지를 결정한다\n\n\n바이트로(8비트) 예를 들자면 1바이트에서 부호가 없는 숫자를 다루면\n2^8-1 = 0~255 까지 인데\n\n앞에 1비트를 부호를 나타내는걸로 사용하여 -128~127 까지 표현 하도록 설계 하였다\n\n\n\n1의 보수\n\n보수의 개념을 이용하여 음수를 표현하는 방법\n\n\nex) 0100\n\n이때 기준값은: 2^비트수-1 = 15\n현재 표현 하고 싶은 음수는 -4 인 상황\n4 를 2진법으로 변환해서 0100이 된것\n0100의 모든 비트를 반전 = 1011 10진법으로 11\n즉 15 기준값이 되기 위해서는 4가 더 필요하다 이렇게 해석이 된다\n11 의 15 에대한 보수는 4 이다. 라고 표현하여 음수를 구하는 것\n\n\n결과적으로 주어진 모든 비트를 반전시키면 1의 보수임\n이때 맨 앞 비트는 부호비트 이기 때문에 가능 범위는 -7 까지\n\n2의 보수\n\n실제 컴퓨터에서 음수를 표현하기 위한 방법\n\n\n1의 보수의 문제가\n\n0 을 보수를 사용하지 않는 양수에서 표현하면 0000 인데\n보수를 사용하는 음수에서 0 을 표현하면 1111 이다\n즉 0의 표현이 양수0 과 음수0 이 존재하게 되어 값이 낭비가 된다\n\n\n2의 보수는 1의 보수값의 1을 더하여 문제를 해결한다\n\n0000 의 1의 보수 1111 에 +1 을 하게되면\n모든 비트가 자리 올림이 일어나서 0000 이 된다\n이렇게 되면 음수와, 양수 모두 0000 이 되게 된다\n\n\n이렇게 되면서 4비트 에서 -8 을 표현 할 수 있게 되었다\n\n8 =1000\n1의 보수에서는 0111부호비트가 양수가 되어버려 표현이 불가\n2의 보수에서는 1000 으로 부호비트가 음수로 표현되어 가능하다\n\n\n4비트에서 양수8은 표현이 안되서 범위는 -8 ~ 7 까지이다\n\n사실 1000을 양수8을 포함시키면 저장 가능한 수의 개수는 똑같은데\n걍 0 이 두개 있는게 맘에 안들었나 보다\n\n\n\n구하는 법\n\n모든 비트를 반전 (1의 보수를 구하는것임)\n\n0110 0001 → 1001 1110\n\n\n+1\n\n1001 1110 → 1001 1111\n\n\n\n오버플로우 / 언더플로우\n\n오버 플로우\n\n예를들어 signed 4비트에서 10을 표현하려고 한다\n\n10 = 1010\n이때 부호비트가 1이 되므로 컴퓨터는 음수로 판단하여 2의 보수로 해석한 값 -6이 나오게 된다\n즉 signed 4 비트에서 표현가능한 양수 범위는 7 까지 인데 이를 넘어서 부호 비트가 1로 바뀌게 되어 컴퓨터는 음수로 판단하는 것\n이것이 오버플로우 이다\n\n\n\n\n언더 플로우:\n\n예를들어 signed 4비트에서 -10을 표현하려고 한다\n\n10 = 1010\n이걸 그대로 음수로 바꾸기 위해 2의 보수를 취한다면 0110 즉 6 이 나오게 된다\n한 비트만 추가되면 10 을 01010 으로 표현하여 문제 없겠지만\nsigned 4 비트에서 표현가능한 음수 범위는 -8 인데 이를 넘어서서 부호 비트가 0으로 바뀌어서 컴퓨터가 양수로 취급 하는것\n이것이 언더플로우 이다\n\n\n\n\n\n부호화 절대치\n\n부호를 제외한 수를 양수로 읽고 - 를 붙이는 방법\n\n\n예를들어 3 을 0011 로 표기하고 -3 을 1011 로 표기하는\n즉 부호비트만 1로 바꾸고 다른 비트는 양수와 동일하게 가져간다\n\n문제점\n\n사실 이렇게 그냥 부호비트만 다르게 하여 음수 양수를 구분하면 되는데 왜 보수를 사용하는지 의문이 들 수 있다\n부호화 절대치의 제일 큰 문제는 연산에 있다. 3에 3을 빼는 연산을 한다면\n컴퓨터는 뺄샘 연산을 음수로 변환하여 덧샘 연산을 한다\n\n3+(-3) 부호화 절대치로 표현하면 0011 + 1011 이 된다\n이를 계산한 결과는 1110 즉 -6 이여서 전혀 다른 결과가 나온다\n\n\n이를 2의 보수를 사용하여 계산하면\n\n0011 + 1101 = 0000\n즉 원하는 데로 0 이 나오게 된다\n\n\n\n정수\n2의 보수법으로 2진수를 음수로 바꾸기\n\n주어진 2진수:  1000 0001\n\n\n모든 비트 반전\n\n0111 1110\n\n\n반전된 2진수의 +1 하기\n\n0111 1111\n\n\n만약 반전된 2진수의 부호 비트가 0인경우 1로 바꾸기\n\n1111 1111\n\n\n10진수로 바꾼 결과 값 에다 부호만 - 붙이기\n\n-127\n\n\n\n결과\n\n129 → -127\n\n129는 8비트에서 표현가능한 음수 범위를 초과 하여 -127로 표현된다\n그니까 사실 언더플로우 이다\n\n\n참고로 128 에 경우 2진수로 바꾸면\n\n1000 0000 이 되는데 이걸 2의 보수를 취해도 자리올림 때문에 1000 000 임\n그래서 음수로 0 으로 해석되는데 2의 보수표현법으로 -128을 표현한다\n\n\n\n부호가 있는 2진법을 10진법 으로 해석하기\n\n주어진 2진수를 부호있는 2진법 (2의 보수법) 으로 해석하기\n부호있을때 2진수를 10진수로 바꾸는\n\n부호비트가 음수(맨앞 비트가1)인 경우\n\n이때는 2의 보수법을 사용하여 해석해야함\n주어진 2진수: 1001 1110\n\n\n모든 비트 반전\n\n0110 0001\n\n\n반전된 2진수의 +1 하기\n\n0110 0010\n\n\n\n\n해당 수를 그대로 10진수로 변경하고 - 붙임\n\n-98\n\n\n\n결과\n\nunsigned 일때: 158\nsigned 일때: -98\n\n부호 비트가 양수(맨앞 비트가0)인 경우\n\n이때는 원래 2진수 바꾸던 데로 해석하면 됨\n주어진 2진수: 0111 1111\n\n\n=127\n\n결과\n\nunsigned 일때: 127\nsigned 일때: 127\n\n결론\n\n부호비트가 0인 경우에는 그냥 2진수로 변환 하면됨\n하지만 1인 경우 2의 보수 사용\n바이트로 설명하자면 0~127 까지는 unsigned 와 같지만\n128 부터는 부호비트가 1 이되므로 2의 보수로 해석\n\n10진수 음수를 2진수로변환\n\n주어진 수: -98\n\n\n절대값을 2진수로 변환 (1바이트 표기법 씀)\n\n0110 0010\n\n\n모든 비트 반전\n\n1001 1101\n\n\n반전된 2진수의 +1 하기\n\n1001 1110\n\n\n부호비트 1로\n\n1001 1110\n\n\n\n결과\n\n-98 → 1001 1110\n\n부동 소수\n\n\n소수를 2진수로 (저장할 소수: 13.625)\n\n13 을 2진수로 (1101)\n정수 부분을 때고 0.625 만 남긴다.\n정수 부분이 1로 떨어지거, 똑같은 숫자가 나올때 까지 2를 곱한다. 이때 정수부분은 버리고 계산한다 계산 하면서 나온 정수값에 순서가 소수에 2진수 이다 101\n13을 2진수로 계산한 1101 과 소수 부분을 계산 하면서 나온 값 101 을 합쳐\n1101.101 이라는 결과를 얻게 된다\n\n\n\n부동 소수 저장\n\n\n\n\n1101.101 을 정수 부분 하나만 남긴 체 소수로 이동 1.101101 소수점을 3번 옮겼으니 지수는 2^3\n가수 부분 왼쪽에서 부터 소수부분 101101 입력 (나머지는 0으로)\n지수 부분에 지수 + 127(8비트) 한 수를 2진수로 입력 10000010\n(즉, 아까 구한 지수가 2^3 이니까 3+127 = 130 = 10000010)\n\n진법정리\n2진법 (Binry)\n\nBin\n접두어: 0b\n\n\n컴퓨터의 텍스트를 모든것은 2진법 이다. 모든 길은 2진법을 통한다\n\n8진법 (Octal)\n\nOct\n접두어: 0\n\n\n예전에는 많이 쓰였지만 지금은 잘 쓰이지 않음\nUnix 쪽에서는 파일권한 같은걸 8진법으로 사용하는듯\n\n변환법\n\n10진법 → 8 진법\n\n2진법이 2로 계속 나누듯이 8진법은 8로 계속 나누면됨\n88 → 227\n\n\n8진법 → 10진법\n\nex) 1 x (8^2) + 2 x (8^1) + 7 = 87\n오른쪽에서 부터 (8^0) x a + (8^1) x a +(8^2) x a...\n\n\n8진법 ←&gt; 2진법\n\n16진법이 4비트씩 이라면 8진법은 3비트씩 끊으면 됨\n72 ←&gt; 111 010\n\n\n\n16 진법 (hexadecimal)\n\nHex\n접두어: 0x\n\n\nBinry를 1바이트 단위로 나타내기가 편하다\n색상 값 같은 걸 표기할 때 많이 쓴다\n보통의 바이너리 뷰어는 16진법으로 변환하여 보여준다\n\n변환법\n\n바이트 → 16진법\n\n1110 1100 → 0xEC\n\n\n16진법 → 바이트\n\nE가 14니까 2진수로 변환 하면 → 1110\nC가 12니까 2진수로 변환 하면 →  1100\n이렇게 4비트씩 끊어서 처리할 수 있으니까 2진법으로 바꾸기 편하다\n\n\n16진법 → 10진법\n\nex) 2FA\n2x(16^2) + 15x(16^1) + 10 = 762\n오른쪽에서 부터 (16^0) x a + (16^1) x a +(16^2) x a...\n\n\n16진법 → 10진법\n\n2진법이 2로 계속 나누듯이 16진법은 16로 계속 나누면 되긴 하는데\n차라리 2진법 으로 변환하서 계산하는게 편할듯\n\n\n\n표기법\n\n10: A\n11: B\n12: C\n13: D\n14: E\n15: F\n"},"스터디/파일-링크":{"title":"파일 링크","links":["스터디/IPC-통신"],"tags":[],"content":"\n\n                  \n                  참고자료 \n                  \n                \n\n\n리눅스 파일 링크 (하드 링크 / 심볼릭 링크 / inode)\n\n\n\n파일 시스템의 아이노드 (inode)\n\n우리가 보는 모든 파일은 그것이 실제로 거기에 위치한것이 아니다\n\n\n우리가 보는 파일은 운영체제가 실제로 관리하고 있는 파일 위치에 포인터, 즉 주소값을 보는것이다\n이 주소값에는 실제 파일에 (고유번호, 소유권, 엑세스 모드, 타임스템프) 를 가진다\n파일을 접근하면 실제 파일이 위치한 곳에서 바이너리 데이터를 넘겨주는 식이다\n각 파일시스템(NTFS, FAT32 등) 마다 구현은 다르지만 기본 개념은 똑같다\nFile Descriptor 와는 다른개념이니 오해하지 말자\n\nFile Descriptor:\n\n열려 있는 파일이나 소켓을 관리하기 위해 운영체제가 프로세스에 부여하는 인덱스. 이는 메모리에 할당된 열린 파일들을 식별하고 관리하는 데 사용\n\n\ninode:\n\n저장된 파일을 파일 시스템에서 관리하기 위해 부여되는 고유 식별자.\n\n\n\n\n\n바로 가기\n\n바로가기는 실제로 해당 파일 위치로 이동하는 .lnk 파일을 생성해 준다\n\n\n그래서 바로가기를 실행하면 실행된 파일에 위치는 원본 파일이 존재하는 위치이다\n바로가기 자체의 경우 디랙토리가 아니라 해당 디렉토리로 리다이렉트 시켜주는 파일에 불과하다\n\n하드 링크\n\n원본 파일과 동일한 주소 값을 가지는 파일을 생성\n\n\n원본과 동일한 파일을 생성 해주는 건데, 아예 그대로 복사하는게 아니라 포인터 (주소) 값만 복제하여 실제 디스크 할당 없이 동일한 파일을 만들어 낸다\n\n만약 원본파일을 수정하면 복제된 하드링크 파일에도 수정사항이 반영\n하드링크 파일을 수정해도 원본파일에도 수정사항이 반영\n\n\n원본 파일이 삭제되더라도 상관이 없다\n\n실제 파일을 가르키고 있는 포인터가 n개가 존재하는 거기 때문에 모든 파일들이 제거되야 비로소 디스크에서 재거가 된다\n\n\n윈도우에서 만들경우 실제 육안으로는 확인이 불가하다  (그냥 파일 처럼 생겼다)\n하드 링크의 경우 실행되는 위치는 해당 하드링크 자체 파일에 위치 값\n다만 디랙토리는 링크가 불가하다\n\n윈도우\nmklink /h &lt;링크파일&gt; &lt;대상파일&gt;\n리눅스\nln &lt;대상파일&gt; &lt;링크파일&gt;\n심볼릭 링크\n\n원본 파일의 주소값을 참조하는 포인터를 가지는 파일/폴더 를 생성\n\n\n하드링크와 거의 동일하게 작동하지만 몇가지를 보완\n예를들어 원본 파일 index 가 1 이면 새롭게 생성되는 심볼릭 링크 파일의 인덱스는 2 인데 2 인덱스는 1을 또 참조하게 만든 것\n\n2 → 1 → 파일\n그렇기 때문에 바로가기와 동일하게 실행된 파일에 위치는 원본 파일이 존재하는 위치이다\n\n\n파일 시스템 (NTFS 같은거)이 달라도 생성이 가능\n윈도우에서 만들경우 바로가기 아이콘과 동일한 모양으로 생성된다\n다만 원본 파일이 삭제되면 해당 파일에 접근 불가\n\n윈도우\n\n파일\nmklink &lt;링크파일&gt; &lt;대상파일&gt;\n\n\n폴더\nmklink /d &lt;링크파일&gt; &lt;대상파일&gt;\n\n\n\n리눅스\nln -s [대상파일] [심볼릭링크파일]\n"},"스터디/프로그래밍-언어-공부-순서":{"title":"프로그래밍 언어 공부 순서","links":[],"tags":[],"content":"기초\n\n변수, 상수, static\n자료형\n표준 입/출력\n연산자\n조건문\n\nif, switch-case\n\n\n반복문\n\nfor, foreach, while\n\n\n배열(리스트)\n\n기본\n\n언어 고유 문법 (리터럴 문자열 등)\n예외 처리\n파일 I/O\n내부 라이브러리\n람다 표현식\n비동기 처리\n멀티쓰레딩\nHTTP Request\n소켓\nenum\nGUI\n\n객체 지향\n\n클래스 &amp; 객체생성\n접근제어(pubilc, privete)\n생성자\n상속\n인터페이스 &amp; 추상클래스\n제네릭\n오버로딩, 오버라이딩\n캐스팅\n\n심화\n\n패키지 관리자(npm, gredle 등)\n외부 라이브러리 &amp; 프레임워크\n\nReact, Spring 등\n\n\n프로젝트 폴더 구조\n"},"시험-정리/2-2/교양-파이썬-기말/교양-파이썬-기말":{"title":"교양 파이썬 기말","links":[],"tags":[],"content":"객체\nclass Bicylce():\n\tisinstance_count = 0 # 클래스 변수 (정적 변수 느낌)\n    def __init__(self, size, color):\n        self.size = size # 인스턴스 변수\n        self.color = color\n \n\t#인스턴스 메소드\n    def move(self, speed):\n        print(&quot;자전거: 시속 {}킬로 미터로 전진&quot;.format(speed))\n    def turn(self, dic):\n        print(&quot;자전거: {}회전&quot;.format(dic))\n    def stop(self):\n        print(&quot;자전거({}, {}): 정지&quot;.format(self.wheel_size, self.color))\nby = Bicylce()\n\nby.speed = 0 == self.speed = 0\n클래스 멤버변수 접근은 self로 이루어짐\n__init__은 생성자 인자로 무조건 self 넣어줌\n객체를 생성하거나 인스턴스 함수를 호출할때 자동으로 self를 넣어서 넘겨줌\nisinstance_count 접근은 생성한 객체 변수가 아닌 Car.isinstance_count 로 접근가능함, 또한 해당 변수는 클래스와 독립적 또한 클래스 내부 함수도 동일하게 접근\n\n정적 메소드\n@staticmethod #데코레이터\ndef checK_type(model_check\n\tif (model_check &gt;= 20):\n\t\tprint(&quot;전기차&quot;)\n\telif (model_check &gt;=10):\n\t    print(&quot;가솔린&quot;)\n\telse:\n\t\t print(&quot;디젤&quot;)\n\nself를 받지 못하고 인스턴스 변수나, 인스턴스 함수에 접근불가\n접근시 클래스명.checK_type\n\n클래스메소드\n@classmethod #데코레이터\ndef count_instance(cls):\n\tprint(&quot;car 만든것&quot;, cls.instance_count)\n\nCar.instance_count 이렇게 한걸 인자를 받아 cls.instance_count 로 할수 있게\n즉 클래스변수 초기화 용도\n\n상속\nclass 자식(부모객체)\n\n자바와는 다르게 부모 생성자를 꼭 호출할 필요는 없음\n부모 생성자, 함수호출시\nsuper().__init__(wheel_size, color, state) #생성자\nsuper().move(speed) #인스턴스 메소드\n# self 인자 없이\n또는\nBicycel.__init__(self, wheel_size, color, state)\nBicycel.move(self, speed)\n# 이때는 self 인자를 넘겨야함 \n\n\n모듈\n모둘 불러오기\n\nimport 모듈명\n모든 함수, 변수, 클래스 불러오기 사용시 모듈명.변수 이런식으로\nfrom 모듈명 import 함수, 변수, 클래스\n해당 모듈에서 변수나 함수만 불러오기\nfrom 모듈명 import *\n모든 함수, 변수, 클래스 불러오는데 모듈명.변수 이렇게 쓰지 않고 직접 접근 가능\nfrom 모듈명 import 함수 as 별명1, 변수 as 별명2\n이런식으로 여러개 별명주기\n\n메인문제\n\n모듈을 불러오면 해당 모듈에 모든 내용을 실행함 근데 모듈에 자체를 실행하였을 때만 실행해야 되는 부분도 모두 실행한다는 문제가 있음 그럴떄는\nif __name__ == &quot;__main__&quot;:\n해당 블록 안에서 실행하면됨\n이게 뭔소리냐면 모듈 자체를 실행하면 __name__==__main__ 인데 다른 파일이 import 해서 실행되면 __name__==[내 모듈명] 임\n\n내장모듈\n\nrandom: 난수 값 생성모듈 randint\ndatetime: 날짜 시간처리 모듈\n\n주요 클래스\ndate(year, month, data): 날짜를 표현\ntime(hour, minute, second): 시간을 표현\ndatatime(year, month, day, hour, minute, second): 날짜 및 시간표현\n\n\ncalendar: 달력생성 모듈\ncalendar.calendar(2023): 2023년 달력표시\n"},"시험-정리/2-2/네트워크-프로그래밍-중간/네트워크-프로그래밍-중간고사-정리":{"title":"네트워크 프로그래밍 중간고사 정리","links":[],"tags":[],"content":"인터넷 구성요소\n\n호스트, 라우터, 통신 프로토콜\n\n패킷 전송 과정\n\n프로그램 → TCP → IP → 이더넷\n\nTCP 와 UDP\nTCP\n\n연결 설정 후 통신 가능\n일대일 통신\n데이터 경계 구분x (바이트 스트림 서비스)\n\nUDP\n\n연결 설정 없이 통신가능\n일대일, 일대다 상관없음\n데이터 경계 구분\n\n소켓생성\nsocket(IP 타입, TCP|UDP, 0);\n\nip 타입\nAF_INET: IPv4\nAF_INET6: IPv6\nTCP: SOCK_STREAM, UDP: SOCK_DGRAM\n\nIPv6 사용시: AF_INET6, sockaddr_in6\nTCP 소켓 처리 과정\n서버\n\nWSAStartup(): 소켓 초기화\nsocket(): 서버 소켓 생성\nbind(): 소켓 바인딩 (소켓의 지역 IP 주소와 지역 포트 번호를 결정)\nlisten(): 클라이언트 연결대기, 클라이언트 얼마나 받을건지 결정\naccept(): 클라이언트 접속 수락, 클라이언트 통신을 위한 새로운 소켓 생성\nsend(): 메시지 보내기 &amp;&amp; recv(): 데이터 받기\n\n클라이언트\n\nWSAStartup(): 소켓 초기화\nsocket(): 클라이언트 소켓 생성\nconnect(): 서버와 연결 설정, 연결을 위한 패킷 교환\nsend(): 메시지 보내기 &amp;&amp; recv(): 데이터 받기\n\nUDP 소켓 처리과정\n서버\n\nWSAStartup(): 소켓 초기화\nsocket(): 서버 소켓 생성\nbind(): 소켓 바인딩 (소켓의 지역 IP 주소와 지역 포트 번호를 결정)\nsendto(): 메시지 보내기 &amp;&amp; recvfrom(): 데이터 받기\n\n클라이언트\n\nWSAStartup(): 소켓 초기화\nsocket(): 클라이언트 소켓 생성\nsendto(): 메시지 보내기 &amp;&amp; recvfrom(): 데이터 받기\n\nTCP 서버 - 클라이언트 동작 원리\n\n서버는 소켓을 생성, 클라이언트 대기\n클라이언트가 서버에 접속, 연결설정을 위한 패킷 교환 (3-way handshaking 방식)\n서버는 클라이언트와 데이터 송, 수신을 위한 새로운 소켓 생성\n기존에 생성한 소켓은 다른 클라이언트 접속대기\n\n\nUDP 서버 - 클라이언트 동작 원리\n\n서버는 소캣 생성, 클라이언트가 데이터 보내길 기다림\n클라이언트는 연결설정 없이 서버와 곧바로 데이터 주고 받음, 이때 서버는 소켓 한 게만 사용\n아무 클라이언트가 접속해서 서버와 통신할 수 있다\n\nTCP 메시지 경계 구분\n\n항상 고정길이로 보냄\n\n주고받을 데이터의 길이 변동폭이 크지 않을 때 적합\n\n\n가변 데이터를 보내고 특별한 표식(EOR)을 붙임\n\n생성 데이터의 길이를 미리 알 수 없을 때 적합\n\n\n고정 길이 보내고 가변길이 보냄, 수신자는 고정길이 읽고, 뒤에 가변길이 알아냄\n\n생성 데이터의 길이를 미리 알고 있는 상황에서 구현이 쉽고 처리 효율성이 높음\n\n\n모든 가변 데이터 보내고 연결종료\n\n한쪽에서 일방적으로 가변 길이 데이터를 보낼 때 적합\n\n\n\nUDP 데이터 전송\n\n유니캐스팅\n\n1:1 데이터 전송\n\n\n브로드캐스팅(IPv4)\n\n특정 네트워크에 속한 모든 객체\n\n\n멀티캐스팅\n\n동일한 그룹에 속한 객체\n\n\n애니캐스팅(IPv6)\n\n동일한 그룹에 속한 객체 중 가장 가까운 객체\n그리고 그 객체가 나머지 객체들에게 전송\n\n\n\n브로드케스팅\n\n송신자가 보낸 데이터 하나를 다수의 수신자가 받는 방식\n소켓 생성 이후 setsockopt() 함수 인자로 SO_BRODCAST 를 넘겨 브로드케스팅 허용\n\n브로드케스트 주소 종류\n\n네트워크 브로드 케스트\n[ip].[ip].[255].[255]\n서브넷 브로드케스트\n[ip].[ip].[ip].255\n지역 브로드케스트\n255.255.255.255\n\n바이트 정렬\n메모리에 데이터 저장때 바이트 배치 순서\nex) 0x12345678\n\n빅엔디언: 0x12, 0x34, 0x56, 0x78\n리틀 엔디언: 0x78, 0x56, 0x34, 0x12\n\n\n바이트 정렬을 고려하는 경우\n\nIP 주소에 문제가 발생하는 상황\n포트 번호에 문제가 발생하는 상황\n응용 프로그램 데이터에서 문제가 발생하는 상황\n\n\n\n멀티 쓰레딩\n교착 상태 해결\n\n데이터  송수신 부분 잘 설계\n\n빠른 구현, 모든 경우 해결책x\n\n\n소켓에 타입아웃 적용\n\n구현 쉬움, 성능 떨어짐\n\n\n넌블로킹 소켓 사용하기\n\n조건 만족하지 않아도 소켓 함수 즉시 리턴으로 교착 해결, 구현복잡  &amp;&amp; 자원낭비\n\n\n입출력 모델 사용\n\n넌블로킹 단점 보안, 구현복잡\n\n\n\n생성\n쓰레드 함수\nDWORD WINAPI 함수명(LPVOID param){ }\n쓰레드 API\nCreateThread(NULL, 0, 함수명, 보낼데이터주소, 0, NULL): 쓰레드 생성\n종료\n\n함수 안 return\n쓰레드 함수 내 ExitThread() 호출\n다른 쓰레드가 TerminateThread() 호출\n\n종료 대기\n종료 대기를 위해 쓰레드 함수를 HANDLE에 메핑\nHANDLE hThread = CreateThread();\n\n한개: WaitForSingleObject(HANDLE 쓰레드, int 대기시간=INFINITE)\n여러개: WaitForMultipleObjects(int 대기할개수, HANDLE *쓰레드주소, TRUE, 대기시간=INFINITE)\n\n실행 제어\n\n일시 정지: SuspendThread(HANDLE 쓰레드) || Sleep(HANDLE 쓰레드)\n재실행: ResumeThread(HANDLE 쓰레드)\n종료:  CloseHandle(HANDLE 쓰레드)\n\n동기화\n\n[임계영역, 뮤텍스, 이벤트, 세마포어, 대기가능 타이머]\n\n임계영역\n\n두 개 이상의 스레드가 공유 자원에 접근할 때, 오직 한 스레드만 접근을 허용해야 하는 경우에 사용\n\n//쓰레드 입장\nCRITICAL_SECTION cs; //전역변수 선언\n \nDWORD WINAPI thread() {\n\tEnterCriticalSection(&amp;cs);\n\t// 공유 자원 접근\n\t// 해당 영역이 건드는 모든 변수는 LeaveCriticalSection 를 호출하기 전까지 다른 쓰레드가 건들면 대기시켜 버림\n\tLeaveCriticalSection(&amp;cs);\n}\n \n// 메인 입장\nmain() {\n\tEnterCriticalSection(&amp;cs);\n\t// 공유 자원 접근\n\t// 해당 영역이 건드는 모든 변수는 DeleteCriticalSection 를 호출하기 전까지 다른 쓰레드가 건들면 대기시켜 버림\n\tDeleteCriticalSection(&amp;cs);\n}\n이벤트\n\n사건이 발생하기 전 다른 쓰레드에게 알리는 기법\n생성\nCreateEvent(NULL, FALSE, FALSE, NULL): HANDLE\n이벤트 발생\nSetEvent(HANDLE 이벤트)\n이벤트 종료\nResetEvent(HANDLE 이벤트)\n\n용어정리\n\n소켓: 전송 계층과 응용 프로그램 사이의 인터페이스 역할을 하며 떨어져 있는 두 호스트를 연결함 (전송 프로토콜, ip, 포트번호로 정의)\n패킷: IP주소, 포트 번호, 오류코드 + 데이터\n\n함수 정리\n\nWSACleanup() 소켓 끝 마무리\nclosesocket(SOCKET): 소켓 종료\ngethostbyname(Char * 도메인): 호스트 이름 → ip주소\ngethostbyaddr(char *ip주소, int 길이, int ip타입): ip 주소 → 호스트 이름\ninet_ntop(int ip타입, Char *반환할 데이터, void *담을주소): 바이트 ip 주소 → 실제 ip 주소\ninet_pton(int ip타입, void *변환값주소, 버퍼, 길이): 실제ip 주소 → 바이트 ip 주소\n(자료형*)malloc(sizeof(자료형)): 동적 메모리 할당\n(p[0], p[1]…) 이렇게 할당하는 순간 해당 하는 만큼 메모리를 할당해서 무한대로 쓸 수 있음\n\n바이트 정렬함수\n\nhtons(): 2바이트 호스트 바이트 → 네트워크 바이트(빅엔디안 으로 변환)\nhtonl(): 4바이트 호스트 바이트 → 네트워크 바이트(빅엔디안 으로 변환)\n\n프로그램이 소켓에 넘겨주기 전\n\n\nntohs(): 2바이트 네트워크 바이트 → 호스트 바이트\nntohl(): 4바이트 네트워크 바이트 → 호스트 바이트\n\n소켓이 전송한 결과를 받아볼 때\n\n\n\n클라이언트는 hton* 로 전송하고 서버는 ntoh* 로 받으면 됨"},"시험-정리/2-2/자료구조-기말/자료구조-기말":{"title":"자료구조 기말","links":["힙-삽입.pdf","힙-삭제.pdf"],"tags":[],"content":"이중 연결리스트\n\n삽입시\nif (p-&gt;head == NULL) {\n\tp-&gt;head = newNode;\n} else {\n\tnewNode-&gt;Rlink = p-&gt;head-&gt;Rlink;\n\tp-&gt;head-&gt;Rlink = newNode;\n\tnewNode-&gt;Llink = p-&gt;head;\n\tif (newNode-&gt;Rlink) {\n\t\tnewNode-&gt;Rlink-&gt;Llink = newNode;\n\t}\t\t\n}\n\n\n스택\n\n후입후출(마지막에 온게 먼저)\n\n구성요소\n\nisEmpty(): 공백인지 아닌지 리턴\nisFull(): 배열에 끝인지 리턴\npush(): 삽입\npop() 삭제(맨 마지막에 온놈)\nprintStack(): 스택 순회\n\n배열스택 과정\n\ntop 을 -1 로 설정\npush() 하면 isFull() 확인하고 ++top 하고 아이템삽입\npop() 하면 isEmpty() 확인하고 top 위치꺼 리턴해줌\n\n연결리스트 스택 과정\n\npush() 하면 이전에 head를 새로운 노드에 링크하고 head를 넘김\npop() 하면 isEmpty() 확인하고 head꺼 리턴하고 head를 다음 링크로 넘기고 free()\n\n연결리스트 스택으로 괄호판단\n\n여는괄호면 push() 닫는괄호면 pop()\n\n큐\n\n선입선출(먼저온게 먼저나감)\n\n구성요소\n\nfront: 삭제된 지점을 가르키는 변수\nrear: 삽입된 지점을 가르키는 변수\nisEmpty(): front=rear 면\nisFull(): rear가 마지막 인덱스면\nenQueue(): 큐 삽입\ndeQueue(): 큐 삭제\n\n배열 큐 과정\n\nfront, rear 를 -1로 초기화\nenQueue() 하면 isFull() 검사하고 ++rear 하고 arr[rear] 에 삽입\ndeQueue() 하면 isEmpty() 검사하고 ++front 하고 삭제된거 리턴\n\n원형 큐 과정\n\nfront, rear 를 0으로 초기화 (0으로 초기화 하는 이유는 순환구조 특성상 하나를 무조건 비워나야함)\nisFull() 함수는 (rear+1)%size 한게 front랑 같으면\nenQueue(), deQueue() 할 시 무조건 size 나머지 연산자 쓸것\nq-&gt;rear = (q-&gt;rear + 1) % QUEUE_SIZE;\n\nprintQueue()에 경우 rear, front 값을 +1 해서 불러옴\nint rear = (q-&gt;rear + 1) % QUEUE_SIZE;\n// rear 가 가르키는게 삽입된 값의 인덱스이니 거기까진 값이 있으므로 +1함\nint front = (q-&gt;front + 1) % QUEUE_SIZE;\n// front 가 가르키는게 삭제된 값의 인덱스이니 그 다음 인덱스부터 실제 값 이므로 +1함\n전부 위에처럼 나머지 연산자 쓸것 (오버플로우 나기때문), front 부터 rear 까지 출력하면 됨\n\n연결리스트 큐 과정\n\nhead 를 front, rear 이렇게 생성\nisEmpty() 함수에 경우 front==null &amp;&amp; rear == null\nenQueue()시 isEmpty()면 front 와 rear를 새로운노드에 위치시킴\n아니면 rear→link 에 삽입하고 rear를 새로운노드로 위치시킴\ndeQueue()시 isEmpty() 판단하고 front를 다음노드로 가르키게 하고\n이전노드 삭제 후 해당노드 데이터 리턴 이때, 다음노드가 null 인경우 front 도 null로 만듬\n\n데크\n\nfront, rear 에서 삽입, 삭제가 동시에 되는 구조, 이중 연결리스트 사용\n\n구성요소\nisEmpty(): front==null &amp;&amp; rear == null 이면\ninsertFront(): 앞쪽에 삽입\ndeleteFront(): 앞쪽에 삭제\ninsertRear(): 뒤쪽에 삽입\ndeleteRear(): 뒤쪽에 삭제\n과정\n\nFront 요소는 무조건 head가 앞에 존재해야 하고\nRear 요소는 무조건 뒤에 존재해야함\n나머지는 연결큐와 동일\ndeleteFront() 시 Llink를 NULL 로 만들어야하고\ndeleteRear() 시 Rlink를 NULL 로 만들어야함\n\n트리\n\n용어\n\n노드: 트리원소\n간선: 노드를 연결하는 선\n형제노드: 같은 부모 노드\n서브트리: 부모노드와 연결된 간선을 끊었을때 생기는 노드\n즉 자식이 있으면 서브트리\n차수: 노드에 연결된 자식 노드 수\n\n이진트리\n\n트리에 모든 노드이 차수를 2이하로 제한\n최대 노드개수 2^(h+1)-1\n포화 이진트리: 모든 노드가 2개씩 가득 차 있는경우\n완전 이진트리: 왼쪽부터 자식을 채워 넣었고 포화상태가 아닌경우\n\n배열 이진트리\n\n위 사진 순서대로 저장\n노드 n에 부모: n/2\n노드 n에 왼쪽 자식: 2*n\n노드 n에 오른쪽 자식: 2*n+1\n\n연결리스트 이진트리\n\n왼쪽자식 주소, 오른쪽 자식 주소 각각 구현\n\n이진트리 순회\n\n전위 순회: 나→ 완쪽 → 오른쪽\nA →B →D→H→I→E→C→F→G\nvoid freeOrder(TreeNode* root) {\n\tif (root != NULL) {\n\t\tprintf(&quot;%c&quot;, root-&gt;data);\n\t\tfreeOrder(root-&gt;left);\n\t\tfreeOrder(root-&gt;right);\n\t}\n}\n\n중위순회: 오른쪽→나→왼쪽\nH→D→I→B→E→A→F→C→G\nvoid inOrder(TreeNode* root) {\n\tif (root != NULL) {\n\t\tinOrder(root-&gt;left);\n\t\tprintf(&quot;%c&quot;, root-&gt;data);\n\t\tinOrder(root-&gt;right);\n\t}\n}\n\n후위순회: 왼쪽→오른쪽→나\nH→I→D→E→B→F→G→C→A\nvoid postOrder(TreeNode* root) {\n\tif (root != NULL) {\n\t\tpostOrder(root-&gt;left);\n\t\tpostOrder(root-&gt;right);\n\t\tprintf(&quot;%c&quot;, root-&gt;data);\n\t}\n}\n\n\n이진 탐색 트리\n\n왼쪽은 루트보다 낮은 수\n오른쪽은 루트보다 큰 수\n서브트리도 이진 탐색 트리\n\n\n찾으려는 인덱스가 루트보다 작으면 왼쪽, 높으면 오른쪽\n\n\n삽입, 인덱싱은 모두 제귀함수 쓰면 됨\n삭제는 3가지로 나뉨\n\n자식이 없는경우\n그냥 삭제하면 됨\n단일자식만 있는경우\n자기자리를 자식한테 넘기고 삭제\n두 자식을 가진경우\n오른쪽 자식 중 가장 작은 자식을 후계자로 삼던가\n왼쪽 자식 중 가장 큰 자식을 후계자로 삼던가\n\n\n\n힙\n\n완전 이진트리 상황에서 가장 큰 값, 또는 가장 작은 값 찾기위해\n루트가 제일 큰 값이면 최대힙, 작은값이면 최소힙\n즉 모든 서브 트리에 자식요소는 부모보다 작거나 크면 됨\n삽입은 끝 쪽에 임시로 놔두고 부모랑 대소비교 해서 삽입될 위치 파악해서 삽입\n힙 삽입.pdf\n삭제는 무조건 루트노드 삭제, 맨 마지막 원소를 루트에 넣고 자손들이랑 대소비교 해서 교환\n힙 삭제.pdf\n\n정렬\n\n버블정렬\nfor (int j = 7; j &gt;= 0; j--) {\n\tfor (int k = 0; k &lt; j; k++) {\n\t\tif (arr[k] &gt; arr[k + 1]) {\n\t\t\ttemp = arr[k + 1];\n\t\t\tarr[k + 1] = arr[k];\n\t\t\tarr[k] = temp;\n\t\t}\n\t}\n}\n\n선택정렬\n0번부터 인덱스를 늘려가며 작은 수 찾아서 위치 바꾸는 방식\nfor (int i = 0; i &lt; 8; i++){\n\tmin = arr[i];\n\tminIndex = i;\n\tfor (int k = i; k &lt; 8; k++) {\n\t\tif (min &gt; arr[k]) {\n\t\t\tmin = arr[k];\n\t\t\tminIndex = k;\n\t\t}\n\t}\n\ttemp = arr[i];\n\tarr[minIndex] = temp;\n\tarr[i] = min;\n}\n\n삽입정렬\n정렬한 배열에서 정렬될 원소가 들어갈 위치를 찾아 삽입하는 구조\nfor (int i = 1; i &lt; 8; i++) {\n\ttemp = arr[i];\n\tfor (int j = i-1; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j--) {\n\t\tarr[j + 1] = arr[j];\n\t\ttempIndex = j;\n\t}\n\tarr[tempIndex] = temp;\n}\n\n\n검색\n순차검색, 선형검색\n\n배열이나 연결리스트에서 원하는 항목을 찾는 방법\n정렬된 자료에 경우 찾는 값이 배열에 있는 항목보다 크면 바로 종료할수있기에 좀더 빠름\n"},"시험-정리/2-2/자료구조-중간/자료구조-중간-정리":{"title":"자료구조 중간 정리","links":[],"tags":[],"content":"기본 지식\n\n컴퓨터는 메모리에 저장할 떄 입력한 값에 오른쪽에서 부터 왼쪽으로 저장한다\n\nint num = 0x41424344;\t//A B C D 16진수\n// 메모리: D C B A\n\n다차원 배열은 1차원으로 매핑 후 메모리에 저장한다\n\nint i = {{ 1, 2, 3 }, { 4, 5, 6 }}\n// 메모리: 1, 2, 3, 4, 5, 6 \n\n배열로 다항식 표현\n\n// 3x^10+x+4\n{{10, 3}, {1,1}, {0, 4}}\n// 지수, 정수 \n\n배열 문자열과 포인터 문자열에 차이\n\nchar a[10] = {&quot;hello&quot;} //hello\\0, ...빈값\nchar *a = {&quot;hello&quot;} // hello\\0\n\n\n2의보수\n\n2진수 반전\n끝에 +1\n\n\n\n메모리 영역 구분\n\n프로그램\n\n프로그램 코드\n\n\n데이터\n\n전역 변수와 정적 변수 저장\n\n\n힙\n\n사용자가 직접 정의 하는 메모리 공간\n\n\n스택\n\n함수에 호출과 관련된 지역변수와 매게변수 저장, 스택이므로 가장 늦게 저장된게 가장 먼저 호출\n\n\n\n\n\n부동 소수 저장\n\n\n소수를 2진수로 (저장할 소수: 13.625)\n\n13 을 2진수로 (1101)\n정수 부분을 때고 0.625 만 남긴다.\n정수 부분이 1로 떨어지거, 똑같은 숫자가 나올때 까지 2를 곱한다. 이때 정수부분은 버리고 계산한다 계산 하면서 나온 정수값에 순서가 소수에 2진수 이다 101\n13을 2진수로 계산한 1101 과 소수 부분을 계산 하면서 나온 값 101 을 합쳐\n1101.101 이라는 결과를 얻게 된다\n\n\n\n부동 소수 저장\n\n\n\n\n1101.101 을 정수 부분 하나만 남긴 체 소수로 이동 1.101101 소수점을 3번 옮겼으니 지수는 2^3\n가수 부분 왼쪽에서 부터 소수부분 101101 입력 (나머지는 0으로)\n지수 부분에 지수 + 127(8비트) 한 수를 2진수로 입력 10000010\n(즉, 아까 구한 지수가 2^3 이니까 3+127 = 130 = 10000010)\n\n로그\nlog₂8 = 3\n\n해석: 2를 몇번 곱해야 8이 나오는가? 이것이 로그에 정의이자 답\n여기서 2 는 밑 8은 진수 라고 한다.\n결국은 로그는 지수를 찾는 것\n\n\n빅-오 표기법 에서 O(logN) 은 O(log₂N) 을 줄여서 부르는 것\n즉 정수(밑) 이 2인 상황이다\n\n용어\n알고리즘: 문제해결 방법을 추상화하여 단계적 절차를 논리적으로 기술해 놓은 명세서\n(입력, 출력, 명확성, 유한성, 효과성)\n힙: 완전 이진 트리로 구성된 자료공간\n큐: 선입선출(나중에 온게 먼저 나감) 하는 자료공간\n스택: 후입 선출(먼저온게 먼저나감) 하는 자료공간\n공간복잡도: 프로그램 실행 시 들어가는 총 용량\n시간복잡도: 실행완료 시간\n빅-오 표기법: (O(n)): 어떤 수행 작업이 걸리는 시간의 최악의 경우\n빅-오메가 표기법: (Ω(n)): 어떤 수행 작업이 걸리는 시간이 최상인 경우\n빅-세타 표기법: (θ(n)): 어떤 수행 작업이 걸리는 시간에 평균\n\nO(1): 수행 시간이 일정할 때\nO(n log n): 수행해야하는 양이 두 배로 늘어날 때 올라가는\nO(n): 수행해야하는 만큼 걸리는 시간이 비례할 때\nO(2n): 수행해야하는 만큼 걸리는 시간이 2배 일 때\nO(n^2): 수행해야하는 만큼 걸리는 시간이 재곱 수 일 때\nO(2^n): 수행해야하는 만큼 걸리는 시간이 2 에 n승일 때\n\n자료 분류\n단순 구조\n\n정수, 실수, 문자열 등\n\n선형 구조\n\n순차리스트, 선형리스트, 연결리스트, 큐,  스텍\n(자료 사이의 1대1관계)\n\n비선형 구조\n\n트리, 그래프\n(자료 사이의 1대 N 관계)\n\n\n선형 이냐 아니냐 이 말은 1 차원이냐 2차원 이냐를 묻는거, 자료가 한쪽 방향으로만 정리된다면 1차원, 아니면 2차원\n\n순차자료 구조와 연결자료 구조\n\n순차 자료구조: 빈 자리 없이 서로 연속하는데로 (배열)\n\n배열을 이용한 구현\n\n\n연결 자료구조: 논리적 순서에 의해 저장 (연결리스트)\n\n포인터를 이용한 구현\n\n\n\n시간복잡도\n\n탐색\n\n배열: O(1)\n연결리스트: O(n)\n\n\n추가 및 삭제\n\n배열: 맨앞 = O(n), 맨뒤 = O(1),\n연결리스트: 맨앞 = O(1), 맨뒤 = O(n)\n\n\n\n재귀함수\n정의: 함수 안에서 자기 함수를 호출하는\n\n재귀 함수가 효과 적인 경우 (팩토리얼 계산)\n\nint fact(int a) {\n\tif (a &lt;= 1) {\n\t\treturn 1;\n\t} else {\n\t\treturn (a * fact(a - 1));\n\t}\n\t// 일반적인 반복문 보다 깔끔 하면서 더 빠르다\n}\n\n반복 구조 써야 하는 경우 (피보나치 수열)\n\nvoid main() {\n\tint temp1 = 1, temp2 = 0, now = 0;\n\tfor (int i = 0; i &lt; 10; i++) {\n\t\tprintf(&quot;%d &quot;, now);\n\t\tnow = temp1 + temp2;\n\t\ttemp2 = temp1;\n\t\ttemp1 = now;\n\t}\n}\n\n재귀함수로 처리할 시\n\nint fib(int n) {\n\tif (n == 0) return 0;\n\tif (n == 1) return 1;\n\treturn (fib(n - 1) + fib(n - 2));\n\t// 순환 호출에 비효율\n\t// 이렇게 되면 fib 함수 실행이 무한대로 늘어남 (2^n)\n}"},"시험-정리/3-1/기말---딥러닝-프로그래밍/딥러닝-프로그래밍-기말":{"title":"딥러닝 프로그래밍 기말","links":[],"tags":[],"content":"RNN (순환 신경망)\n\n입력과 출력을 시퀀스 단위로 처리하는 모델  피드포워드 신경망과 유사하지만 순환하는 연결도 있다는게 차이점\n\n\n\n시퀀스: 문장 같은 단어가 나열된 것 (쉽게말해서 문장을 처리하기위한 모델이다)\n셀: RNN 에서 은닉층에서 함수를 통해 결과를 내보내는 역할\n\nRNN은 입력을 받아 출력으로 나온 값을 다시 입력으로 넣는 순환구조 를 가지고 있다\n\n\n메모리 셀: 이전 셀의 값을 기억하는 역할을 하는 셀\n현재 출력 데이터는 다음 노드에 입력이되는 약간 연결리스트 구조를 가진다.\n\n그래서 시간의 축으로 나타낼 수도 있는데 이를\nunrolling the network through time라 부름\n\n\n\n모델 종류\n\n시퀀스-투-시퀀스: 번역기\n\n각 단어 하나하나를 셀로 분활하며 인코더와, 디코더를 거쳐 번역되는데 이때 이전 단어결과를 참조하여 번역되게 되는\n인코더-디코더:\n\n\n입력을 인코더로 처리하여 중간 백터(context)를 만들고 디코더가 백터를 바탕으로 출력을 만드는 구조\n\n\n인코더는 입력만, 디코더는 출력만 처리\n\n\n\n\n\n\n시퀀스-투-벡터: 감정처리, 문서 분류\n\n입력 시퀀스를 고정된 길이의 벡터로 인코딩하는 모델\n쉽게 말해 입력데이터를 가지고 하나의 결론을 도출하는\n인코더만 필요\n각 셀에 출력값은 무시하며 그냥 그 값을 다음 셀에게 전달 하게 된다\n이러한 특성땜에 감정처리, 문서 분류에 사용한다 (굳이 단어 하나하나 당 결과를 도출하는게 아니니)\n\n\n벡터-투-시퀀스: 음성 합성, 영상 생성 같은\n\n하나의 입력만 가지고 순환처리하는 구조\n디코더만 필요\n\n\n상태가 있는(Stateful) RNN:\n\n긴 시퀀스에 데이터를 처리 할 때 각 배치 상태를 유지시켜 긴 패턴을 더 잘 학습 시킬 수 있다\n\n\n\nBPTT\n\n타임 스탭으로 네트워크를 펼치고 보통의 역전파를 사용\n\n\n걍 연결되있는 쉘 따라서 역전파 하는\n\nLSTM 셀\n\nRNN 에 단점 중 하나가 길이가 길어 셀이 먼 곳에 위치한 경우 성능 저하 발생\n\n\nRNN 훈련할 때 단기 데이터의 의존하지 않고 장기 데이터의 의존하게 끔\n은닉층에 call-state를 추가한 구조\n\n구조\n\n\nRNN에 히든 stage를 추가 한 구조\n오래된 기억은 삭제 쪽을 지나 일부를 잃고 덧셈 연산으로 새로운 기억 추가\n만들어진 건 바로 출력하게 됨\n연산 후 상태가 복사 되어 tanh 함수로 전달\n결과는 출력 게이트에 의해 걸러지고 단기상태를 만듬\n게이트:\n\ninput gate: 이번 입력을 얼마나 반영할지\noutput gate: 이번 정보를 얼마나 내보낼지\nforget gate: 과거정보를 얼마나 잊을지\n\n\n\n코드\ntf.keras.models.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\n    tf.keras.layers.Dense(14)\n])\nGRU 셀\n\nLSTM의 간소화 버전\n\n\nGRU는 게이트가 2개 (LSTM 은 3개)\n출력게이트가 없는것이 특징이다\n게이트:\n\nupdate gate: 이번 입력을 얼마나 반영할지\nreset gate: 과거정보를 얼마나 잊을지\n\n\n\nWaveNet\n\n층마다 팽창비율을 두배로 늘리는 1D 합성곱 층을 쌓음\n\nfor rate in (1, 2, 4, 8) * 2:\n    wavenet_model.add(tf.keras.layers.Conv1D(\n        filters=32, kernel_size=2, padding=&quot;causal&quot;, activation=&quot;relu&quot;,\n        dilation_rate=rate))\nwavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))\nChar-RNN\n\n문자 단위 RNN\n\n\n\n글자 예측\ny_proba = shakespeare_model.predict([&quot;To be or not to b&quot;])[0, -1]\ny_pred = tf.argmax(y_proba)  # 가장 가능성이 높은 문자 ID 선택\ntext_vec_layer.get_vocabulary()[y_pred + 2]\n \n# 결과\ne\n\n\n다음 단어 예측\nprint(extend_text(&quot;To be or not to be&quot;, temperature=0.01))\n\n\ntemperature 값이 낮으면 원본에 가깝게 나오고 높으면 창작률이 높아진다\n\n\n마스킹: 모델이 패딩 토큰을 무시하게 만들기 (의미없는 데이터를 날려버리는)\n\n\nGreedy decoding\n\nChar-RNN에 디코딩 방식\n\n\n초기 텍스트를 주입하고 모델이 가장 가능성 있는 다음글자 예측\n예측한 글자를 텍스트 끝에 추가하고 늘어난 텍스트를 모델을 전달 다음글자 예측\n\n모델\n\nELMo: 심층 양방향 언어 모델의 내부상태에서 학습된 문맥이 반영된 단어 임베딩\nNLP: 대규모 텍스트, 말뭉치에서 LSTM 언어 학습한 다음 미세 튜닝\n\nNMT\n\n인간에 신경을 모방한 머신러닝 기법\n\n\nSeq2Seq 기반\n원리\n\n전체 문맥 파악 후 통째로 번역\n\n\n아주 짧은 문장에서는 작동이 잘되지만 아직 두 언어에 능숙하지 못하며 특히 긴 문장에서는 어려움\n\n구조\n\n\n인코더:\n\n예를들어 i love you 를 받으면 의미를 요약한 백터 생성\n\n\n디코더:\n\n인코더에서 생성된 컨텍스트 벡터를 입력으로 받아 타겟 시퀀스를 생성\n\n\n어텐션 메커니즘:\n\n인코더의 문제가 고정크기에 모든 정보를 압축할려니 정보 손실이 발생함\n디코더가 번역을 생성할때 인코더의 모든 은닉 상태를 참조하도록 하여, 각 타임스텝에서 중요한 부분에 더 집중할 수 있게\n\n\n\n정규화\n\n여러 특성 데이터 값의 범위를 사용자가 원하는 범위로 제한하는\n데이터의 편차가 크면 큰 특성 위주로 데이터를 이상하게 해결하는\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n특성배치 정규화 (Batch Normalization)층 정규화 (Layer Normalization)정규화 대상미니 배치의 각 샘플층의 뉴런들배치 크기 의존성의존적비의존적순차 데이터 모델덜 효과적효과적특화 모델CNN 모델에 효과적순차데이터나 소규모 배치에 적합 (RNN 같은)특이사항RNN에 쥐약이다. 각 시점마다 다른데이터가 연속적으로 나오기 때문각 층별로 정규화를 진행하는거라 입력 데이터와 출력이 많이 다른것들에 유리하다\n양방향 RNN (BI-RNN)\n\n두 개의 독립적인 RNN을 사용하여 입력 시퀀스를 처리\n\n\n\n예를들어 거지같은 행색, 꺼지지 않는이런 문맥상 정보가 필요한경우 사용\n하나의 RNN은 정방향으로 입력받고 다른하나는 역방향으로 입력받는\n양방향으로 메모제이션(계산결과 저장후 다음 것이 결과 사용하는) 얻어진 출력 값을 time-step 기준으로 연결되어 출력\n\n오토 인코더\n\n표현 학습 작업에서 신경망을 사용하도록 하는 비지도 학습법\n\n\n입력이 들어왔을 때, 해당입력을 최대한 압축, 데이터의 특징을 추출하여 본래의 입력형태로 복원시키는 신경망\n즉 한마디로 말하면 복원 비복원 하는 신경망\n입력, 출력층의 뉴런수가 동일 그외에는 일반 MLP 와 비슷\n\nUndercomplete Autoencoder\n\n불완전한 오토인코더\n\n\n출력층이 입력층보다 작아지는, 즉 저처원으로 표현되는 오토인코더\n여기서 활성화 함수를 sigmoid, 손실함수로 MSE를 사용한다면 PCA(주성분 분석)을 구현 할 수 있다\n\n활용\n\n차원감소\n이미지 잡음 제거\n이미지 생성 등\n\n생성형 적대 신경망 (GAN)\n\n예측 값을 도출하는 것이 아닌 새로운 결과 값을 생성하는 모델\n\n단순 원리\n\n지폐 위조범과 그걸 감시해내는 경찰로 설명하겠다\n\n\n위조지폐범은 위조지폐를 만들고\n경찰이 그걸 위조 지폐로 판단하면\n더 진짜 같은 위조지폐를 만들어 낸다\n이 과정을 반복하는 것처럼 진짜 같은 가짜를 생성하게 해주는게 GAN 에 핵심\n\n작동 원리\n\n\n노이즈 이미지 입력\nGenerator 가 가짜 이미지를 생성\nDiscriminator 에 만들어진 가짜 이미지와 실제 이미지를 input\nDiscriminator가 두 사진을 보고 가짜 이미지에 대한 판단을 하는것\n\n가이드라인\n\n판별자 풀링층을 스트라이드 합성곱 층으로\n생성자, 판별자 배치 정규화 사용\n출력층 제외하고는 ReLU 함수\n판별자는 LeakyReLU 활성화 함수\n\nDiffusion Model (확산 모델)\n\n데이터로 부터 노이즈를 조금씩 더해가며 데이터를 완전한 모델로 노이즈로 만들어 가는\n\n\n흔히 하는 이미지 생성 ai가 다 이 모델 이다\n\n원리\n\n\n사진 하나를 주어 진다 치자\n이를 노이즈를 점점 끼게하여 맨 첫번째 사진으로 만든다\n그리고 노이즈 낀 사진을 다시 원래 사진으로 복원하는 과정을 거치게 된다\n\n상세\n\n노이즈:  gaussian noise\n노이즈를 추가하는 단계: Noising\n원본으로 복원하는 단계: Denoising\n\n시계열 데이터 예측 모델\n\nAR: 자기회귀\n\n과거의 값이 현재값에 영향을 미친다는것을 전제 즉 자기상관 시계열, 과거와 현재 값에 관계를 모델링하는\n\n\nMA: 이동평균\n\n시계열을 따라 이동하는 (주식 이동 평균이랑 같은 개념)\n\n\n\nARMA (AR+MR) 모델\n\n과거 값에 간단한 가중치 합+이동평균 하여 예측수정해 가는 모델\n\n\nARMA는 데이터가 꾸준하게 정상적이라는 전제로 함 그래서 주식 이나 이런 실제 데이터를 예측하기에는 좀 그럼\n\nARIMA\n\nx 번의 차분을 수행하여 시계열 데이터를 좀 정상적으로 만들고 ARMA 모델을 적용한 것\n\nSARIMA\n\nARIMA 와 같은 모델을 사용하지만 주어진 빈도(주, 월간)에 대한 항을 추가로 모델링\n\n\n\n예를들어 뭐 1분기 실적 2분기 실적 이런거 할때 좋음\n\n\n그리드 서치와 좋합이 좋다\n\n\n간단한 예제\n\n\nmodel = ARIMA(rail_series,\n              order=(1, 0, 0),\n              seasonal_order=(0, 1, 1, 7)) # seasonal_order 들어가면 SARIMA 모델\n             \nmodel = model.fit()\ny_pred = model.forecast()  # 427,758.6 반환\n용어정리\n역전파\n\n목표와 모델의 예측결과와 얼마나 차이 나는지 확인 후 가중치, 편향 을 뒤에서 앞으로 갱신하는\n\n경사하강법\n\n2차 함수 그래프가 있다 치자, 위에서 계산된 예측결과 x 값이 그래프에 최소 값 으로 부터 기울기가 얼마나 차이나는지를 계산한다\nMSE: 예측값과 실제값 사이의 차이를 제곱한 후 평균을 구한 손실 함수\n\n\n종합하자면 경사 하강법 은 손실함수(MSE 등) 값을 최소화 하도록 만들어낸 알고리즘 인것\n\n\n다변량 시계열: 시계열 분석에 있어 변수가 2개 이상으로 분석할 때\n단변량 시계열: 이건 변수 하나로\n자기상관 시계열: 시계열이 시간이 지연된 자기 자신과 상관관계를 가질 때\n\n예를 들어 지하철 이용객이 일주일 전과 현재가 급격한 차이를 보인다면\n오늘 또는 그때에 무슨 일이 있겠구나를 알 수 있다\n\n\n배치: 훈련 시 데이터 샘플 묶음\n"},"시험-정리/3-1/기말---알고리즘/알고리즘-기말":{"title":"알고리즘 기말","links":[],"tags":[],"content":"퀵 정렬\n\n솔직히 이건 외우자\n\nint pl = left;\nint pr = right;\nint x = a[(pl + pr) / 2];\n \nwhile (pl &lt;= pr) { // pl 항상 ++ pr은 항상--\n\twhile (a[pl] &lt; x) {\n\t\tpl++;\n\t}\n\twhile (a[pr] &gt; x) {\n\t\tpr--;\n\t}\n\tif (pl &lt;= pr) {\n\t\tswap(a, pl++, pr--);\n\t}\n}\n \nif (left &lt; pr) {\n\tquickSort(a, left, pr);\n}\nif (pl &lt; right) {\n\tquickSort(a, pl, right);\n}\n퀵 정렬 (피벗 선택)\n// x[a], x[b], x[c]을 정렬(중앙값의 인덱스를 반환 )\nstatic int sort3elem(int[] x, int a, int b, int c) {\n\tif (x[b] &lt; x[a])\n\t\tswap(x, b, a);\n\tif (x[c] &lt; x[b])\n\t\tswap(x, c, b);\n\tif (x[b] &lt; x[a])\n\t\tswap(x, b, a);\n\treturn b;\n}\n \nint m = sort3elem(a, pl, (pl + pr) / 2, pr);\nint x = a[m];\nswap(a, m, right - 1);\npl++;\npr -= 2;\n\n\n알고리즘을 이해하자면\n\n첫 값, 중앙값, 마지막 값을 정렬하고\n중앙 인덱스 반환 하기\n\n병합 정렬\n\n\nstatic int[] buff; //작업용 배열\nstatic void __mergeSort(int[] a, int left, int right) {\n\tif(left&lt;right) {\n\t\t// 쓰인변수 i, center, p, j, k\n\t\tint i;\n\t\tint center = (left+right) / 2;\n\t\tint p =0;\n\t\tint j =0;\n\t\tint k = left;\n\t\t\n\t\t__mergeSort(a, left, center);\n\t\t__mergeSort(a, center+1, right);\n\t\n\t\t// for문은 buferr 복사\n\t\tfor(i=left; i&lt;=center; i++)\n\t\t\tbuff[p++] = a[i];\n\t\t// while은 원래배열에 뭔짓거리\n\t\twhile(i&lt;=right &amp;&amp; j&lt;p)\n\t\t\ta[k++] = (buff[j]&lt;= a[i]) ? buff[j++] : a[i++];\n\t\twhile(j&lt;p)\n\t\t\ta[k++] = buff[j++];\n\t}\n}\nstatic void mergeSort(int[] a, int n) {\n        buff = new int[n];\n        __mergeSort(a, 0, n-1);\n        buff = null;\n}\n부르트-포스법\n\n무식하게 하나 하나 다 대조해보는\n\nstatic int bfMatch(String txt, String pat) {\n\tint pt = 0; \t// 택스트 담당\n\tint pp = 0;\t// 패턴 담당\n\t\n\twhile (pt != txt.length() &amp;&amp; pp != pat.length()) {\n\t\tif (txt.charAt(pt) == pat.charAt(pp)) {\n\t\t\tpt++;\n\t\t\tpp++;\n\t\t} else {\n\t\t\tpt = pt-pp+1; //+1 인거 기억\n\t\t\tpp = 0;\n\t\t}\n\t}\n\t\n\t// 검색 성공\n\tif (pp == pat.length()) {\n\t\treturn pt-pp; // 이것도 기억\n\t} else {\n\t\treturn -1;\n\t}\n}\nKMP 법\nstatic int kmpMatch(String txt, String pat) {\n\tint pt = 1; // kmpMatch 는 1부터 시작\n\tint pp = 0;\n \n\tint[] skip = new int[pat.length()+1];\n \n\tskip[pt] = 0; // 0 넣고\n\twhile(pt != pat.length()) {\n\t\tif(pat.charAt(pt) == pat.charAt(pp)) //pat 으로만 분석\n\t\t\tskip[++pt] = ++pp; //앞 pt 뒤 pp\n\t\telse if(pp == 0) \n\t\t\tskip[++pt] = pp;\n\t\telse\n\t\t\tpp = skip[pp]; // 여긴 모두 pp\n\t}\n\tpt = pp = 0; // 모든게 끝나면 초기화\n \n//  이후 로직은 부르트 포스와 비슷\nelse if(pp == 0)\n\tpt++;\nelse \n\tpp= skip[pp];\n}\n보이어 무어 법\n\n처음 시행 시 주어진 패턴의 전체가 맞지 않는다면 해당 영역만큼 건너 뛰는게 가능하다\n\nstatic int bmMatch(String txt, String pat) {\n\tint pt;\n\tint pp;\n \n\t// 보이어는 길이 필요\n\tint txtLen = txt.length();\n\tint patLen = pat.length();\n \n\t// Character.MAX_VALUE +1 \n\tint[] skip = new int[Character.MAX_VALUE +1];\n \n\t// 첫 for 문은 Character.MAX_VALUE 까지\n\tfor(pt = 0; pt&lt;=Character.MAX_VALUE; pt++)\n\t\tskip[pt] = patLen;\n\tfor(pt = 0; pt&lt;patLen-1; pt++)\n\t\t// 스킵 배열 참조 = patlen - pt -1;\n\t\tskip[pat.charAt(pt)] = patLen-pt-1;\n\twhile(pt &lt; txtLen) {\n\t\tpp = patLen -1;\n \n\t\twhile(txt.charAt(pt) == pat.charAt(pp)) {\n\t\t\tif(pp == 0)\n\t\t\t\treturn pt; \n\t\t\tpp--;\n\t\t\tpt--;\n\t\t}\n\t\t//여기선 두 번째 for 처럼 첨조 근데 -가 pp 맞으면 첫번째꺼 틀리면 뒤에꺼\n\t\tpt += (skip[txt.charAt(pt)] &gt; patLen-pp) ? skip[txt.charAt(pt)] : patLen-pp;\n\t}\n\treturn -1;\n}"},"시험-정리/3-1/기말---인공지능-프로그래밍-기말/인공지능-프로그래밍-기말":{"title":"인공지능 프로그래밍 기말","links":["시험-정리/3-1/중간---인공지능-프로그래밍-중간/인공지능-프로그래밍-중간","시험-정리/3-1/중간---딥러닝-프로그래밍/딥러닝-프로그래밍-중간"],"tags":[],"content":"로지스틱 회귀\n\n어떤 사건이 발생할지 말지 예측이 아니라 그 확률을 계산하는 것\n\n특징\n\n선형회귀+로지스틱 함수\n\n로지스틱 함수는 시그모이드 함수와 거의 동일\n\n\n이진 분류의 경우 확률이 0.5 이상이면 참 아니면 거짓\n이진 분류는 시그모이드, 다중분류는 소프트 맥스\n\nLogisticRegression\n\ninit:\n\nC: float: 규제 정도 (로그 값) 과적합 방지\nmax_iter: int: 경사하강법 횟수\n\n\nproperty:\n\nclasses_: 분류되는 타깃 정보 저장\ncoef_: 분류되는 가중치\nintercept_: 절편값\ndecision_function(test 데이터) -&gt; list: 계산된 Z 값 (가중치+편향)\npredict_proba(test 데이터) -&gt; list: 각 샘플들 예측 확률\n\n\n\n확률적 경사 하강법\n\n하나의 샘플을 랜덤하게 골라서 오차를 최소하는 지점 a, b 를 고르는 방법\n최소 오차를 찾기 위해 반복하게 되는데 이를 에포크\n\n특징\n\n새로 들어온 데이터에 대해 점진적으로 학습하는\n미니 배치 경사 하강법:\n\n훈련데이터 에서 무작위로 샘플 추출해서 하는방식\n\n\n배치 경사 하강법:\n\n전체 훈련 데이터에서\n\n\n손실함수\n\n로지스틱 손실 함수 (이진분류)\n\n타깃값이 1이면: 예측확률 x 타깃 x -1\n타깃값이 0이면:  1 - (예측확률 x 타깃 x -1)\n\n\n크로스 엔트로피 손실 함수 (다중분류)\n\n엔트로피: 불확실성의 척도 (ex: 동전던지기)\n\n\n\n\n\nSGDClassifier\n\n확률적 경사하강법 분류용 클래스\n\n\ninit:\n\nloss: &quot;log_loss&quot; | &quot;hinge&quot;: 손실함수 log_loss=로지스틱 손실함수\npenalty: &quot;l1&quot; | &quot;l2&quot; = &quot;l1&quot;: 규제\nmax_iter: int: 에포크\ntol: number: 반복조건 (손실이 tol 만큼 줄어들지 않으면)\n\n\nproperty:\n\npartial_fit(특성, 타겟): 점진적 학습\n\n\n\n결정 트리\n\n설명 가능한 AI\n\n특징\n\n\nAI 는 뛰어난 판단 능력을 보이지만 이 판단 과정을 설명하는게 너무 어려움\n\n\n이걸 마치 스무고개 하듯이 구조를 짜 놓는것\n\n\n\n\n분할된 노드가 하나의 값을 가질 때 까지 반복 분활\n\n\n결정 트리는 표준화 할 필요 없다\n\n\n장단점\n\n분류시 계산이 빠르고 쉬움 +\n예측 결과에 대해 왜 그런 결과가 도출되었는지 설명 가능 +\n선형 모형에는 적합하지 못함 -\n과대 적합 가능성이 있음 -\n\n과적합 방지\n\n가지치기:\n\n사전 가지치기: 최대깊이, 각 노드에 최소 샘플 수 정해서 훈련\n사후 가지치기: 훈련 후 데이터 수가 적은 노드 삭제\n\n\n앙상블 기법\n\n지니계수\n\n불순도 측정에 사용되는 값\n\n\n불순도: 여러 범주의 데이터가 섞여 있는 정도 (예측한 값과 실제 값이 다른 값들이 많이 모인 정도)\n지니 불순도: 1 - (데이터총계/특성1개수)**2 + (데이터총계/특성2개수)**2\n\n\n0.5 즉 반반이 제일 낮은 거 1 이 제일 높은 거\n\n\n\nDecisionTreeClassifier\n\n결정트리 클래스\n\n\ninit:\n\nmax_depth: int: 최대 깊이 설정\nrandom_state\n\n\nproperty:\n\nfeature_importances_: list: 각 특성별 중요도 출력\n그 외 다른 훈련 클래스랑 동일\n\n\n\nplot_tree()\n\n결정트리 시각화 함수\n\n\n매개 변수:\n\nmax_depth: int: 시각화 최대 길이\nfilled: bool: 색칠\nfeature_names: list: 특성 이름 정하기\n\n\n\nK-폴드 교차 검증\n\n검증세트 (validation set) 를 K개의 부분(폴드)로 나누는 방법\n\n특징\n\nK 가 3인 경우 (### 3-Fold cross validation)\n\n3개로 세트를 나누고 그 3개 데이터 셋 을 각각 훈련, 평가 하고 평균을 계산\n\ncross_validate()\n\n교차 검증 함수\n\n\n매개 변수:\n\nestimator: any: 학습 모델 객체 (DecisionTreeClassifier 같은거)\nX: any: 훈련 특성\nY: any: 훈련 타겟\ncv: any: 분할기 (이걸로 K(폴드) 값 조정)\nreturn_train_score: bool: 학습에 쓰인 데이터도 스코어 출력 할지 말지\n\n\nReturn:\n\nfit_time: [] score_time: [] test_score: [] train_score: [] (return_train_score)\n\n각각은 다 list\n\n\ntest_score 나 train_score 이거 리스트 평균구하면 교차검증 끝\n\n\n\nStratifiedKFold()\n\n분류 모델에 사용하는 분할기\n\ncross_validate(model, train_input, train_target, cv=StratifiedKFold())\n\n매개 변수:\n\nn_splits: int=5: K 값 조정\nshuffle: bool: 데이터를 섞을지 말지\n\n\n\n그리드 서치\n\n하이퍼 파라미터 값이 많을경우 하나씩 입력해보며 최적 파라미터 구하는 방법\n\n특징\n\nK-폴드 검증이 데이터 세트에 대한 검증이라면 이건 하이퍼 파라미터에 대한 검증\n교차검증 까지 해줌\n\nGridSearchCV\n\nGridSearch 객체\n\n# min_impurity_decrease가 찾을 파라미터임\nGridSearchCV(dt, {&#039;min_impurity_decrease&#039;: [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}, n_jobs=1)\n\ninit:\n\nestimator: 학습 모델 객체\nparam_grid: dict: 찾을 하이퍼 파라미터를 딕셔너리로 저장\nn_jobs: int: 사용할 cpu 코어 수 (-1 이면 모든 코어 사용)\ncv: any: 분할기 (이걸로 K(폴드) 값 조정)\n\n\nproperty:\n\nfit(X, Y): 최적 파라미터 찾기수행\nbest_params_: dict: 최적값 찾아서 반환 {min_impurity_decrease: 0.001}\ncv_results_: dict: 각 파라미터별 학습 결과를 도출 (DataFrame 생성해서 볼 수 있음)\n\nmean_test_score: arr: 각 파라미터별 교차 검증 평균성능\n\n\n\n\n\nRandomizedSearchCV\n\n랜덤서치 객체\n\n\n그리드서치에 문제가 내가 지정해놓은 탐색 하이퍼 파리미터가 최적값과는 먼 경우 실제 최적값을 찾는다는 보장이 없다\n랜덤서치는 단순 범위만 지정해놓고 모델이 직접 거기중에 하나 뽑아서 최적 파라미터 구하는 방식\n\nRandomizedSearchCV(dt, {&#039;min_impurity_decrease&#039;: uniform(0.0001, 0.001)}, n_iter=100)\n\ninit:\n\nestimator: 학습 모델 객체\nparam_grid: dict: 찾을 하이퍼파라미터를 딕셔너리로 저장\n\nrandint(0, 10): 이 안에 있는 정수 값으로 범위 제한\nuniform(0, 10: 이 안에 있는 실수 값으로 범위 제한\n\n\nn_jobs: int: 사용할 cpu 코어 수 (-1 이면 모든 코어 사용)\nn_iter:int : 주어진 범위 중 실제 훈련에 쓰일 거 계수\n\n\nproperty:\n\n그리드 서치와 동일\n\n\n\n앙상블\n\n여러개 분류 모델을 조합하여 분류의 과적합을 방지하는 방법 (분류)\n\n\nVoting:\n\n같은 데이터 셋을 다른 알고리즘으로 예측하고 다수결로 투표해서 최종 예측 값을 결정하는\n\n\nBagging:\n\n같은 모델을 사용하는데 다른 데이터 셋 으로 예측하고 투표해서 최종 예측 값을 결정하는\nBootstrap: 훈련 데이터에서 중복을 허용하여 랜덤하게 샘플을 추출\n\n\nBoosting:\n\n이전 모델에 예측 결과가 오답인 모델에 더 많은 가중치를 부여\n각각 모델의 가중치가 추후 예측하게 될 모델에 영향을 미쳐 분류 규칙을 만들어 내는 것\n\n\n\n랜덤 포레스트 (Bagging)\n\n결정 트리기반, 여러개의 결정트리 모델(depth 를 다르게하는등)을 만들고 예측값을 취합 하여 결정하는 방식\n\n특징\n\n각 모델들의 독립성, 무작위성, 일반화 등을 최대화 시켜 최대한 모델간의 상관관계를 약화시킨다\n결정트리 장점을 그대로 \\\n하이퍼 파라미터가 많아 튜닝시간이 길다\n랜덤 포레스트는 여러 모델을 조합한거라 그리드서치보다 성능이 더 뛰어나다\n\nRandomForestClassifier\n\n랜덤 포레스트 수행 객체\n\n\ninit:\n\nn_estimators: int=100: 결정트리 계수\nbootstrap: bool: 부트스트랩 허용, 비허용\noob_score: bool: 부트스트랩에 포함되지 않은 남은 샘플을 이용하여 훈련\n\n\nproperty:\n\nDecisionTreeClassifier 와 동일\n\n\n\nExtraTreesClassifier\n\n랜덤 포레스트 수행 객체 2\n\n\n결정트리 만들때 전체 데이터 셋 활용\n노드 분할시 가장 좋은 게 아니라 랜덤 분활\n\n그레이디언트 부스팅 (Boosting)\n\n깊이가 앝은 결정트리를 사용하여 오차를 보정해가는, 선형회귀 에도 사용 가능\n\n특징\n\n경사하강 방식을 사용하여 결정트리를 계속 추가하여 손실이 적을 수 있도록\n과대적합에 강함\n랜덤 포레스트보다 성능은 더 좋지만 훈련 속도는 느림\n\nGradientBoostingClassifier\n\n그레이디언트 부스팅 수행 객체\n\n\ninit:\n\nlearning_rate: float: 학습률 (오차를 얼마나 보정할 것인지)\n그외 RandomForestClassifier 와 동일\n\n\nproperty:\n\nRandomForestClassifier 와 동일\n\n\n\n군집\n\n이게 어떤 건지(타겟) 모른 상태로 분류하는 것\n\n\n군집 (cluster): 비슷한것 끼리 묶음\n군집화(clustering): 주어진 데이터를 cluster(군집) 으로 묶음\n중심점(centroid): 각 클러스터 중심\n\nK-Means (K 평균)\n\nK-Means 알고리즘: 각 군집에 평균을 사용하여 K개의 군집으로 묶는 것\n\n작동 원리\n\n\n군집 개수 K 정함\n초기 중심점 설정 (랜덤으로 정하거나 특정 알고리즘에 의해)\n모든 중심점 중 제일 가까운것들을 군집에 할당\n중심점을 조금씩 움직이면서, 데이터들을 재할당\n어느정도 할당 당하면 중심점의 위치가 변하지 않게 됨\n\n장단점\n\n\n\n직관적이고 구현이 쉬움\n\n\n\n\n대용량 데이터에 적용 가능\n\n\n\n\n클러스터의 모양을 원형으로 간주\n\n\n\n\n특성이 많을 수록 값이 분산되어 군집하기 어렵다는\n\n\n\n실루엣 분석\n\nK-Means 에 성능 평가 방법\n\n\n각 군집 간의 거리가 얼마나 효율적으로 분리되있는지를 나타내는\n1 에 가까울 수록 멀리, 0 에 가까울 수록 가깝다는\n음수값이 나온다면 잘못 할당된것\n일반적으로 값이 높을수록 군집화가 잘 됬다 볼 수 있지만 항상 그런건 아니다.\n\n적당히 분포된게 제일 좋은거\n\n\n\n적절한 K 찾는 방법\n\n엘보우: K 값을 늘려 가며 이니셔의 변화를 관찰하는 법 일반적으로 클러스터 개수가 늘면 이니셔도 감소하여\n\n이니셔: 클러스터 중심과 속한 데이터 사이 거리의 제곱합\n\n\n\nKMeans\n\nK-Means 수행 객체\n\n\ninit:\n\nn_clusters: int=8: 군집 개수\ninit: &#039;kmeans++&#039; | &#039;random&#039;: 초기 중심점 설정 방법\nn_init: int=10: 초기 중심점 찾기 반복 횟수\nmax_iter: 최대 반복횟수\n\n\nproperty:\n\nfit(X): 다른 것들과 다르게 특성 만 가지고 학습 (타겟 정보가 없는 알고리즘)\ninertia_: 각 샘플들 이니셔값\ncluster_centers_: list[list[float]]: 각 군집에 중심점\n\n\n\n차원 축소\n\n특성이 많은 고차원 데이터의 중요하지 않은 특성을 빼서 꼭 필요한 특징만 데이터로 표현하는 것\n\n특성공학\n\n예를 들어 키, 몸무게 라는 특성은 2차원으로 표현 가능하다, 근데 여기에 성별이 추가되면? 3차원\n필요한 이유:\n\n모델이 복잡해지고, 오버피팅 가능성이 있다\n데이터 개수를 늘리면 개선 되기도 하는데 학습 속도 느려짐\n\n\n\n투영\n\n\n3차원 데이터를 2차원으로 이동 시켰다.\n\n\n고차원 데이터를 저차원 공간으로 수직 이동시키는\n일부 고차원 데이터를 표현 가능하다\n\n메니폴드\n\n데이터가 존재하는 공간을 다양한 시각으로 바라 볼 수 있다는\n\n\n데이터가 고차원 이라도 데이터에 집합을 포함하는 저차원에 무언가(다향체)가 있다는것\n\n주성분 분석 (PAC)\n\n\n가장 널리 사용하는 차원 축소 기법\n\n\n기존 데이터에서 가장 연관성 없는 데이터의 축을 찾아 그 차원으로 축소\n찾아낸 주성분 중 몇 개의 특성을 선택하여 분석을 실행하면 계산과 시각화가 용이\n\n원리\n\n각 축에 평균값을 구하고 해당점이 원점이 되도록 shift 한다\n데이터에 원점을 지나는 수선의 발 내려 해당 길이가 최대가 되는 직선 찾기\n데이터에서 원점을 지나는 직선의 수선에 발을 내려, 해당길이가 최대가 되는 직선 찾기\n찾은 직선을 PC1(첫 주성분), Loading score 구하기 (X 축 길이와 Y 축 길이의 비율)\nPC1에 직교하는 직선을 PC2로 설정\nPC1과 PC2를 축으로 하여 회전시켜 주성분 분산 계산\n이후 막대그래프를 생성해 얼마나 특징점이 있는지 계산\n더 특징을 나타내는 주성분을 선택하여 축소\n\nPAC\n\nPAC 수행 객체\n\n\ninit:\n\nn_components: int: 주성분 개수\n\n\nproperty:\n\ncomponents_: 찾아낸 주성분\nexplained_variance_ratio_:  각 주성분에 분산\ninverse_transform(): 주성분으로 원본데이터 재구성\n\n\n\n용어정리\n점진적 학습 (온라인 학습)\n\n학습이 완료된 모델에 미니배치라 부르는 작은 데이터를 주입하여 학습하는 방식\n\n\n무슨 소리냐면 모델을 대중에게 배포한다 치면 거기서 쌓인 데이터를 통해 또 학습하고 학습하며 점진적으로 하는 방식\n\n검증세트 (validation set)\n\n테스트 세트를 이용하지 않고 모델 성능 측정 위해\n\n\n결국은 테스트 세트로 성능체크 해도 테스트 세트에 편향적인 모델이 만들어 질 수 있으니\n전체 데이터 중 20 ~ 30% 정도로 둠\n\n이미지 처리\n\n흑백사진 인 경우 배열은 2차원 이면 충분하다. (블랙 값에 수치만 조정 하면되니)\n\n그 외\n\n독립변수, 종속변수:\n\n키가 몸무게의 영형을 얼마나 주나 뭐 이런거\n이때 키 는 독립변수x, 몸무게 는 종속변수 y\n또는 특성, 타겟\n\n\n정형 &amp; 비정형 데이터:\n\n정형은 우리가 읽는데이터, 비정형은 컴퓨터가 읽는거\n\n\n\n함수정리\n함수정리\n표준화\n\nStandardScaler 클래스 사용\n\nss = StandardScaler()\nss.fit(train_poly)\ntrain_scaled = ss.transform(train_poly)\n\nMinMaxScaler 클래스 사용\n\nmm = MinMaxScaler()\nmm.fit(train_input)\ntrain_scaled = mm.transform(train_input)\nscipy\n\nexpit(list): 시그모이드 함수 (decision_function() 결과값을 넣어 시그모이드로 예측한 값 얻음)\nspecial.softmax(list, axis=1): 소프트맥스 함수 (이것도 decision_function()과 같이)\n\n활성화 함수\n기타\n\npyplot:\n\nimshow(cmap=&#039;gray-r&#039;): 색상 반전\n\n\n"},"시험-정리/3-1/중간---Iot/Iot-중간":{"title":"Iot 중간","links":[],"tags":[],"content":"회로 기초\n\nGND 는 - 이고 보통 저항 달고 저항 끝을 그라운드에 연결 한다\n+ 극쪽은 아날로그, 디지털 핀에 다이렉트로 연결한다\n5V 전압은 + 이다\nLED에 긴다리가 + 짧은다리가 - 이다\nLOW 값은 0V HIGH 값은 5V\n버튼을 누르면 브레드 보드에 양쪽 칸이 모두 활성화\n버튼은 따로 음극, 양극을 따지지 않는다\n\n플로팅 현상\n\n디지털 핀의 전압이 HIGH 와 LOW를 왔다 갔다 하는 현상\n즉 HIGH 도 LOW 도 아닌상태\n디지털 핀을 입력으로 설정하면 아주 작은 전류가 흐르기에\n\n\n그래서 평상시 전압을 LOW 또는 HIGH에 고정\n\n풀다운\n\nLOW 에 고정\n\n\n버튼:\n\n5V 에서 전류가 옴\n입력핀과 그라운드 핀이 저항을 사이에 두고 연결\n안누르면 모든 작은 전류가 그라운드로 빠저 전압이 LOW 가 됨\n\n\n\n풀업\n\nHIGH 에 고정\n\n\n버튼:\n\n5V 에서 전류가 옴\n5V 전원핀과 입력핀 사이에 저항을 둠\n5V 전원선과 연결되에 HIGH 가 됨\n반대로 누르면 LOW\n\n\n\n\n5V 에 저항 있으면 풀업\n입력핀, 그라운드사이에 저항있으면 풀다운\n\n아날로그\n\n0V~5V 사이를 왔다갔다 할 수 있는\n\n\n입력은 입력핀에 꼿고\n출력은 ~ 표시있는 데다 (3,5,6,9,10,11 핀)\n\nPWM\n\n디지털 기계가 아날로그 신호를 출력하는 방법\n\n\n사람눈엔 10ms(100hz) 수준에 깜박임은 눈으로 감지가 안됨\n예를 들어 led를 5초 키고 5초뒤 끈다면 계속 켜져있는거 같음\n근데 이 켜짐과 꺼짐에 시간 비율을 잘 조정하면 밝기강도가 낮아지는 효과를 볼 수있음\n이 비율을 표시하는 용어가 듀티 사이클 임\n\n듀티 사이클\n\n시간당 HIGH 비중에 비율\n아두이노 최대값은 255\n\n채터링\n\n스위치가 눌릴때 기계적인 진동에 의해 매우 짧은 시간동안 접점이 붙었다 때지는 현상\n\n\n즉 여러번 눌린거 같은 효과를 가진다\n이 때문에 토글 구현 하는것이 힘들다\n\nvoid loop() {\n    int mode = digitalRead(5);\n    if (mode) {\n    // 버튼을 누를때 여러번 눌리는 판정이라 수시로 바꿔버림\n        state = !state;\n    }\n    digitalWrite(10, state);\n}\n디바운싱\n\n짧은 시간동안의 버튼 상태 변화를 무시하여 채터링을 해결\nSW, HW 방법이 있음\n\n로직을 통한 임시 해결\n\n이전 상태 값과 비교하여 눌릴때만 작동하도록 한다\n\nint mode = digitalRead(5);\n// 이전값이 땐 상태고 현제 버튼을 누른 경우 발생되게\nif (last == LOW &amp;&amp; mode == HIGH) {\n    state = !state;\n}\ndigitalWrite(13, state);\nlast = mode;\n\n이 경우 채터링으로 인한 오류가 존재함\n\npolling 방식\n\n한번 스위치를 읽고 일정시간 지난 뒤 한번 더 읽어 본다\n\nint mode = digitalRead(5);\nif (mode != last) {\n\t// 사람 손에 눌린거라면 0.1만에 값이 달라질 수가 없기 떄문\n\tdelay(10);\n\tmode = digitalRead(5);\n}\n// 나머지 코드는 위에랑 동일\nInterrupt 방식\n\n아두이노가 보내는 이벤트로 처리\n\nvoid setup() {\n// Interrupt는 2, 3 번 핀만 사용 가능하다\n  attachInterrupt(digitalPinToInterrupt(2), change, RISING);\n}\n \nvolatile int val9 = LOW;\nvoid loop() {\n  digitalWrite(9, val9);\n}\n// _delay_ms 사용\nvoid change() {\n  _delay_ms(50);\n  if (digitalRead(2) == LOW) return;\n  val9 = !val9;\n// millis() 사용\nvoid change() {\n  t_now = millis();\n  if ((t_now - t_prev) &gt; 300) {\n    t_prev = t_now;\n    t_switch = !t_switch;\n  }\n}\n\n\n2번 인자 는 처리함수 이벤트가 발생하면 해당 함수 실행\n\n\n3번 인자 는 RISING, FALLING, CHANGE, LOW 사용가능\n\nRISING: 전압이 HIGH 로 바뀔때\nFALLING: 전압이 LOW로 바뀔때\nCHANGE: HIGH, LOW 가리지 않고\n\n\n\n_delay_ms() 함수 사용이유:\n\ndelay() 함수는 아두이노 자체에 주는 딜레이\nInterrupt 가 호출되는 순간 작동을 안한다\n_delay_ms() 함수는 소프트웨어 적으로 주는 딜레이\n\n\n\nvolatile 예약어\n\nInterrupt에 의해 값이 바뀌면 그 값을 인식 못하는 경우가 있음\n변수를 읽을 때 마다 항상 Ram 에 접근해서 읽어오게\n\n\n\n어노드 / 케소드\n\n어노드:  LED 에 긴다리가 +\n케소드: LED 에 긴다리가 -\n\n\n3색 LED는 저항 없이 바로 그라운드로 연결해도 된다\n만일 어노드면 3.3V 또는 5V 에 연결\n\n7세그먼트\n\n\n가운데 단자를 그라운드에 연결 (220 짜리 저항과 함께)\n어노드 타입:\n\n키려면 LOW 끄려면 HIGH\n\n\n케소드 타입\n\n키려면 HIGH 끄려면 LOW\n\n\n\nSerial 통신\n\n아두이노가 사용하는 통신 프로토콜\n\nif (Serial.available()) {\n    content = Serial.readString();\n    Serial.print(content);\n}\n함수 사용\n\nSerial.begin(): 시리얼 사용 설정\nSerial.available(): int: 버퍼에 저장된 데이터 길이 반환\nSerial.read(): char: 맨 앞에 1바이트 읽음\nSerial.readString(): String: 전체 바이트 읽음\n\n피에조 스피커\n\n소리내는 스피커\n\n\ntone() 이후에 또 다른 tone() 이 들어갈 경우 delay 안주면 연속적으로 됨\n이게 무슨 소리냐면 재생시간은 아두이노 시간 흐름에 영향을 안 받음\n재생시간을 1000을 준다 해서 tone() 함수가 1000 동안 대기 하는 게 아님\n\n함수 사용\ntone(핀, 소리값, 재생시간):  재생시간 동안 소리값 소리를 냄\nnoTone(핀): 해당핀 소리 끔\n초음파 센서\n\n초음파를 사용해 사물이나 벽까지에 거리를 알려주는\n\n\nVCC: 5V\nGND: GND\n나머지는 디지털 핀\n\n동작 원리\n\nTrig 부분에서 초음파를 보냄\n벽에 부딪히면 초음파가 반사\nEcho 부분으로 초음파는 돌아올 곳으로 설정\nEcho 부분이 초음파를 받으면 전압이 HIGH 가 됨\n\n\n이 돌아오는 시간을 계산 하면 거리를 알 수 있다\n물체와의 거리 (cm) = 왕복 소요 시간(㎲) / 58.3\n\n  digitalWrite(2, HIGH);\n  delayMicroseconds(10);\n  digitalWrite(2, LOW);\n  \n  long duration = pulseIn(3, HIGH);\n  long distence = duration / 58.2;\n함수 사용\n\ndelayMicroseconds(값): 마이크로초 딜레이\npulseIn(핀, LOW|HIGH): LOW|HIGH 되는 시간 초 계산\n\n모터\nDC 모터\nDC(직류) AC(교류)\n\n전기 → 역학적 에너지\n\n\n\n특징:\n\n회전속도 방향제어 용이\n기동 토크가 큼\n작은 모터로 힘을 내려면 기어박스 사용\n\n\n\n사용:\n\n자동차/보트 무선조종\n\n\n\n모터 드라이버랑 연결해서 사용\n\n\nvoid loop() {\n// a, b = 좌우 HIGH 로 준 방향으로\n// 모터 드라이버에 IN 방향\n digitalWrite(a,HIGH);\n digitalWrite(b,LOW);\n// 속도를 PWM 으로 결정 모터 드라이버에 ENA\n analogWrite(speed,200);\n delay(1000);\n}\nL298N 모터 드라이버\n서브모터\n\n전압입력 → 회전각\n\n\n특징:\n\nDC 모터와 동일\n움직임 제어를 위한 제어부 포함\n\n\n사용:\n\n비행기 날개\n\n\n\n함수 사용\n\ninclude “Servo.h”\n\n\nservo.attach(핀): 서브모터 사용 설정 (setup에)\nservo.write(값): 해당 각도로 설정\n\n함수정리\n\npinMode(핀, input | output): 핀모드 설정\ndigitalWrite(핀, 값): 디지털 쓰기\ndigitalRead(핀, 값): 디지털 읽기\nanalogRead(핀, 값): 아날로그 읽기\nanalogWrite(핀, 값): 아날로그 읽기\ndelay(): 딜레이 주기\nrandomSeed(값): 랜덤 시드 설정\nrandom(최대값): 최대값 -1 값중 랜덤값\nmillis(): 아두이노 상대 시간 밀리초 반환\n"},"시험-정리/3-1/중간---딥러닝-프로그래밍/딥러닝-프로그래밍-중간":{"title":"딥러닝 프로그래밍 중간","links":[],"tags":[],"content":"\n딥러닝(심층학습): 기계학습 종류 중 하나인 인공신경망 방법론 중 하나이다.\n\n딥러닝의 중요 요소\n\n데이터의 양과 질:\n\n잡음이 없는 데이터와 많은 데이터 양이 확보\n\n\n적절한 모델선택:\n\n이미지 처리: CNN 자연어처리: RNN\n\n\n하이퍼 파라미터 튜닝:\n\n학습률, 배치크기, 에폭 수 등의 하이퍼 파라미터를 적절히 설정\n\n\n고성능 pc:\n\n좋은 CUDA 그래픽 성능을 가진 글카 필요\n\n\n\n퍼셉트론\n\n\n다수의 신호를 입력받아서 하나의 신호를 출력하는 알고리즘을 의미 쉽게말하면 여러개를 투입해서 하나의 결과값을 도출 0, 1\n하나의 층 안에 놓인 하나 이상의 TLU로 구성 각각 TLU는 모든 입력에\n입력 구성 입력 층\nTLU 층 구성 출력 층\n사이킷런에서 Perceptron Api 지원\n\nfrom sklearn.linear_model import Perceptron\nper = Perceptron()\n다층 퍼셉트론 (MLP)\n\n\n퍼셉트론을 여러개 쌓아올리면 일부 제약을 줄일 수 있음\n\n비 선형적으로 분리되는 데이터에 대해서는 제대로된 학습이 불가하다\nXOR(입력값이 같으면 0 다르면 1) 연산 학습 불가\n\n\n기존 퍼셉트론에 은닉층(hidden layer) 이 추가된 것\n입력, 출력층 사이 여러 개 은닉층이 존재하는 것을 &lt;심층 신경망&gt;\n이것을 학습하기 위해 고안된 알고리즘이 &lt;딥러닝&gt;\nsklearn 에서 MLPRegressor() api 사용하여 해볼 수 있음\n\n순전파, 역전파 알고리즘\n\n순전파:\n\n입력층 → 은닉층 → 출력층 순으로 노드에 값이 전달되는 형식\n\n\n역전파:\n\n후진모드 자동미분과 경사 하강법을 결함\n은닉층에선 출력값에 대한 기준이 존재하지 않음 그래서 오차율 측정이 어려움\n출력층에서 발생하는 오차값을 역으로 입력층에 보내면서 은닉층에 노드사이즈에 가중치를 제조정\n이 과정을 반복하며 오차가 더 줄어들때까지 함 이것을 &lt;에보크&gt;\n\n\n\n회귀\n\n하이퍼 파리미터:\n\n은닉층 수: 1~5\n은닉층 뉴런 수: 10 ~ 100\n출력 뉴런 수: 예측 차원 마다\n은닉층 활성화 함수: RELU\n출력층 활성화 함수: None\n손실함수: MSE(평균 제곱 오차), 이상치 발생시 후버\n\n\n\n와이드 &amp; 딥 신경망\n\n비 순차적 신경망\n정규화 이후 층 연결 한번하고\n정규화로 넘어와서 은닉층을 구성함\n\n\n즉 정규화 과정이 두 번 반복되는 구조\n\n\n케라스\n\n딥러닝 프레임워크, 그러나 텐서플로우에도 자체 내장이 되어있음\n\n구현방식\nSequential API\ntf.random.set_seed(42)\n1 model = tf.keras.Sequential() \n2 model.add(tf.keras.layers.Input(shape=(28, 28)))\n3 model.add(tf.keras.layers.Flatten())\n4 model.add(tf.keras.layers.Dense(300, activation=&quot;relu&quot;))\n5 model.add(tf.keras.layers.Dense(100, activation=&quot;relu&quot;))\n6 model.add(tf.keras.layers.Dense(10, activation=&quot;softmax&quot;))\n\n모델 만들기\n첫번째 층(입력층)을 만들어 모델에 추가\nFlatten 층 추가\n\n주요 특징을 1차원 자료로\n\n\n뉴런 수 300 가진 Dense 은닉층 추가\n뉴런 수 100개 가진 Dense 은닉층 추가\n클래스 마다 하나씩 뉴런 수 10개 가진 Dense 은닉층 추가\n\nFunctional API\nnormalization_layer = tf.keras.layers.Normalization()\nhidden_layer1 = tf.keras.layers.Dense(30, activation=&quot;relu&quot;)\nhidden_layer2 = tf.keras.layers.Dense(30, activation=&quot;relu&quot;)\n \ninput_ = tf.keras.layers.Input(shape=X_train.shape[1:])\nnormalized = normalization_layer(input_)\nhidden1 = hidden_layer1(normalized)\nhidden2 = hidden_layer2(hidden1) \n \nmodel = tf.keras.Model(inputs=[input_], outputs=[output])\nSubclassing API\n\n위 Functional, Sequential 장 단점\n\n장점:\n\n모델 저장 복사 쉬움\n구조를 출력, 분석 쉬움\n\n\n단점\n\n정적 구조: 반복문과 다양한크기 등의 동적인 구조가 아님\n\n\n\n\n\n\n명령형 프로그래밍 스타일 이 필요하면 서브 클래싱이 답\n\nclass WideAndDeepModel(tf.keras.Model): # tf.keras.Model 을 상속 받아서 생성\n    def __init__(self, units=30, activation=&quot;relu&quot;, **kwargs):\n        super().__init__(**kwargs)\n        self.norm_layer_wide = tf.keras.layers.Normalization()\n        self.norm_layer_deep = tf.keras.layers.Normalization()\n        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n        self.main_output = tf.keras.layers.Dense(1)\n        self.aux_output = tf.keras.layers.Dense(1)\n콜백\n\nModelCheckpoint: 일정 간격으로 모델의 체크포인트 저장\nEarlyStopping: 일정 에포크 동안 점수향상 없으면 중단시키기\n\n하이퍼 파라미터 튜닝\n\nGridSearchCV, RandomizedSearchCV를 사용하여 하이퍼 파라미터 튜닝\n은닉층수, 각 층의 뉴런수, 학습률 등\n\nrandom_search_tuner = kt.RandomSearch(\n    build_model, objective=&quot;val_accuracy&quot;, max_trials=5, overwrite=True,\n    directory=&quot;my_fashion_mnist&quot;, project_name=&quot;my_rnd_search&quot;, seed=42)\n하이퍼 파라미터\n\n모델에 설정하는 변수 학습률, 에보크수(훈련 반복횟수), 가중치 초기화 등을 결정\n\n종류\n\n학습률: 일반적으로 최적의 학습률은 최대 학습률의 절반정도\n옵티마이저: 더 좋은 옵티마이저 선택\n배치크기: VRAM에 맞는 가장 큰 배치 크기 사용이 권장\n활성화 함수: ReLU가 대부분에 경우에 좋음\n반복횟수: 조기종료 사용하는게 좋음\n\n은닉층의 뉴런 개수\n\n필요한 입, 출력 작업에 따라 결정\n구성방식은 각 층의 뉴런을 점점 줄여 깔대기 식\n과대 적합이 생기기 전까지 점진적으로 늘려가는 식\n\n합성곱 신경망 (CNN)\n\n이미지 처리에 탁월한 신경망 알고리즘 (Convolutional Neural Network)\n\n\n그림은 픽셀이 달라저도 의미는 같은 경우가 많은데 다층퍼셉트론 만 가지고는 그게 안됨\n\n합성곱 층\nConv2D\n\n많은 ram을 요구, 왜냐하면 각 층에서 계산된 모든 연산값을 가지고 있어야 되서\n보통 그림 데이터는 3차원인데 이를 2차원 형태로 배치하는 과정에서 문제\n한 픽셀, 픽셀에 색 정보는 담을 수 있지만 그 위치값은 담지를 못함\n\n구조\n\n\n첫 층에 뉴런은 모든 픽셀에 연결하는 게 아닌 뉴런 수용장 안에있는 픽셀만 연결\n그 다음 층에는 그렇게 연결한 것들을 연결\n저수준 → 더큰 고수준 특성으로 조합 되는 방식\n이런식으로 커널(필터) 라는걸 만들어서 각 행열을 곱하고 결과를 냄\n이렇게 반복해서 출력값을 다 채우면 그게 &lt;특성맵&gt;\n\n\n이런 식으로 여러 특성맵을 쌓아 여러 특성을 감지하게 된다\n\n스트라이드\n\n한 수용장과 다음 수용장 사이 수직, 수평 스텝 크기\n큰 입력층을 훨씬 작은 층에 연결하면 계산 복잡도 가 크게 낮아짐\n\n제로 패딩\n\n입력크기보다 특성맵이 작아지는걸 방지하기 위해 테두리 값을 0으로 채우는 것\n\n풀링 층\n\n계산량, 메모리량, 파라미터 수를 줄이기 위해 입력 이미지에 부표본을 만드는 것\n가중치가 없어 최대, 평균 합산함수 사용\n최대 풀링, 최소 풀링\n최대 풀링에 경우 작은 변화에 대한 불변성도 챙겨줌\n\n\n최대 풀링은 해당 영역에서 최대 값을 검출하여 출력맵을 구성\n평균은 평균 으로\n\n종합 CNN 구조\n\n입력 → 합성곱 → 풀링 → 합성곱 → 풀링 → 완전연결\n\n대표적인 CNN 구조\n\nLeNet-5: 손글씨 숫자인식에 널리사용\nAlexNet: 2012 이미지넷 우승작, 처음으로 합성곱 위 합성곱 구조를 채택\nGoogLeNet: AlexNet 보다 10배 적은 파라미터를 가지면서 훨씬 효과적\nResNet: 스킵연결, 숏컷 연결 구조, 3.6 이하에 탑5 오류율\n\n\n정확도, 모델크기, CPU GPU 속도를 고려하여 선택하는게 좋음\n\n그 외\n\n객체 탐지:\n\n이미지 중앙에 놓인 하나의 물체 분류 위치 찾기\nCNN으로 이미지 훑으면서 예측을 만듬\n\n\n객체 추적:\n\nDeepSORT56:\n\n이전에 감지된 객체에 가장 가능성이 높은 위치로 지정\n딥러닝으로 새로운 탐지결과와 기존객체와의 유사성 비교\n헝가리 알고리즘 사용해 새로운 탐지 대상을 기존 추적 객체에\n\n\n\n\n시멘틱분활:\n\n사진에 나온 요소들이 해당하는 위치를 분활 하는것\n\n사전 훈련된 CNN을 FCN 으로 변환\nCNN이 입력이미지 에 적용하는 전체 스트라이드는 32\n업샘플링 층을 하나 더 추가\n\n\n\n\n\n텐서플로우 기본\n\n텐서: 데이터의 배열 edge 를 따라 흐르는 값\n\nrank(차원), shape(행, 열), type 매개변수를 가짐\n\n\noperation: 추상적 연산, numpy 같은거\n\nconstant, variable, placehorder 등등\n\n\n\n실행\na = tf.Variable(10)\nb = tf.Variable(20)\n \ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n  sess.run(init)\n  sess.run(tf.add(a, b))\n\n코드가 아무리 상단에 위치한다고 한들 run() 함수 흐름에 따라 가게 된다\n즉 tf.add(a, b) 하기 전까지는 a, b 값은 undefined\n\n변수선언\n a = tf.Variable(값)\n b = tf.constant(값) #상수\n c = tf.cast(b, tf.float32)\n\nconstant는 클래스가 아니라\nVariable은 사용하기전 반드시 초기화 과정을 거쳐야 함\nglobal_variables_initializer()\n텐서플로우 변수는 형변환이 자동으로 이뤄지지 않는데 자료형으로 인한 오류 방지 목적임\n형변환은 성능 문제를 일으키므로 주의해서 사용\ntf.Tensor: 변경 불가능한 객체\n\nplaceholder\na = tf.placeholder(2, tf.int32)\n\n변수 값을 이후에 초기 하는 용도 즉 미리 메모리 값을 할당하는\ntf.Session() 과정에서 실행 흐름에 초기화 해 줄 수 있음\n\nsess.run(실행변수, {a: 10})\n연산함수\n\nadd(): 덧셈\nsubtract(): 뺄셈\nmultiply(): 곱셈\ndivide(): 나눗셈\n\n추가 지원 자료구조\n\n희소 텐서\n텐서 배열\n래그드 텐서\n문자열 텐서\n집합\n큐\n\n기타\n\ntensor → numpy\n\na = tf.constant(1)\na.numpy()\n\nnumpy → tensor\n\na = np.array([10, 20])\ntf.square(a)\n\n연산문제\n\na = tf.constant([1, 2, 3], tf.float32)\nb = tf.constant([5, 5, 5], tf.float32)\n \nc= a+b\nc= tf.add(a,b)\n\n모든 연산은 연산 함수를 쓰던 기본 연산자를 쓰건 동일하다\n\n용어정리\n후버 손실\nL1 손실: 그래프가 V 형태라 미분 불가 지점이 존재\nL2 손실: 그래프가 U 형태라 모든곳 미분 가능 그러나 이상치에 취약\n\nL1, L2 손실을 보완한 모든곳 미분가능, 이상치에 강한 손실함수\n\n활성화 함수\n\n퍼셉트론의 출력값을 결정하는 비 선형 함수\n\n사인(sign) 함수\n\n\n입력값의 총 합이 0보다 크면 1, 작으면 -1을 출력하는 함수\n계단 함수 라고 부르기도함\n\n시그모이드(Sigmoid) 함수\n\n\n모든 입력값에 대해 출력값이 실수 값으로 정의\n값이 작아질수록 0, 커질수록 1에 수렴\n극단적으로 0 과 1만 반환하는 사인 함수에 단점을 보완하여 이상치 문제를 해결\n\n랠루(Relu) 함수\n\n\n입력값이 양수면 입력값을 출력\n음수면 걍 0 으로\n계산효율성이 시그모이드 보다 높다\n시그모이드에서 발생 할 수 있는 포화문제 방지\n\n포화문제란 출력이 1또는 0에 너무 가까워져서 가중치와 편향을 업데이트 하기 어려워 지는 것\n\n\n\n소프트맥스(Softmax) 함수\n\n시그모이드와 비슷\n0~1 값을 정규화 하여 출력, 총합이 1\n\n가중치, 편향\n\n가중치: 어떤 데이터 집합이 있을 때 각각의 데이터에 중요도를 나타내는 수치\n편향: 가중치를 다 더한 뒤 해당 값에 더해주는 상수, 예를 들자면\n인싸라면 파티에 갈 편향은 10이라 치고\n아싸라면 파티에 갈 편향을 0이라 치자\n둘 다 파티에 갈 모든 경우(가중치) 에 합이 10일때\n인싸는 합이 20이므로 확율이 높고\n아싸라면 합이 0이기 때문에 확율이 낮다\n\n\n수식에서 가중치는 W, 편향은 B로 많이 표현된다.\n\n뉴런(신경망)\nwww.youtube.com/watch\n\n하나의 숫자를 담는다 (0.0 ~ 1.0)\n예를들어 \n이런 픽셀이 주어졌을 때 흰 픽셀을 0 검은픽셀을 1로 처리한다. 픽셀 모두를 하나에 행열로 만든다\n픽셀에 검은 부분에 대한 가중치 (얼마나 검은지)를 부여\n한층이 활성화 되는 정도를 열백터로 표현\n모든 층을 모아 행렬백터로 표현, 각 열 한층과 다음층에 특정 뉴런에 대한 연결 표현\n가중치에 따라 활성화 정도를 더한것이 행렬 백터곱을 하여 나오는 열백터의 각 원소에 대응\n\n\n한마디로 정의하면 위 영상처럼 여러개의 데이터가 서로 어떤 규칙에 의해 얼키고 설킨\n\n행렬\nvelog.io/@yuns_u/%EB%B2%A1%ED%84%B0%EC%99%80-%ED%96%89%EB%A0%AC\n\n백터: 1차원 데이터\n행렬: 2차원 데이터\n스칼라: 단일 숫자\n"},"시험-정리/3-1/중간---알고리즘/알고리즘-중간":{"title":"알고리즘 중간","links":[],"tags":[],"content":"선형검색\n보초법\nint i = 0;\narr[arr.length-1] = value;\n\twhile (true) {\n\tif (arr[i] == value) break;\n\ti++;\n}\nreturn i == arr.length ? -1 : i;\n\n배열에 크기를 한칸 더 받음아서 처리\n\n재귀함수\n팩토리얼 계산\nif (n &lt;= 0) {\n\treturn 1;\n}\nreturn n*nPack(n-1);\n최대 공약수\nif (b == 0) {\n\treturn a;\n} else {\n\treturn GCD(b, a % b);\n}\n\n최소 공배수 x*y/gcd(x, y)\n\n하노히 탑\n// no 옮길 원반 수, x 시작기둥 y 목표 기둥\nstatic void move(int no, int x, int y) {\n\tif (no &gt; 1) {\n\t\tmove(no - 1, x, 6 - x - y);\n\t}\n\tif (no &gt; 1) {\n\t\tmove(no - 1, 6 - x - y, y);\n\t}\n}\n\n첫 if는 끝에 6-x-y\n뒤 if는 중앙에 6-x-y\n시작할때 move(원반수, 1, 3)\n\n정렬\n버블 정렬 개선 2\nwhile(k&lt;n-1) {\n\tint last = n-1;\n\tfor(int j=n-1; j&gt;k; j--) {\n\t\tif(a[j-1] &gt; a[j]) {\n\t\t\t...\n\t\t\tlast = j;\n\t\t}\n\t}\n\tk = last;\n}\n\nk 까진 정렬 된것으로 보고 범위를 좁힌다\n\n삽입정렬\nfor(int i=1; i&lt;n; i++) {\n\tint j;\n\tint tmp = a[i];\n\tfor( j=i; j&gt;0 &amp;&amp; a[j-1]&gt;tmp; j--) {\n\t\ta[j] = a[j-1];\n\t}\n\ta[j] = tmp;\n}\n\n2 3 5 4 이렇게 된경우 4를 정렬할 시 해당 4 위치를 5로 바꾸고 원래 5 위치에 삽입 됨\n\n셀 정렬\n int h;\n for (h = 0; h &lt; arr.length; h = h * 3 + 1);\n \nfor (; h &gt; 0; h /= 3) { // 나눗셈\n\tfor (int i = h; i &lt; arr.length; i++) { // 그 나눈것들로 하나씩 올리는 for 문\n\t\tint j, temp = arr[i]; \n\t\tfor (j = i - h; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= h) { // 그 i 값으로 - h 해주면서 \n\t\t\tarr[j + h] = arr[j]; // j+h 에 j 대입\n\t\t}\n\t\tarr[j + h] = temp; // j+h에 temp 대입\n\t}\n}\n\n중앙부는 삽입이랑 비슷\n"},"시험-정리/3-1/중간---인공지능-프로그래밍-중간/인공지능-프로그래밍-중간":{"title":"인공지능 프로그래밍 중간","links":[],"tags":[],"content":"\n인공지능: 컴퓨터 시스템이 인간과 유사한 지능적 작업을 수행할 수 있는 능력을 가지는 지능\n\n머신러닝 학습법\n지도 학습\n\n특정 기준에 의한 데이터들이 필요한 방식\n\n분류\n\n기준(레이블)으로 분류되어 학습한 후에 입력 데이터를 찾아 내는 법\n이진분류: ex) 고양이 이냐 아니냐?\n다중분류: ex) 사자 호랑이 코끼리 중 어디?\n\n회귀\n\n예측값이 실(양적데이터) 일때 특성과 결과 사이의 관계파악 하여 결과를 예측하는법\n\n키와 몸무게의 데이터가 주어졌다면 키로 대강의 몸무게가 예측이 가능할것\n주택 가격 예측 같은것들\n\n\nK-NN (최근접 이웃)\n선형회귀\n다항회귀\n의사결정 트리\n\n비지도 학습\n\n특정 기준에 의한 데이터들이 존재하지 않은방식\n\n군집화\n\n무작위에 데이터가 주어졌을때 그것들을 구분해서 그룹화 하는 것 데이터들에 특성을 고려 집단(클러스터)을 정의하고 대표점을 찾는 방식\n\n예) 한 택배 회사에서 받은 물류들을 분류하는것\n\n\nK-means(평균)\nDBSCAN(밀도기반)\n\n강화 학습\n\n보통 우리가 생각하는 딥러닝\n어떤 게임을 한다고 쳤을 때, 특정 상태 에서 어떤 행동을 취하는 것이 최적인지를 학습하는 것이다.\n\n머신러닝 프로세스\n\n문제파악\n데이터 탐색\n데이터 전처리\n모델 학습\n예측\n\nK-NN 분류 (최근접 이웃)\n\n예측할 데이터가 위치하는 곳에 가까운 이웃에 데이터를 비교하는 방식\n\n특징\n\n학습시 api 는 KNeighborsClassifier\nK값에 따라 이웃 개체를 K개 찾았을 때 어떤 데이터가 더 많은지 비교, 따라서 K값은 홀수로 둬야 함 (동점이 존재하니까)\n유클리드 거리 방식\n\n유클리드: 피타고라스 대각선 거리\n맨하튼거리: 각 좌표에 차를 모두 더한 것\n\n\n구현이 쉽고 훈련이 빠르다\n데이터가 많아지면 분류(예측) 단계에서 시간이 많이 걸리고 메모리가 많이 필요 적절한 K의 선택이 필요\n\n사용법\n\n여기서 fish_data 는 전체 데이터를 의미하고 fish_target은 실제 분류된 결과를 의미\n학습 과정\n\n# 모든 생선의 길이, 무개 값 저장 도미 35마리 빙어 14마리\nfish_data = [[len, weight] for len, weight in zip(length, weight)] \n# 도미를 1로 빙어를 0으로 설정\nfish_target = [1] *35 + [0] *14\n \nfrom sklearn.neighbors import KNeighborsClassifier\n \nkn = KNeighborsClassifier() # K_NN 모델 생성 : 기본으로 생성\n# n_neighbors=3 이걸로 K값 조정\nkn.fit(fish_data, fish_target) # 학습\n# 인자로 데이터, 결과를 넘겨주는 것\nkn.score(fish_data, fish_target) # 성능평가\n \n[새로운 예측]\n# 분류라서 1 또는 0\nans = kn.predict([[30 ,600]])\n \nif ans == 1:\n  print(&quot;도미입니다.&quot;)\nelse:\n  print(&quot;빙어입니다&quot;)\n예측 확률\n\npredict 가 예측 결과를 출력할때 그 예측을 한 확률\n\n\nkn.classes_: 학습 결과로 얻어 정답(타겟값 출력)\nkn.predict_proba(테스트 셈플들):  샘플에 대한 예측 확률 구하기\n\nK-NN 회귀 (최근접 이웃)\n\n이웃들에 평균치로 계산\n\n특징\n\n학습 시 api는 KNeighborsRegressor\n결정계수:\n\n회귀 모델에서 성능평가 지수\n\n\n\n평균 절대값 오차 (MSE)\n\n타겟 에서 에측값 을 뺀값을 절대값으로 바꿔 평균을 낸것\n예측값 과 타겟 이 얼마나 차이나는지 알 수 있다\nmean_absolute_error(예측값, 타겟): 함수로 바로 구할 수 있음\n\n데이터 전처리\n\n거리기반 알고리즘이라 범위 영향을 많이 받음\n따라서 표준점수((값-평균) / 표편) 기준을 잡는게 좋음\nmean(): 평균\nstd(): 표준편차\n\ntrain_scaled =(train_input - mean) / std\n# train_scaled에 있는 모든 값을 표준점수로 바꿔줌\n선형 회귀\n\n특성이 하나인 경우 특성과 타겟간 관계를 잘 나타내는 직선을 예측\n\n특징\n\n학습 시 api는 LinearRegression\n1차 함수 형태이므로 기울기, 절편 값이 필요\n\n기울기: coef_\n절편: intercept_\n\n\n\n# 찾아낸 기울기와 절편으로 1차 방정식 그래프 그리기 : x의 범위는 15에서 50까지\nplt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])\n단순 선형 회귀\n\n데이터와 한 개의 특성의 선형 상관관계를 모델링하는 회귀분석 기법\n데이터 = a*특성 + b 꼴\n\n특징\n\n가장기초, 많이 사용됨\n입력값(특성) 이 하나 인 경우 작동\n키가 몸무게의 영형을 얼마나 주나 뭐 이런거\n이때 키 는 독립변수x, 몸무게 는 종속변수 y\n\n손실함수\n\n실제값과 예측값에 차이를 나타내는 함수 (평균 절대값 오차)\n그냥 오차값이라 생각하자\n\n\n최소 제곱법 :\n오차값의 제곱에 합이 최소가 되는 a,b 를 구하는 방법\n2차함수 형태, 미분하여 기울기가 0이되는 지점을 구하면 됨\n경사 하강법:\n위 a, b 값을 직접 구하는 방식 a, b값을 적절히 조절하여 오차가 최소가 되는 지점을 찾는 법\n\n이게 제일 많이 사용되는데 왜냐면 실제 손실함수는 미분으로 할 수 없는 난잡성을 보이는 경우가 많음 그래서 이부분은 수동으로 해주는 게\n\n\n\n다항 회귀\n\n단순한 1차원 직선으로 관계예측이 일어나기 힘든경우 즉 완전 비례, 반비례 관계가 아닌경우, 에초에 거의 모든 데이터는 완전 비,반비례 관계를 가지기 어렵다.\n\n특징\n\n사실상 다중회귀에 일종\n특성값에 변형을 줘서 다른 특징을 만들어 내는것\n1차 곡선 형태의 그래프\n\n\n𝑦= 𝑎x^2+𝑏𝑥+𝑐 꼴에 식\n\na, b: 가중치, 계수,\nc: 절편\n\n\n차수(차원)수가 늘어나면 𝑦= 𝑎x^n+...+𝑏𝑥+𝑐\n\n\n교제에 나온건 column_stack() 함수를 사용하여 첫 인자에 원본 데이터 제곱\n해서 훈련, predict([[예측제곱값, 예측값]]) 로 예측\n\n무게 = 1.01 * 길이^2 - 21.6*길이+116.05\n\n\n\n다중 회귀\n\n데이터와 여러개의 특성(입력값)에 대한 결과 예측을 위한것\n예) 평균기온, 강수량 → 아이스크림 판매량\n\n특징\n\n특성이 여러개 이므로 2, 3 차원의 형태\n특성이 타겟을 넘지않아야 한다\n\n특성공학 &amp; 규제\nPolynomialFeatures()\n\n각 특성을 제곱한 항과 특성끼리 서로 곱한 항을 추가할 때 사용\n자동으로 다항 특성 만드는 클래스\n\ndegree=2: 차수 (2차원 곡선이면 2, 3차원이면 3)\ninclude_bias=True: 0차항 포함여부 (편향, 절편)\ntransform(): 실제로 데이터에 적용하여 특성추가\nfit(): 생성할 특성 조합 찾기\n\n\nfit() 다음 transform() 이렇게 처리해야 하고 transform 된 데이터를\nLinearRegression() api 에 집어넣으면 됨\n다항, 다중 회귀에 사용\n보통 bias 값은 1일텐데 그 이유는 선형 방정식에 절편값은 1이라 근데 보통 자동으로 추가해줘서 include_bias=False 추천\n\n특성공학\n\n\n머신러닝 모델의 성능을 향상 시키기 위해 데이터의 특성을 만들거나 변형하는 과정\n\n특성선택: 가지고 있는 특성 중에서 훈련에 가장 유용한 특성 선택\n특성추출: 특성을 결합하여 더 유용한 특성을 만듬\n차원축소: 특성의 개수를 줄이는 것\n\n\n\nStandardScaler(표준화):\n각 특성을 평균 0, 분산 1로 스케일링\n\n\nMinMaxScaler(정규화):\n각 특성의 값을 0~1 사이값으로\n\n\nOneHotEncoder(이산화):\n범주형 데이터의 숫자형 변환\n\n\n규제\n\n훈련세트를 너무 과도하게 학습하지 못하도록 제한\nalpha 값을 조정할 때 상용로그 값으로 취함\n그래프 그릴 땐 np.log10() 함수로 로그 값을 구해서\nalpha 값으로 규제 강도를 조정하고 적당한 alpha 값을 찾는것이 중요하다\n선형회귀: 각 항의 계수(가중치, 계수, 기울기)가 너무 커지지 않도록\n분류: 하이퍼 파라미터 조정\n\n\n여기서 계수는 편향, 절편을 의미\n\n릿지(Ridge)\n\n계수를 제곱한 값을 기준으로 규제 적용\n회귀 계수를 작게\n\n사용법\n\nRidge(alpha=1) api 사용\n이외 다른 과정은 다른 ML api 와 동일하다\n\n라쏘(Lasso)\n\n계수의 절대값을 기준으로 규제 적용\n회귀 계수값을 0으로 하거나 적게\n예측영향이 적은 특성에 회귀계수를 0으로 해당 특성을 학습에서 제외\n\n사용법\n\nlasso(alpha=1) api 사용\n이외 다른 과정은 다른 ML api 와 동일하다\ncoef_: 학습 결과로 찾아낸 게수를 저장, 이때 0이면 그 값은 제외된거\n\n표준화\n\nStandardScaler 클래스 사용\n\nss = StandardScaler()\nss.fit(train_poly)\ntrain_scaled = ss.transform(train_poly)\n\n규제 api 사용시에는 반드시 표준화를 거친 데이터값을 사용\nlasso.fit(train_scaled, test_data)\n\n데이터 처리\n성능 하락 요인\n\n과대적합: 훈련 성능 ▲ 테스트 데이터 ▼\n\n일반성 이 떨어진다는 의미\n\n\n과소적합: 훈련 성능 ▼ 테스트 데이터 ▲\n\n모델이 너무 단순해서 제대로 학습이 안됬다는 의미\n\n\n대표성 없는 데이터\n낮은 품질 데이터\n충분하지 않는 데이터\n관련성 없는 특성\n\n데이터 종류\n\n양적 데이터: 숫자 데이터, 얼마나 크고 작은지\n\n면적, 온도, 판매량\n\n\n범주형 데이터: 이름데이터\n\n계절, 날씨\n\n\n\n데이터 전처리\n\n이상치 제거\n결측치 처리\n데이터 변환\n표준화, 정규화, 이산화, 범주화\n\n특성공학\n\n특성 선택: 가지고 있는 특성중 가장 유용한 특성 선택\n특성 추출: 특성을 결합해 더 유용한 특성 만듬\n차원 축소: 특성의 개수 줄이기\n\n용어정리\n\n머신러닝(기계학습):\n여러 개의 입력값과 결과값을 컴퓨터에 제공하기만 하면 이 데이터를 바탕으로 컴퓨터가 스스로 내부 동작을 만들어 내는것\n모델:\n머신러닝 알고리즘을 구현한 프로그램 또는 데이터 간의 상관관계를 수식으로 표현하는 것\n성능평가:\n실제값과 모델에 의해 예측 값을 비교하여 두 값의 차이를 구하는것\n실제값-예측값\n샘플링 편향:\n훈련 데이터 세트와 테스트 세트 간에 편중\n파라미터:\n새로운 샘플에 대한 예측을 하기위해 사용\n\n하이퍼 파라미터:\n모델에 설정하는 변수 학습률, 에보크수(훈련 반복횟수), 가중치 초기화 등을 결정\n모델 파라미터:\n모델 훈련에 결과로 얻어지는 값들\n\n\n손실함수:\n예측값과 실제값에 차이 즉, 오차를 계산하는 함수\n사례기반 학습:\n훈련 샘플을 암기하며 학습\n\n스팸 메일 필터 (KNN)\n\n\n모델기반 학습:\n샘플들에 모델을 만들어 예측해서 사용하는 것\n\nGDP에 따른 더 나은 삶의 지표 변화 예측 (선형 회귀)\n\n\n\n함수정리\n\naxis=1: 행, axis=0: 열\nlen(): 리스트나 numpy 배열 길이 출력\nzip(): 메게변수로 주어진 리스트에서 항목을 각각 꺼내 튜플로\n\n\nNumpy, Pandas 정리\n\n리스트 컴프리션\nex) 리스트 원소에 2배를 곱하는\n\nx2 = [ n*2 for n in x ]\nx2\n# 연산 for 변수 in 참조값 \n\nnumpy 배열은 백터연산이 되서 해당 코드로도 위 결과와 같은 효과를 냄\n\n xa * 2\n\n브로드 케스팅 (차수 달라도 연산되게)\n\na =  array([[1, 2], [3, 4], [5, 6]])\nb = array([10, 20])\na * b = array([[10, 40], [30, 80], [ 50, 120]])\nSklearn\n\nfit(데이터, 타겟): 학습\nscore(데이터, 타겟): 성능평가\npredict([ [ 값 ] ]): 예측하기\ntrain_test_split(데이터, 타겟,  train_test_split=): 훈련데이터 세트와 테스트 데이터 세트 나눠주기\n\nrandom_state=: 랜덤시드\nstratify=데이터: 해당 데이터와 비율을 맞춤\n\n\nkneighbors([[ 값 ]]): distances, indices  : 최근접 이웃 항목 반환\n\ndistances: 이웃과에 거리데이터\nindices: 최근접 이웃들의 인덱스\n\n\ntrain_test_split(): 훈련데이터와 테스트 데이터 세트로 나누어줌\n\ntrain_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)\n#random_state 랜덤 시드 설정\nNumpy\n\nastype(): 타입 변환\ncolumn_stack(([1,2,3], [4,5,6])):\n\n[이런식으로 반환]\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n\nrandom.shuffle(index): 랜덤값 배열 생성 (주로 샘플링 편향 해소에 사용)\nisin([&#039;칼럼&#039;]) -&gt; [True | False]: 해당 칼럼만 선택\n\nDataframe\n\ntail() 끝 값 5개 출력\niloc[행, 열]: 특정행렬만\nloc[행값 , [&quot;행키&quot;]]: 행키를 가진 행값열 까지\n"},"시험-정리/3-2/기말---기계학습프로그래밍/기계학습프로그래밍-기말":{"title":"기계학습프로그래밍 기말","links":[],"tags":[],"content":"최적 가치함수\nSARSA\n\n일반화된 정책반복과 시간차 학습이 접목된 알고리즘\nS: 상태, A: 행동, R: 보상, S+1: 다음 상태, A+1: 다음 행동\n\n\n일반화된 정책 반복:\n\n정책 평가를 할때 정책향상을 위해 굳이 평가를 끝까지 해보지 않고 적정양의 정책평가만으로도 정책향상을 이룰 수 있다는 것\n\n\n\n설명\n\nQ 함수, 행동가치(Q) 함수를 통해 학습, 즉 상태가치(V)를 사용하는게 아님\n현재 상태(S)에서 행동(A)을 취한 후 보상(R)과 다음 상태(S+1)를 받고\n에이전트가 다음 행동(A+1)을 무엇을 취할 것 인지를 알아야 한다.\n다음 행동(A+1) 는 다음 상태(S+1)를 안다면 무엇인지 알 수 있다\n이때 다음 상태를 선택하는 정책으로 𝜀-greedy 를 사용한다\n\n𝜀-greedy (엡실론 그레디)\n\ngreedy: 기본적으로 가장 좋은것을 고르겠다는 의미\n\n\n\ngreedy:\n\n학습 초반에 행동가치 함수는 (Q) 는 보통 균등 분포될것\n이 과정에서 우연하게 위와 같은 코스를 반복해서 간다면 현재 시작지점에서의 최대 행동가치는 위 사진처럼 아래로 한번 갔다가 가는 것일것이다\n원래라면 오른쪽으로 이동 하는 게 가장 최적인데\n\n\n𝜀(엡실론):\n\n𝜀은 𝜀의 확률만큼 랜덤하게 움직이게 한다는 것이다\n이때 랜덤하게 움직이는것을 탐험\n\n\n즉 𝜀-greedy는 greedy 하게 움직이되 𝜀 확률만큼 랜덤하게 움직이는 정책\n\nQ-Learning\n\nSARSA 와 유사한 시간차 기반 예측 알고리즘\n\n\n기반은 SARSA\n현재 상태(S)에서는 𝜀-greedy 정책을 통해 행동을 선택하고, 다음 상태(S+1)에서는 가장 큰 행동 가치 함수(Q)를 사용\n\nExpected SARSA\n\n행동 가치 추정 시 분산이 높다는 SARSA 방식의 단점을 보완함\n\n\nAgent 는 정책에 따라 행동하므로 다음에 취할 행동들 확률이 얼마인지 미리 알지 못함\nExpected SARSA는 다음 상태에 모든 행동들을 고려한다\n즉 SARSA는 다음 행동들중 하나만 샘플링 하지만, Expected SARSA는 모든 행동에 대해 샘플링 한다\n\nOn-policy &amp; Off-policy\n\nOn-policy:\n\nSARSA 처럼 행동하는 정책과 학습할 때 사용하는 정책이 서로 같은 경우\n\n\nOff-policy\n\nQ-Learning 처럼 행동하는 정책과 학습할 때 사용하는 정책이 서로 다른 경우\n\n\n\nOn-policy (SARSA)의 문제점\n\n\n사진 처음 오른쪽으로 이동한 뒤 다음 행동으로 아래를 선택할 경우 당연히 안좋은 선택이므로 마이너스 점수 부여\n그리고 다시 처음으로 와서 이번엔 바로 아래로 이동하는 행동을 취한 경우 이것도 안좋은 행동이므로 마이너스 점수 부여\n그럼 최악에 경우에는 빠져 나오질 못하고 갇히는 현상이 발생함\n\nOff-policy (Q-Learning)의 장점\n\n위에 상황이 Q-Learning 에서는 해당이 안된다\n다음 행동을 결정할 때 가장 큰 행동 가치 함수(Q) 를 사용하기 때문에 잘못된 선택을 할 가능성이 없다\nSARSA 문제는 𝜀-greedy에 의해 초반에 상태가치는 균등 분포 되기 때문에 아주 잘못된 선택이 가치가 살짝 더 높게 나와 해당 행동을 취할 가능성이 생겨 버리기 때문이다.\n반면 Q-Learning은 다음 선택을 재대로 하는 경우로 학습이 이루어진다. 또한 가장 큰 행동 가치 함수(Q) 가 계속 발전하기에 추후에 행동이 재평가 받기도 한다\n\nOn-policy (SARSA) 를 사용할 때\n\n현재 정책 기준으로 학습을 진행하여 정책이 일관성 있게 유지된다는 장점이 존재\n이로 인해, 매우 불안정 하거나 예측하기 힘든경우 Off-policy 보다 유리함\n예를들어 예측할 수 없는 현실에 문제를 다룬다거나\n\n종합\n\n\n여기서 파란색=SARSA, 빨간색=Q-Learning\n\n\nSARSA &amp; Expected SARSA 는 좀 더 안전한 경로로 갈려는 경향이 있고\nQ-Learning 은 최적적인 경로로 갈려는 경향이 있다.\n\n모델 기반 학습\n모델\n\n분포 모델:\n\n모든 발생 가능한 상황에 대한 확률분포를 정확히 기술\n\n\n샘플 모델:\n\n모든 발생 가능한 상황에 대해 임의의 가능성만 샘플링 됨\n\n\n\n계획 &amp; 학습\n계획\n\n주어진 목적을 달성하기 위해 수행해야 할 일에 대한 방법 마련\n\n\n동적 프로그래밍, 모델 기반 강화학습이 이에 해당\n이것을 Model-Based Algorithm 이라고 함\n모델 기반 강화학습은 Agent 가 행동을 결정할 때 환경모델을 통한 결정이 이루어짐\n즉 계획은 정확히 알려진 환경정보를 바탕으로 가능한 모든 행동을 고려하게 됨\n\n모델이 어떻게 사용됨?\n\nAgent 가 환경과 상호작용이 일어난 경우\n환경은 모델에서 정보를 받고 정책을 업데이트 하는 방식으로 이뤄짐\n상태 공간 계획:\n\n상태 공간을 탐색할 때 방문하는 각 상태에 대한 가치 함수 추정이 이루어짐\n가치 함수를 계산하기 위해 시뮬레이션에 의한 경험 데이터 기반으로\n역갱신(backup) 과정 수행\n\n\n\n학습\n\n학습은 계획과 반대로 agent 가 직접 경험을 통해 최적의 행동을 직접 배움\n\n\nSARSA, Q-Learning 등이 이에 해당\n이것을 Model-Free Algorithm 이라고 함\n\n온라인 계획\n\n임의의 환경이 주어진 경우 아래 두 가지 과정을 동시에 지속적으로 수행\n\n환경을 모방하는 모델 자체에 대한 학습\n계획을 통한 가치 및 정책 학습\n\n\n환경 상호작용 → 모델 변화 → 가치/정책 업데이트\n\nDyna-Q 알고리즘\n\n온라인 계획 및 강화학습이 동시에 수행되는 대표적 알고리즘\n학습 속도를 높히고 효율적으로 환경을 탐색하기 위한\n\n\n\nModel-Based Algorithm &amp; Model-Free Algorithm의 접근법을 결합해서\n실제 환경에서 얻은 데이터를 활용해 행동 가치 함수(Q) 를 업데이트 하고\n환경 모델을 활용해 가상의 경험을 생성하고 학습에 활용함\n배치 학습 하는것과 비슷한 점이 있다\n\n직접 강화학습 &amp; 간접 강화 학습\n\n간접 (Q-Planning):\n\n환경과 상호작용에 의한 경험 정보가 적더라도 그런 정보를 반복적으로 사용\n모델 학습이 충분히 되지 않는다면 편향(bias)가 있을 수 있음\n\n\n직접 (Q-Learning):\n\n환경과 직접 상호작용 하며 얻는 경험을 곧바로 활용해 정책에 반영하여 편향(bias)를 줄일 수 있음\n\n\n\n초기 탐색 방법\n\n탐험적 시작\n행위 정책으로 𝜀-greedy 사용\n\n장점\n\n더 적은 에피소드로 경험이 높은 학습성과 효율을 보임\n\n갱신 &amp; 샘플링\n기대 갱신 &amp; 샘플 갱신\n\n\n기대 갱신:\n\n발생 할 수 있는 모든 가능한 상황을 고려하는 가치 갱신\n\n\n샘플 갱신:\n\n발생 할 수 있는 간단한 샘플만 활용한 가치갱신\n\n\n\n샘플링\n\n궤적 샘플링:\n\n환경 또는 환경 모델과 지속적으로 상호작용 하면서 상태와 보상을 얻어오는것\n\n\n균등분포 샘플링:\n\n샘플링 대상 후보들을 어떤 편중 없이 임의로 선택\n\n\nOn-policy 분포 샘플링:\n\n현재 주어진 정책 기반으로 각 상태에서 행동 선택\n효과:\n\n실제 일어날 법한 상황에 좀 더 집중\n\n\n\n\n\n정리\n\n\n대부분의 강화학습은 환경 또는 환경모델로 부터 얻는 궤적에 따라 가치들을 역 갱신\n대부분의 강화학습 방법은 정책 반복에 일반화 전략을 따름\n\n중간 고사 내용\n\n몬테카를로: 타입 스텝 사이에서는 정책 개선 불가, 부트스트랩 사용안함\n\n 모든 방문 MC 예측\n\n매번 방문할 때 마다 모든 보상의 합 평균\n\n\n첫 방문 MC 예측\n\n동일 에피소드 내 처음 방문했을 때에만 그 이후 모든 보상들의 합 평균\n\n\n탐험적 시작\n\n위 두 방법에서 방문하지 못한 에피소드 환경이 발생 하는데 이걸 극복하기위해\n초기 상태를 임의로 시작\n\n\n\n\n시간차 학습 (TD): 에피소드 완료를 기다리지 않고 타임 스텝별로 모델 갱신, 부트스트랩 사용\n역 갱신 다이어그램:\n\n종료 상태를 만나면 해당 종료 상태로부터 역 방향으로 생성 되면서 얻은 여러 누적 보상을 평균하여 상태(s) 의 가치를 갱신\n\n\n"},"시험-정리/3-2/기말---시스템서버운영/시스템서버운영-기말":{"title":"시스템서버운영 기말","links":[],"tags":[],"content":"\n컴파일러:\n\n자연어로 구성된 고급언어를 컴퓨터가 이해할 수 있도록 기계어로 번역해주는 소프트웨어\n\n\n디버깅:\n\n모든 소프트웨어에서 소스 코드의 오류를 찾아서 수정하는과정\n\n\n셸:\n\n사용자-커널 사이 중계자 역할을 하는 프로그램 (본, C, 콘, 배시)\n명령어 해석기, 프로그래밍, 사용자 환경설정\n\n\n시스템 프로그래밍:\n\n컴퓨터 시스템의 운영 체제(OS)나 하드웨어와 밀접하게 연관된 저수준(low-level)의 프로그래밍\n\n\nfork:\n\n부모 프로세스 에서 프로세스를 복제하여 자식 프로세스를 생성\n\n\n아카이브: 특정 시간에 함께 묶인 파일과 디랙토리 집합\n라이브러리: 유용한 기능을 가진 프로그램에 모듈을 모아놓은 파일\nmake: 모듈로 구성된 대규모 소스를 자동화하여 관리하도록 하는 도구\n\n프로세스\n\n프로그램: 저장장치에 저장된 정적인 코드\n프로세스: 프로그램을 실행하여 메모리에 올라온 것\n부모 프로세스는 PPID, 자식 프로세스는 PID\n고아 프로세스:\n\n자식 프로세스가 종료되지 않은 상태에서 부모 프로세스가 죽어버린경우\n\n\n좀비 프로세스:\n\n부모 프로세스가 자식 프로세스의 종료를 기다린 뒤에 종료를 처리 해 주는 과정을 거치지 않고 부모 프로세스를 종료해 버린 경우\n\n\n백그라운드: 실행되었지만 눈으로 보이지 않는\n작업 제어: 포그라운드 ←&gt; 백그라운드 전환하는 제어 (작업전환, 일시중지, 종료)\n작업 전환: 단순히 포그라운드 ←&gt; 백그라운드 전환을 의미\n\n링킹\n\n정적 링킹:\n\n최종 실행파일에 필요한 오브젝트 파일들을 미리 링크하여 실행 파일에 함께 포함\n실행파일만 있으면 별도 파일 없이 실행 가능\n\n\n동적 링킹:\n\n필요한 파일들을 미리 링크하지 않고, 실행하려고 할때 필요한 프로그램 모듈들을 결합하여 계속 사용\n실행파일 크기가 작어져 메모리를 조금 차지\n그러나 별도 필요 라이브러리가 제공되야함\n\n\n\n라이브러리\n\n유용한 기능을 가진 프로그램에 모듈을 모아놓은 파일\n\n\n정적 라이브러리 (.a):\n\n프로그램 빌드 시에 라이브러리가 제공하는 코드를 실행 파일에 넣어서 빌드하는 방식\n\n\n동적 라이브러리:\n\n실행도중에 동적으로 로딩\n필요한 라이브러리가 수시로 등록, 실행 제거가 가능\n\n\n공유 라이브러리 (.so):\n\n어떤 라이브러리가 제공하는 기능을 다른 애플리케이션에서 사용하고 싶을 때 라이브러리 코드를 메모리에 하나만 두고 각 애플리케이션에서 이를 공유하는 방식\n\n\n\nGCC\n\ngcc &lt;옵션&gt; &lt;대상파일.c&gt;\n\n-o: 출력파일이름 지정\n-g: gdb 디버깅 용으로 컴파일\n-L&lt;라이브러리&gt;: 라이브러리가 있는 경로\n-l&lt;라이브러리&gt;: 라이브러리 이름 (맨 뒤에 인자를 줘야)\n\n라이브러리 이름은 libm.a 또는 lib.so 파일에서 lib 접두사를 땐 m\n\n\n-I&lt;해더파일 경로&gt;: 해더파일 탐색 경로\n-rdynamic: 동적 라이브러리 경로 심볼정보 포함\n\n\nldd &lt;실행파일&gt;: 라이브러리 링크 보는법\n\n단순 파일 연결\n\n단순히 여러파일을 작성한경우\n\n\n이때는 무조건 출력파일을 지정해야 함\n\ngcc -o &lt;출력파일명&gt; &lt;대상파일.c | 대상파일.o...&gt;\n\n\n\n정적 라이브러리 (.a)\n\n생성:\n\ngcc -c &lt;대상파일.c&gt; → 오브젝트 파일 또는 목적파일(.o)\n\n링킹 과정이 되지않은 원시 기계어 코드\n\n\nar rcs &lt;라이브러리파일.a&gt; &lt;.o파일&gt; → 정적 라이브러리 파일(.a)\n\n\n링킹:\n\ngcc -L&lt;라이브러리&gt; &lt;대상파일.c&gt; -I&lt;라이브러리 이름&gt;\n\n\n\n공유 라이브러리 (.so)\n\n생성:\n\ngcc -shared -o &lt;생성할 공유 라이브러리 파일.so&gt; &lt;대상파일.c&gt; → 공유 라이브러리 파일(.so)\n\n\n링킹:\n\ngcc -L&lt;라이브러리&gt; &lt;대상파일.c&gt; -I&lt;라이브러리 이름&gt;\n\n\n\n경로 문제\n\n동적 라이브러리 는 시스템에 의해 로드되므로 시스템 경로에 없다면 오류가 발생함\n\n\n\n환경변수 지정\nexport LD_LIBRARY_PATH=라이브러리 경로\n\n\n동적 라이브러리\n\n생성:\n\ndlfcn.h include\ndlopen() 으로 라이브러리 로드\n\n\n링킹:\n\n링크시 libdl.so 와 링크\n\ngcc -rdynamic &lt;대상파일.c&gt; -Idl\n\n\n\n\n\nmake\n\n모듈로 구성된 대규모 소스를 자동화하여 관리하도록 하는 도구\n\nMakefile 예제\n# main.o -&gt; search.o -&gt; update.o 이 순으로 실행함\n \n# 대상 : 의존 파일\n\t# 명령\nsample : main.o search.o update.o\n\t# 실제 수행 작업 (커멘드)\n\tgcc -o sample main.o search.o update.o\n \nmain.o : main.c\n\tgcc -c main.c\n \nsearch.o : search.c\n\tgcc -c search.c\n \nupdate.o : update.c\n\tgcc -c update.c\n \n# 해당 명령 접근은 make clean\nclean :\n\trm -rf sample main.o search.o update.o\n\n\n변수(메크로) 대상으로 task 자동 생성\n\n$@: 있는 ${TARGET} 즉 대상\n$^: ${OBJ} 즉 정의된 의존파일 전체를 나열함\n$?:  ${OBJ} 즉 정의된 의존파일중 변경된것들 다열함\n$&lt;: ${OBJ} 즉 정의된 의존파일중 가장 첫번째 꺼\n\n# 변수\nCC=gcc\nTARGET=sample\nOBJ=main.o search.o update.o \n \nall : ${TARGET}\n \n${TARGET} : ${OBJ}\n\t# $@=위에 있는 ${TARGET} 즉 task 키 값을 의미\n\t# $^=위에 있는 ${OBJ} 즉 정의된 필수 작업\n\t${CC} -o $@ $^\n \n# 암시적으로 .c 파일을 .o 오브젝트 파일로 컴파일 하는거 정의\n.c.o:\n\t# $@=여기서는 모든 .o 파일\n\t# $&lt;=현재 진행중인 target의 의존파일\n\t$(CC) -c -o $@ $&lt;\n \nclean :\n\trm -rf ${sample} ${OBJ}\n\n\n아카이브\n\n아카이브: 특정 시간에 함께 묶인 파일과 디랙토리 집합\ntar, cpio, ar 명령 등 이존재\n\ntar\n\n\n폴더 tar로\ntar cvf &lt;파일&gt;.tar &lt;묶을 폴더&gt;\n\n\ntar 파일 내용 출력\ntar tvf &lt;대상 파일&gt;\n\n\ntar 풀기\ntar xvf &lt;대상 파일&gt;\n\n\ntar에 파일 수정하여 추가\n즉 tar 파일 내부에 있는 파일을 업데이트 하는\ntar uvf &lt;대상파일&gt;.tar &lt;추가할 파일&gt;\n\n\ntar에 파일 추가\ntar rvf &lt;대상파일&gt;.tar &lt;추가할 파일&gt;\n\n\n압축\n\ngzip &lt;저장할 파일명.tar&gt; &lt;폴더 | 파일&gt;\ngunzip &lt;압축된파일.gz&gt; | gzip -d &lt;압축된파일명.gz&gt;\nzip -r &lt;압축파일명&gt;.zip &lt;폴더&gt;\nunzip &lt;압축파일&gt;.zip\n\n셸 스크립트\n\n장점: 다른 프로그래밍 언어보다 훨씬 빠르게 처리\n단점: 복잡한 스크립트 들은 셸 스크립트 언어의 자체적인 제한 영역에서 실행 할 수 있다.\n\n셸 변수 설정\n\n띄어쓰기 허용안함\n\n#!/bin/bash\n키=값\n환경 변수 설정\n\n환경 전체에서 사용 가능한 변수\n\n#!/bin/bash\nexport 키=값\n \n# 다시 일반 쉘 변수로 바꾸기\nexport -n 키=값\n \n# 환경변수, 쉘변수 모두 제거\nunset 키\n출력 형식을 지정하여 문자열 출력\n#!/bin/bash\nprintf &quot;%d+%d=%d&quot; 10 20 30\n연산 예제\necho &quot;&gt;&gt; first: &quot;\n# 표준입력: 변수 a\nread a\n \n===연산은 무조건 expr 붙여야함===\nadd=`expr $a + $b`\n===곱하기는 \\n 이 무조건 필요함===\ngob=`expr $a \\* $b`\n패키지 설치\n\n(Advanced Package Tool)\n\n\napt-get:\n\nremove &lt;패키지 이름&gt;: 패키지 삭제\npurge &lt;패키지 이름&gt;: 설정 파일까지 모두삭제\nsource &lt;패키지 이름&gt;: 소스 코드 다운로드\n\n\napt-cache:\n\nstate: 시스템에 설치되어있는 전체 패키지 목록 보기\nsearch &lt;패키지 이름&gt;: 패키지 검색\npkgname: 설치가능한 모든 패키지 검색\ndump: 설치된 모든 패키지 업데이트 (사실은 로컬캐시 데이터 출력)\n\n\ndpkg\n\n-L &lt;패키지 이름&gt;: 패키지 종속성 확인\n-i &lt;.deb 파일&gt;: deb 패키지 설치\n-I &lt;.deb 파일&gt;: deb 파일 정보 확인\n-l: 모든 패키지 확인\n-r &lt;패키지 이름&gt;: 패키지 삭제\n\n\naptitude: 패키지 관리를 쉽게 할 수 있도록 자동화 기능 제공, sudo 필요\n\nsearch &lt;패키지 키워드&gt;: 패키지 검색\nupdate: 패키지 정보 업데이트\nupgrade: 패키지 업데이트\ninstall &lt;패키지 이름&gt;: 패키지 설치\n\n\n\n파일 의미\n\n\n/etc/passwd\nmm1:x:1000:1000:,,,:/home/mm1:/bin/bash\n\n이름:암호(x):UID:GID:,,,(GUD 폰번호등):홈 디렉토리:쉘\n\n\n\n/etc/shadow (sudo 필요):\n\n실제 암호가 저장되는 곳\n\n\n\n/etc/apt/sources.list (sudo 필요):\n\napt 저장소\n\n\n\n명령어 정리\n\nadduser:\n\n:, 엔터 제외하고 다 됨\n-u &lt;UID&gt;\n-g &lt;GID&gt;\n-d &lt;디렉토리&gt;: 디랙토리 지정\n-p &lt;패스워드&gt;\n\n\nusermod &lt;옵션&gt; &lt;유저명&gt;: 계정 정보 수정\n\n-l &lt;바꿀이름&gt;: 계정 명 변경\n-d &lt;디렉토리&gt;: 홈 디렉토리 이동\n-g &lt;그룹명&gt;, -G &lt;그룹명&gt;: 메인그룹, 서브 그룹\n\n\nfind -user UID -exec rm -r {} ;: uid로 사용자 삭제\nid: 사용자들 gid 확인\ngroupadd &lt;그룹명&gt;: 그룹 생성\ngroupdel &lt;그룹명&gt;: 그룹 삭제\nps:\n\n-f: 상세 정보 출력\n-e: 모든 프로세스 정보 출력\n\n\npgrep -l -n &lt;프로그램&gt;: 패턴으로 특정 프로세스 정보 출력\nkill -9 &lt;pid | %작업번호&gt;\n\n-CONT: 실행\n-STOP: 정지\n\n\npkil -9 &lt;프로세스 명령 이름&gt;\nstop &lt;작업번호&gt; 또는 (ctrl+z)\nenv: 환경변수 보기\nsudo su - &lt;사용자&gt;\n\n계정 변경시 해당 사용자의 환경변수를 사용\n- 안붙이면 기존 사용자의 환경변수 사용\n\n\nman &lt;숫자&gt; &lt;명령어&gt;\n\n메뉴얼에 페이지 전환 (1=설명, 2=시스템 구조 (코드), 3=라이브러리)\n\n\ntail -&lt;숫자&gt; &lt;파일&gt;\n"},"시험-정리/3-2/기말---예술과인성/예술과-인성-기말-정리":{"title":"예술과 인성 기말 정리","links":[],"tags":[],"content":"\n사람들이 귀를 닫고 입을 열기를 좋아하는 이유: 주인공\n시빌리우스가 남긴 말: 음악은 언어를 뛰어넘은 영역의 것\n사피어-워프 가설: 언어가 사고를 지배한다\n리브레또 관계 없는것: 연주자의 인지적 반응을 중심으로 쓰였다.\n일반인 오페라 듣는게 힘든 이유 아닌: 사람의 인지가 할 일이 없어서\n예술작품 완성: 관객\n눈만 뜨고 있으면 볼 수 있는 영화: 인생에 대하여 깊히\n자동차와 같은 고가 제품: 시청자 스스로 결론을 내리게 만든다\n\nOP Number\n\n작곡가의 곡의 붙인 작곡 번호\n\n\n모짜르트 (퀘헬 번호): K, KV\n바흐: BVW\n헨델: HVW\n하이든: HOP\n슈베르트: D\n비발디: RV\n\n오페라 제목\n\n서곡: 투우사\n하바네라: 카르멘 돈 호세 추파\n세기디야: 군부대\n집시의 노래: 카르멘 선술집\n투우사의 노래: 투우사 용맹\n꽃의 노래 옥살이 카르멘\n\n의사 소통\n연극\n\n극작가는 자신의 지식을 바탕으로 부호화 하여 연극 대본을 작성\n연출가는 자신의 기반지식을 바탕으로 연극을 해독\n연출가는 자신이 해독한 정보를 바탕으로 배우들에게 전달하며 연극을 진행\n연기는 곧 관객에게 보내는 메시지이며 관객은 수용자\n그럼 메시지를 생성한 극작가, 배우/연출가에게 피드백이 들어온다\n\n미술 (회화)\n\n작가는 자신의 정보를 바탕으로 부호화 하여 그림을 그림\n이때 그림은 메시지 이며 평론가/감상자는 수용자\n그럼 메시지를 생성한 작가에게 피드백이 들어온다\n"},"시험-정리/3-2/기말---컴퓨터보안개론/연습-문제":{"title":"연습 문제","links":[],"tags":[],"content":"7장\n\n전치법은 단순히 문자에 위치를 바꾸는거 대체법은 아예 문자를 바꾸는거\n2\n3\nTO NK WO SI ON HP NI\n3 (안나올듯)\n3\n3\n2\n2\n3\n2\n4\n18\n두 개의 큰 소수 p와 q를 선택하여 그 곱 n=p*q 를 계산하고, 이를 바탕으로 공개키와 개인키를 생성\n3\n송신자가 특정 메시지를 보냈다는 사실을 부인할 수 없도록 보장하는 보안기능\n3\n\n8장\n\n3\n1\n3\n1\n서명자가 해당 전자 문서에 서명하였음을 나타내기 위한 기술\n1-ㄴ, 2-ㄱ, 3-ㅁ, 4-ㄷ, 5-ㄹ\n4\n전자봉투, 전자서명\n1\n1\n정적, 동적\n2\n블록체인\n3\n1\n\n9장\n\n2\n4\n1\n3\n한번에 인증으로 서비스에 모든 권한을 얻는다는 것\n2, 3\n4\n4\n2\n1\n3\n2\n3\n4\n2\n1\n4\n3\n\n10장\n\n1\n2\n퍼셉트론\n3, 4\n\n\n정상: 0\n스팸: 1450/1\n\n\n1\n3\n\n11장\n\n2\n사전 대응 → 침해 사고 대응 → 사고 탐지 → 대응 → 제거 및 복구 → 후속조치\n1\n2\n전문 증거\n2\n4\n안나올듯\nBest Evidence Rule\n우리가 실제로 파일을 삭제했더라도 하드디스크는 실제 데이터를 삭제하는것이 아님\n\n12장\n\n4\n2\n3\n4\n1\n3\n\n13장\n\n1-ㄱ, 2-ㄹ, 3-ㅁ, 4-ㄴ, 5-ㄷ\n1\n해볼 것\n기업에 크기, 시스템 환경, it 예산\n4\n해볼 것\n3\n2\n해볼 것\n4\n해볼것\n해볼 것\n"},"시험-정리/3-2/기말---컴퓨터보안개론/컴퓨터보안개론-기말-PDF":{"title":"컴퓨터보안개론 기말 PDF","links":[],"tags":[],"content":"\n존 매카시: AI라는 용어를 처음 사용한 사람\nDES 알고리즘 암호화: 64비트의 블록 암호화, 56 비트 암호화 키\n모노 알파베틱 암호화: 26!\nPAA: 정책 승인 기관 (최상위 기관)\n커베로스: 클라이언트, 서버, SSO\n금융 보안에 주요기술: 전자서명, 전자봉투\n\n암호화\n\n최초의 암호: 기원전 480년 데마라토스\n암호화 방식:\n\n전치법: 단순히 메시지에 들어가있는 문자 위치 바꿈\n대체법: 글자를 다른 글자로 대체하여 암호화 함\n\n\n\n단일 치환 암호화\n\n키워드를 몰라도 복호화가 가능함\n빈도 분석법으로 복호화가 가능\n\n\n시저 암호화: 알파벳 3자를 오른쪽으로\n모노 알파베틱 암호화: 알파벳 과 암호키를 대칭시킴\n\n다중 치환 암호화\n\n암호화 키와 매핑에 따라 알파벳 하나가 여러 가지 다른 알파벳으로 대체되는\n\n비즈네르 암호화\n\n비즈네르 표를 참고하여 암호화 진행함\n\n\n평문에 글자를 비즈네르 표에 가로\n암호키에 글자를 비즈네르 표에 세로\n만약 암호 키가 평문보다 작은경우 처음부터 쓰면됨\n\nsecret is beautiful → secret is beautiful secret is...\n\n\n복호화는 반대로 일반 평문을 세로, 암호키를 가로\n\n플레이페어 암호화\n\n만약 두 문자가 같은 열(세로)에 위치 한다면:\n\n각 문자에 대해 아래 값으로 대체: AK → UO\n복호화에 경우 위 값으로\n\n\n만약 두 문자가 같은 행(가로)에 위치한다면:\n\n각 문자에 대해 오른쪽 값으로 대체 RD → OB\n복호화에 경우 왼쪽 값으로\n\n\n\nDES 알고리즘 암호화\n\n0, 1 의 이진 데이터를 암호화 하는 알고리즘\n\n\n\nR1을 S-Box 취한 뒤 L1 하고 XOR(둘이 다르면 1) 연산을 한게 L2\nR1 그대로 R2\nR2=L3, L2=R3\n\nS-Box 방법\nR1: 1011 1100 0111\nL1: 1010 0110 1101\n\n\nR1 확장\n\n\n양옆에 이웃 이랑 구분\n# 양옆이웃 원본\n11 1011 | 10 1100 | 01 0111\n\n\n\n\nS-Box 표에서 참고\n\n\n가로: 원본\n\n\n세로: 이웃\n14 | 3 | 1 = 0100 0110 1100\n\n\n\n\n이제 L1 이랑 XOR 연산하면 그게 L2 임\n\n비대칭 암호화\nRSA 암호화\n\n암호화, 복호화에 사용되는 키가 서로 같지 않음\n\n\n평문 → 1번 개인키로 암호화 → 1번 공개키로 복호화 → 평문\n\n장점\n\n자신에 개인키로 암호화된 평문은 자신의 공개키로 밖에 열 수 없음\n\n해시\n8e1463fa1cba3c0713b7777d4f3b9961\n\n\n하나의 문자열을 더 짧은 길이의 값이나 키로 변환한 것\n정보 위, 변조를 확인하기 위한 수단\n대표적인 알고리즘: MD5, SHA\n\n특징\n\n어떤 문자를 넣어도 문자의 길이는 같다\n문자 하나만 달라저도 전혀다른 해시값이 나온다\n사실상 복호화가 거의 불가하다\n\n기타 암호화 알고리즘\n\n대칭\n\nAES 알고리즘: DES 알고리즘을 새롭게 개발한 것\nSEED 알고리즘: 한국 전자상거래 등에서 쓰기위해 만든 국내 암호화 알고리즘\nARIA 일고리즘: 전자정부 구현\nIDEA, RC5, Skipjack, LEA\n\n\n비대칭\n\nLFSR\n\n\n\n전자상거래 보안\n공개키 인증 기관 (CA)\n\n==PAA: 정책 승인 기관 (최상위 기관)==\nPCA: 정책 인증 기관, Root CA 인증서 발급 및 기본 정책 수립\nCA: 인증서 발급과 취소 등의 실질적인 업무 담당\nRA: 등록기관 공인인증서 인증 요청을 확인하고 CA 간 인터페이스 제공\n\n전자 서명\n\n서명자가 해당 전자 문서에 서명하였음을 나타내기 위한 기술\n\n\n공개키 방법을 사용하여 복호화된 해시 값이 편지에서 구한 해시값과 일치하면 위조가 아니라고 판단\n\n기능\n\n위조 불가, 인증, 재사용 불가, 변경 불가, 부인방지\n\n전자 봉투\n\n전달하려는 메시지를 암호화하여 한 사람을 통해 보내고 암호화 키는 다른 사람이 가져가도록 암호학적 으로 구현\n\n\n기밀성, 무결성, 부인방지\n\nSET\n\n비자, 마스터카드의 표준 프로토콜\n\n\n이중 서명: 신용 카드 구매정보, 지불정보 해시값을 합쳐서 또 다른 해시값을 생성 하여 그걸 개인키로 암호화 함\n\n네트워크 암호화\n\n2계층\n\nPPTP: MS\nL2TP: 시스코\n\n\n3계층:\n\nIPSec\n\n\n4계층:\n\nSSL\n\n\n\n기타 암호화\n\nPGP: 그물망, 전자 우편 암호화 방식\nS/MIME: 인증서를 통해 암호화한 이메일 서비스를 제공\n스테가노그래피: 미리 정한 약속에 따라 특정 데이터와 전혀 연관없는 데이터로\n\n보안 시스템\nSSO\n\n시스템이 몇대라도 한 시스템에서 인증 받으면 다른 시스템에 모든 접근 권한을 받는\n\n\n클라이언트가 불특정 자사 서비스에 접근하면 SSO 인증서버에 먼저 요청해서 인증을 받고\n접속하려는 서비스에 접근 시켜줌\n\n방화벽\n\n신뢰하지 않는 외부 네트워크와 내부 네트워크에 사이를 지나는 패킷에 미리 정한 규칙에 따라 허용/비허용을 결정 하는\n\n접근 제어\n\n관리자가 통과시킬 접근과 거부 접근 선택\nIP, 포트 번호로 구성\n\n룰셋 이상현상\n\n음영 이상:\n\n순위가 높은 정책이 순위가 낮은 정책을 포함하고 처리 결과가 다른 경우\n\n\n연관이상:\n\n두 개의 정책이 각기 다른 조건에 대해 서로 포함하는 경우\n\n\n일반화 이상:\n\n순위가 낮은 정책이 순위가 높은 정책을 포함하고 처리 결과가 다른 경우\n\n\n중복 이상:\n\n순위에 관계없이 두 개의 정책 중 하나가 어느 다른 하나의 정책에 포함되고 처리 결과가 같은 경우\n\n\n마침조 이상:\n\n어떠한 패킷도 방화벽 정책에 필터링 되지 않은 경우\n\n\n\n한계\n\n바이러스에 경우 파일 감염 방식이라 효과가 미비 하고\n웜에 경우 정상적 서비스 포트에 대한 공격은 막기 힘듬\n\n침입 탐지 시스템\n\n내부에 해킹이나 악성 코드 활동 탐지와 같이 방화벽이 하지 못하는 일 수행\n\n종류\n호스트 기반 침입 탐지 시스템 (HIDS)\n\n운영체제에 부가적 프로그램으로 설치\n\n네트워크 기반의 침입 탐지 시스템 (NIDS)\n\n네트워크에서 하나의 독립된 시스템으로 운영\n\n침입 탐지 기법\n\n오용탐지 기법:\n\n이미 발견되고 정립된 패턴을 미리 기록하여 해당 패턴이 탐지되면 알림’\n상태 전이 기법:\n\n공격 상황에 대한 시나리오를 작성하여 상태에 따른 공격 분석\n\n\n\n\n이상탐지 기법:\n\n정상적인 상태에서 급격한 변화가 일어나면 알려주는 것\n\n\n\n침입 방지 시스템\n\n침입 탐지 시스템 + 방화벽\n\nNAC\n\nIP 관리 시스템\n\n\n접근 제어, 인증\nPC 네트워크 장치 통제\n해킹 차단\n\n구현 방식\n\n인라인 방식:\n\n방화벽과 같은 방식으로 접근 차단\nGW 처럼 네트쿼크 앞에 NAC 장치를 추가하는 법\n\n\n802.1x 방식:\n\n802.1x 를 지원하는 스위치와 RADIUS 서버 이용\n\n\nVLAN 방식:\n\n인가받지 않는 사용자는 통신이 되지 않는 VLAN에 라우팅하고\n인증받으면 통신가능한 VLAN에 라우팅 되게\n\n\nARP 방식:\n\n신규 접속자가 적합한 사용자여만 ARP로 MAC주소를 알릴 수 있게\n그렇지 않으면 쓰레기 MAC 주소를 리턴\n\n\n소프트웨어 에이전트 방식:\n\n네트워크에 접속하려는 모든 클라이언트에 특수 에이전트 프로그램을 설치 하는 것\n서버에 차단 정책을 설치해서 설치된 클라이언트로 차단\n\n\n\n스팸 필터 솔루션\n\n메일 해더 필터링:\n\n보내는 사람, 도메인 등 특정 내용이 포함되어있는지 판단\n\n\n제목 필터링:\n\n메일을 이용한 웜 공격은 제목에 특수 문자열이 있는 경우가 있어 그걸 검사\n\n\n본문 필터링:\n\n본문 내용에 특정 단어가 포함되어있는지 검사\n\n\n첨부파일 필터링:\n\n이름, 크기, 개수, 파일 이름 길이 등 검사\n\n\n\nDRM\n\n문서에 열람, 편집, 인쇄에 접근 권한을 제한\n\nIOT &amp; AI 공격\nAI 공격\n데이터 변조 공격\n\n회피 공격:\n\n학습 과정에서 오류가 존재하는 노이즈를 고의적으로 추가\n\n\n중독 공격:\n\n악의적인 데이터를 주입해서 시스템이 오작동을 하게 하는\n\n\n전도 공격:\n\n인공지능에 수많은 쿼리를 통해 산출 결과를 분석해 추출\n\n\n\n네트워크 침입 탐지\n\n전문가 시스템:\n\n응용분야 지식및 능력을 잘 조직해서 비전문가도 전문가에 상응하는 능력발휘할 수 있게 도움을 주는 시스템\n\n\n오탐률:\n\n거짓 양성, Type 1 에러: 실제로는 공격이 아닌데 공격이라고 탐지하는 것\n거짓 음성, Type 2 에러: 공격을 받았으나 이를 탐지하지 못하는 것\n\n\n\n포랜식\nCERT\n\n컴퓨터 관련 침해 사고에 대응하기위해 만든 솔루션\n\n위험등급\n\n1등급:\n\n분산 서비스 공격등으로 정상동작이 불가한 상태\n\n\n2등급:\n\n비 인가자에 의해 관리자 명령어가 실행됨\n\n\n3등급:\n\n외부 또는 내부에서 취약점 수집 행위가 계속 발견\n\n\n\n절차\n\n사전 대응 → 침해 사고 대응 → 사고 탐지 → 대응 → 제거 및 복구 → 후속조치\n\n디지털 포렌식\n\n디지털 환경에 장비를 이용하여 디지털 증거 자료를 수집, 분석 하는 기술\n\n증거\n\n전문증거:\n\n전문 법착을 따르지만, 실험자가 직접 진술하지 않고 실험 결과를 타인이 전달받아 재진술 하는 형태\n\n\n증거 개시 제도:\n\n미리 제시하지 않은 증거는 법정에서 원칙적으로 사용하지 못하도록 하는 제도\n\n\n\n원칙\n\n정당성의 원칙: 적법한 절차를 거쳐야만 증거로 인정\n재현의 원칙:같은환경에서 같은 결과가 나오도록\n신속성의 원칙: 메모리 같이 휘발성으로 인해 신속하고 정확하게 수집해야 함\n연계 보관성의 원칙: 이송, 보관, 분석, 법정제출 이라는 과정이 명확해야함\n무결성의 원칙: 위, 변조 되면 안됨\n\n보안 관리\n보안 거버넌스\n\n조직의 보안을 달성하기 위한 구성원 간의 지배구조\n최고 보안책임자(CSO)가 있음\n\n\n구현 요건:\n\n가치전달, 자원관리, 위험관리, 성과 측정, 전략적 연계\n\n\n보안 조직 구성 시 고려사항:\n\n기업에 크기, 시스템 환경, it 예산\n\n\n\n영미권 보안정책\n\nSecurity Policy: 보안 활동에 알반적인 사항, 조직의 상위 관리자가 만듬\nStandards: 소프트웨어나 하드웨어에 일반 운영 규칙\nBaselines: 조직에서 지켜야할 가장 기본\nGuidelines: 하고자 하는 일에 Standards 가 없을경우\nProcedures: 각 절차의 세부 내용\n\n접근 모델\n\n임의적 접근 모델: 정보 소유자가 정보의 보안 레벨을 결정하고 접근 제어 설정 가능\n강제적 접근 제어 모델: 중앙에서 정보 수집 분류\nRBAC: 직책이 바뀌면서 불필요한 권한을 가지는 것\n\n벨-라파둘리 모델\n\n최초의 수학적 모델 정보의 기밀성에 따라 상하 관계가 구분된 정보를 보호\n\n\n읽기 권한: 자신보다 낮은 수준에 문서만\n쓰기 권한: 높은 문서 쓰기 낮은 문서 쓰기 없음\n\n비버 모델\n\n정보의 무결성을 높일때 사용\n\n\n읽기: 상위 레벨 읽기, 하위 레벨 읽기 없음\n쓰기: 상위 레벨 쓰기 없음, 하위 레벨 쓰기\n\nTCSEC 등급\nD: 보안 설정 없음\nC1: 로그인 과정\nC2: 각 계정별 로그인 가능, 그룹아이디로 통제\nB1: 시스템 내 보안 정책 적용가능\nB2: 시스템에 정형화된 보안정책 존재\nB3: 운영체제에서 보안에 불필요한 부분 제거\nA1: 수학적으로 완벽한  \n"},"시험-정리/3-2/기말---컴퓨터보안개론/컴퓨터보안개론-기말":{"title":"컴퓨터보안개론 기말","links":[],"tags":[],"content":"\n존 매카시: AI라는 용어를 처음 사용한 사람\nDES 알고리즘 암호화: 64비트의 블록 암호화, 56 비트 암호화 키\n모노 알파베틱 암호화: 26!\nPAA: 정책 승인 기관 (최상위 기관)\n커베로스: 클라이언트, 서버, SSO\n금융 보안에 주요기술: 전자서명, 전자봉투\n\n암호화\n\n최초의 암호: 기원전 480년 데마라토스\n암호화 방식:\n\n전치법: 단순히 메시지에 들어가있는 문자 위치 바꿈\n대체법: 글자를 다른 글자로 대체하여 암호화 함\n\n\n\n단일 치환 암호화\n\n키워드를 몰라도 복호화가 가능함\n빈도 분석법으로 복호화가 가능\n\n\n시저 암호화: 알파벳 3자를 오른쪽으로\n\n모노 알파베틱 암호화\n\n알파벳 과 암호키를 대칭시켜서 암호화 하고싶은 문자열을 암호키에 대응\n\n\nhello → tpkkm\n\n원본 알파벳: abcdefghijklmnopqrstuvwxyz\n암호키: cyphertxabdfgijklmnoqsuvwz\n\n암호 키 생성\n평문: cyphertxt\n생성된 암호키: cyphertxabdfgijklmnoqsuvwz\n\n\n암호키로 사용할 문자에서 중복을 제거하고\n마지막 키에 스펠링에 영순서로 다음 글자를 채워놓음\n\n예를들어 c 로 끝난경우 (d, e, f, g…)\n\n\n만일 스펠링들이 암호키와 중복이 되는경우 거기부분은 건너뛰고\n\n다중 치환 암호화\n\n암호화 키와 매핑에 따라 알파벳 하나가 여러 가지 다른 알파벳으로 대체되는\n\n비즈네르 암호화\n\n비즈네르 표를 참고하여 암호화 진행함\n\n암호화 하려는 평문: wish to be free from myself\n암호화 키: secret is beautiful\n\n결과: OMUY XH JW GVEY YZTG XQWGCJ\n\n\n평문에 글자를 비즈네르 표에 가로\n암호키에 글자를 비즈네르 표에 세로\n만약 암호 키가 평문보다 작은경우 처음부터 쓰면됨\n\nsecret is beautiful → secret is beautiful secret is...\n\n\n복호화는 반대로 일반 평문을 세로, 암호키를 가로\n\n플레이페어 암호화\n암호화 키 평문: ASSASSINATOR\n암호화 하려는 평문: BE CAREFUL FOR ASSASSINATOR\n\n\n\n암호화 키로 사용할 평문 문자에서 중복을 제거하고 모노 알파베틱 암호키 생성 방식처럼 A-Z 까지를  5x5 행렬에 채워 놓는다, 다만 끝 단어와 상관없이 무조건 A부터 빈공간을 채워놓는다\n\n\n\n암호화 하려는 문자를 2분활 한다.\n이때 두 문자가 서로 같거나, 끝 문자가 하나인 경우 X를 추가한다\n\n\n\n행 (가로) 우선 방식\n결과: OG ON OF EV VL RB SI IS NV IN TS AD CV\n\n\n2글자를 찾고 열, 행으로 따졌을때 각각 이 위치한 곳이 지나는 단어를 선택하는데 행을 먼저 적음\n암호화된 값 에서 똑같은 과정을 거치면 복호화임\n만약 두 문자가 같은 열(세로)에 위치 한다면:\n\n각 문자에 대해 아래 값으로 대체: AK → UO\n복호화에 경우 위 값으로\n\n\n만약 두 문자가 같은 행(가로)에 위치한다면:\n\n각 문자에 대해 오른쪽 값으로 대체 RD → OB\n복호화에 경우 왼쪽 값으로\n\n\n\n열 (세로) 우선 방식\n결과: GO NO FO VE LV BR IS IS VN NI ST DA VC\n\n\n2글자를 찾고 열, 행으로 따졌을때 각각 이 위치한 곳이 지나는 단어를 선택하는데 열을 먼저 적음\n암호화된 값 에서 똑같은 과정을 거치면 복호화임\n만약 두 문자가 같은 열(세로)에 위치 한다면:\n\n각 문자에 대해 아래 값으로 대체: AK → OU\n복호화는 반대\n\n\n만약 두 문자가 같은 행(가로)에 위치한다면:\n\n각 문자에 대해 오른쪽 값으로 대체 RD → BO\n복호화는 반대\n\n\n\nDES 알고리즘 암호화\n\n0, 1 의 이진 데이터를 암호화 하는 알고리즘\n\n\n다른 암호화하고는 다르게 2개의 평문(2진수)로 나눔\n이 두개의 2진수를 L1, R1 으로 각각 정의\n\n방법\n\n\nR1을 S-Box 취한 뒤 L1 하고 XOR(둘이 다르면 1) 연산을 한게 L2\nR1 그대로 R2\nR2=L3, L2=R3\n\nS-Box 방법\nR1: 1011 1100 0111\nL1: 1010 0110 1101\n\n\nR1 확장\n\n\n양옆에 이웃 이랑 구분\n# 양옆이웃 원본\n11 1011 | 10 1100 | 01 0111\n\n\n\n\nS-Box 표에서 참고\n\n\n가로: 원본\n\n\n세로: 이웃\n14 | 3 | 1 = 0100 0110 1100\n\n\n\n\n이제 L1 이랑 XOR 연산하면 그게 L2 임\n\n비대칭 암호화\nRSA 암호화\n\n암호화, 복호화에 사용되는 키가 서로 같지 않음\n\n\n평문 → 1번 개인키로 암호화 → 1번 공개키로 복호화 → 평문\n\n장점\n\n자신에 개인키로 암호화된 평문은 자신의 공개키로 밖에 열 수 없음\n\n해시\n8e1463fa1cba3c0713b7777d4f3b9961\n\n\n하나의 문자열을 더 짧은 길이의 값이나 키로 변환한 것\n정보 위, 변조를 확인하기 위한 수단\n대표적인 알고리즘: MD5, SHA\n\n특징\n\n어떤 문자를 넣어도 문자의 길이는 같다\n문자 하나만 달라저도 전혀다른 해시값이 나온다\n사실상 복호화가 거의 불가하다\n\n기타 암호화 알고리즘\n\n대칭\n\nAES 알고리즘: DES 알고리즘을 새롭게 개발한 것\nSEED 알고리즘: 한국 전자상거래 등에서 쓰기위해 만든 국내 암호화 알고리즘\nARIA 일고리즘: 전자정부 구현\nIDEA, RC5, Skipjack, LEA\n\n\n비대칭\n\nLFSR\n\n\n\n전자상거래 보안\n공개키 인증 기관 (CA)\n\n==PAA: 정책 승인 기관 (최상위 기관)==\nPCA: 정책 인증 기관, Root CA 인증서 발급 및 기본 정책 수립\nCA: 인증서 발급과 취소 등의 실질적인 업무 담당\nRA: 등록기관 공인인증서 인증 요청을 확인하고 CA 간 인터페이스 제공\n\n전자 서명\n\n서명자가 해당 전자 문서에 서명하였음을 나타내기 위한 기술\n\n\n공개키 방법을 사용하여 복호화된 해시 값이 편지에서 구한 해시값과 일치하면 위조가 아니라고 판단\n\n기능\n\n위조 불가, 인증, 재사용 불가, 변경 불가, 부인방지\n\n전자 봉투\n\n전달하려는 메시지를 암호화하여 한 사람을 통해 보내고 암호화 키는 다른 사람이 가져가도록 암호학적 으로 구현\n\n\n기밀성, 무결성, 부인방지\n\nSET\n\n비자, 마스터카드의 표준 프로토콜\n\n\n이중 서명: 신용 카드 구매정보, 지불정보 해시값을 합쳐서 또 다른 해시값을 생성 하여 그걸 개인키로 암호화 함\n\n네트워크 암호화\n\n2계층\n\nPPTP: MS\nL2TP: 시스코\n\n\n3계층:\n\nIPSec\n\n\n4계층:\n\nSSL\n\n\n\n기타 암호화\n\nPGP: 그물망, 전자 우편 암호화 방식\nS/MIME: 인증서를 통해 암호화한 이메일 서비스를 제공\n스테가노그래피: 미리 정한 약속에 따라 특정 데이터와 전혀 연관없는 데이터로\n\n보안 시스템\nSSO\n\n시스템이 몇대라도 한 시스템에서 인증 받으면 다른 시스템에 모든 접근 권한을 받는\n\n\n클라이언트가 불특정 자사 서비스에 접근하면 SSO 인증서버에 먼저 요청해서 인증을 받고\n접속하려는 서비스에 접근 시켜줌\n\n방화벽\n\n신뢰하지 않는 외부 네트워크와 내부 네트워크에 사이를 지나는 패킷에 미리 정한 규칙에 따라 허용/비허용을 결정 하는\n\n접근 제어\n\n관리자가 통과시킬 접근과 거부 접근 선택\nIP, 포트 번호로 구성\n\n룰셋 이상현상\n\n음영 이상:\n\n순위가 높은 정책이 순위가 낮은 정책을 포함하고 처리 결과가 다른 경우\n\n\n연관이상:\n\n두 개의 정책이 각기 다른 조건에 대해 서로 포함하는 경우\n\n\n일반화 이상:\n\n순위가 낮은 정책이 순위가 높은 정책을 포함하고 처리 결과가 다른 경우\n\n\n중복 이상:\n\n순위에 관계없이 두 개의 정책 중 하나가 어느 다른 하나의 정책에 포함되고 처리 결과가 같은 경우\n\n\n마침조 이상:\n\n어떠한 패킷도 방화벽 정책에 필터링 되지 않은 경우\n\n\n\n한계\n\n바이러스에 경우 파일 감염 방식이라 효과가 미비 하고\n웜에 경우 정상적 서비스 포트에 대한 공격은 막기 힘듬\n\n침입 탐지 시스템\n\n내부에 해킹이나 악성 코드 활동 탐지와 같이 방화벽이 하지 못하는 일 수행\n\n종류\n호스트 기반 침입 탐지 시스템 (HIDS)\n\n운영체제에 부가적 프로그램으로 설치\n\n네트워크 기반의 침입 탐지 시스템 (NIDS)\n\n네트워크에서 하나의 독립된 시스템으로 운영\n\n침입 탐지 기법\n\n오용탐지 기법:\n\n이미 발견되고 정립된 패턴을 미리 기록하여 해당 패턴이 탐지되면 알림\n상태 전이 기법:\n\n공격 상황에 대한 시나리오를 작성하여 상태에 따른 공격 분석\n\n\n\n\n이상탐지 기법:\n\n정상적인 상태에서 급격한 변화가 일어나면 알려주는 것\n\n\n\n침입 방지 시스템\n\n침입 탐지 시스템 + 방화벽\n\nNAC\n\nIP 관리 시스템\n\n\n접근 제어, 인증\nPC 네트워크 장치 통제\n해킹 차단\n\n구현 방식\n\n인라인 방식:\n\n방화벽과 같은 방식으로 접근 차단\nGW 처럼 네트쿼크 앞에 NAC 장치를 추가하는 법\n\n\n802.1x 방식:\n\n802.1x 를 지원하는 스위치와 RADIUS 서버 이용\n\n\nVLAN 방식:\n\n인가받지 않는 사용자는 통신이 되지 않는 VLAN에 라우팅하고\n인증받으면 통신가능한 VLAN에 라우팅 되게\n\n\nARP 방식:\n\n신규 접속자가 적합한 사용자여만 ARP로 MAC주소를 알릴 수 있게\n그렇지 않으면 쓰레기 MAC 주소를 리턴\n\n\n소프트웨어 에이전트 방식:\n\n네트워크에 접속하려는 모든 클라이언트에 특수 에이전트 프로그램을 설치 하는 것\n서버에 차단 정책을 설치해서 설치된 클라이언트로 차단\n\n\n\n스팸 필터 솔루션\n\n메일 해더 필터링:\n\n보내는 사람, 도메인 등 특정 내용이 포함되어있는지 판단\n\n\n제목 필터링:\n\n메일을 이용한 웜 공격은 제목에 특수 문자열이 있는 경우가 있어 그걸 검사\n\n\n본문 필터링:\n\n본문 내용에 특정 단어가 포함되어있는지 검사\n\n\n첨부파일 필터링:\n\n이름, 크기, 개수, 파일 이름 길이 등 검사\n\n\n\nDRM\n\n문서에 열람, 편집, 인쇄에 접근 권한을 제한\n\nIOT &amp; AI 공격\nAI 공격\n데이터 변조 공격\n\n회피 공격:\n\n학습 과정에서 오류가 존재하는 노이즈를 고의적으로 추가\n\n\n중독 공격:\n\n악의적인 데이터를 주입해서 시스템이 오작동을 하게 하는\n\n\n전도 공격:\n\n인공지능에 수많은 쿼리를 통해 산출 결과를 분석해 추출\n\n\n\n네트워크 침입 탐지\n\n전문가 시스템:\n\n응용분야 지식및 능력을 잘 조직해서 비전문가도 전문가에 상응하는 능력발휘할 수 있게 도움을 주는 시스템\n\n\n오탐률:\n\n거짓 양성, Type 1 에러: 실제로는 공격이 아닌데 공격이라고 탐지하는 것\n거짓 음성, Type 2 에러: 공격을 받았으나 이를 탐지하지 못하는 것\n\n\n\n포랜식\nCERT\n\n컴퓨터 관련 침해 사고에 대응하기위해 만든 솔루션\n\n위험등급\n\n1등급:\n\n분산 서비스 공격등으로 정상동작이 불가한 상태\n\n\n2등급:\n\n비 인가자에 의해 관리자 명령어가 실행됨\n\n\n3등급:\n\n외부 또는 내부에서 취약점 수집 행위가 계속 발견\n\n\n\n절차\n\n사전 대응 → 침해 사고 대응 → 사고 탐지 → 대응 → 제거 및 복구 → 후속조치\n\n디지털 포렌식\n\n디지털 환경에 장비를 이용하여 디지털 증거 자료를 수집, 분석 하는 기술\n\n증거\n\n전문증거:\n\n전문 법착을 따르지만, 실험자가 직접 진술하지 않고 실험 결과를 타인이 전달받아 재진술 하는 형태\n\n\n증거 개시 제도:\n\n미리 제시하지 않은 증거는 법정에서 원칙적으로 사용하지 못하도록 하는 제도\n\n\n\n원칙\n\n정당성의 원칙: 적법한 절차를 거쳐야만 증거로 인정\n재현의 원칙:같은환경에서 같은 결과가 나오도록\n신속성의 원칙: 메모리 같이 휘발성으로 인해 신속하고 정확하게 수집해야 함\n연계 보관성의 원칙: 이송, 보관, 분석, 법정제출 이라는 과정이 명확해야함\n무결성의 원칙: 위, 변조 되면 안됨\n\n보안 관리\n보안 거버넌스\n\n조직의 보안을 달성하기 위한 구성원 간의 지배구조\n최고 보안책임자(CSO)가 있음\n\n\n구현 요건:\n\n가치전달, 자원관리, 위험관리, 성과 측정, 전략적 연계\n\n\n보안 조직 구성 시 고려사항:\n\n기업에 크기, 시스템 환경, it 예산\n\n\n\n영미권 보안정책\n\nSecurity Policy: 보안 활동에 알반적인 사항, 조직의 상위 관리자가 만듬\nStandards: 소프트웨어나 하드웨어에 일반 운영 규칙\nBaselines: 조직에서 지켜야할 가장 기본\nGuidelines: 하고자 하는 일에 Standards 가 없을경우\nProcedures: 각 절차의 세부 내용\n\n접근 모델\n\n임의적 접근 모델: 정보 소유자가 정보의 보안 레벨을 결정하고 접근 제어 설정 가능\n강제적 접근 제어 모델: 중앙에서 정보 수집 분류\nRBAC: 직책이 바뀌면서 불필요한 권한을 가지는 것\n\n벨-라파둘리 모델\n\n최초의 수학적 모델 정보의 기밀성에 따라 상하 관계가 구분된 정보를 보호\n\n\n읽기 권한: 자신보다 낮은 수준에 문서만\n쓰기 권한: 높은 문서 쓰기 낮은 문서 쓰기 없음\n\n비버 모델\n\n정보의 무결성을 높일때 사용\n\n\n읽기: 상위 레벨 읽기, 하위 레벨 읽기 없음\n쓰기: 상위 레벨 쓰기 없음, 하위 레벨 쓰기\n\nTCSEC 등급\nD: 보안 설정 없음\nC1: 로그인 과정\nC2: 각 계정별 로그인 가능, 그룹아이디로 통제\nB1: 시스템 내 보안 정책 적용가능\nB2: 시스템에 정형화된 보안정책 존재\nB3: 운영체제에서 보안에 불필요한 부분 제거\nA1: 수학적으로 완벽한  \n"},"시험-정리/3-2/중간---기계학습프로그래밍/기계학습프로그래밍-중간":{"title":"기계학습프로그래밍 중간","links":[],"tags":[],"content":"\n인공지능 분류:\n\n약 인공지능: 일부 기술\n강 인공지능: 모든 기술\n초 인공지능: 인간을 뛰어 넘음\n\n\n모티브: 스키너 상자\n\n강화 학습 개념\n\nReword 그니까 보상이라는 개념을 도입하여 가중치와 편향을 학습시키는 방식\n\n\n비지도 학습에 일부분으로 보이나 보통은 지도학습, 비지도 학습, 강화학습 으로 따로 분류되어 기계 학습에 하위로 분류됨\n문제를 MDP로 표현함\n궁극적으로 누적 보상의 기대값을 최대화 하는 학습 전략\n\n구성요소\nAgent\n\n강화학습을 실행하는 주체\n\n환경 (Environment)\n\nAgent가 직접 상호작용하는 대상 Agent의 행동을 입력으로 받음\n\n종류\n\n에피소딕 환경: 종료 상태가 존재하는\n\n예를들어 바둑 두는 인공지능 이라던지\n\n\n지속적 환경: 종료가 없는\n\n주식 가격 예측 이런거\n\n\n결정적 환경:\n\n임의의 상태에 대해 결과(보상) 이 항상 일정함\n\n\n확률적 환경:\n\n위와는 반대로 일정하지 않음\n\n\n\n구성요소\n\n환경 상태:\n\n도구, 장애물 등과 같이 환경을 구성하는 중요 요소들\n가능한 모든 정보는 환경이 될 수 있음\n당연하지만 Agent에게 노출되지 못함\n\n\n관찰정보:\n\nAgent 가 환경을 마주하면서 얻는 즉각적인 상태 표현\n\n\n\n상태 (State)\n\n환경으로부터 받는 관찰 정보, 행동, 보상 등을 저장, 관리하는 상태정보\n\n행동\n\nAgent 가 환경에게 전달하는 입력 정보\n\n\n수행 할 수 있는 동작은 Agent 가 인식하는 상태마다 다름\n\n보상\n\nAgent 가 수행한 행동에 대해 환경이 Agent 한테 전달 하는 값\n\n\n해당 값에 따라 Agent 의 행동을 다르게 바꿀 수 있는 중요 요소\n\n정책 (Policy)\n\nAgent가 주어진 상태에서 어떤 선택을 하는지 결정하는 규칙\n\n\n\n결정적 정책: 임의의 상태에 대해, 행동이 정확히 한 개로 결정\n\n\n위 예제 처럼 집을 가기 위해 Agent가 어떤 곳에 있던 방향이 정확히 하나다.\n\n\n\n확률적 정책: 임의의 상태에 대해 행동이 확률로 결정\n\n\n위 예제처럼 집을 가는 방향이 어디여도 그렇게 상관은 없기에 이동 값을 확률로 준다\n\n\n\n가치함수 (Value Function)\n\n상태 가치 함수:\n\nAgent 가 위치 할 수 있는 경우에 대한 기대값을 정의하는 함수\n임의의 타임스탭에 방문한 임의의 상태에서 임의의 정책에 따라 계속 행동 할 경우 얻을 수 있는 기대 이득\n\n\n행동 가치 함수:\n\n임의의 상태에서 선택하여 수행되는 행동이 얼마나 유익한지를 나타냄\n임의의 타임스탭에 방문한 임의의 상태에서 임의의 행동을 계속 수행 한 이후 임의의 정책에 따라 행동을 계속 할 경우 얻는 기대이득\n\n\n\n종류\nModel-Based Algorithm\n\n환경에 대한 모든 설명을 알고 문제를 해결해 나가는 방식\n모델은 현재 상태와, 행동을 알고 다음 상황을 예측해감 즉 모델은 Planning(계획)에 이용되어 예측 가능한 모든 상황을 고려함\n\nModel-Free Algorithm\n\n위와 반대로 아무런 상태나, 환경에 정보 없이 문제를 해결 해감\nAgent가 행동을 통해 보상 합의 기대값을 최대로 하는\n정책함수(Policy Function)\n\n특징\n\n강화 학습은 타임 스탭으로 나눠진 상태, 행동, 보상 정보를 취급\nAgent 가 행동을 결정하여 얻는 보상은 즉시 받지 못하고 한참 후에 받을수도 있다\n\n이는 강화 학습에 난이도를 높힌다\n\n\n\n강화 학습 절차\n\n\n매 타입 스탭을 거칠 때 마다 Agent 는 환경으로 부터 보상과, 관찰 정보를 동시에 받고 자신에 Memory 에 저장\nAgent는 갱신된 저장소에서 바로 상태 정보를 구성\n해당 정보를 토대로 행동 선택 및, 수행\n수행 이후 환경은 다시 보상 및 관찰정보를 Agant에게 전달, 새로운 상태 구성\n동일한 동작 반복\n\n주요 사용 사례\n\n게임AI, 자율주행, 바둑(알파고)\n\n마르코프 과정 (Markov Process a.k.a MP)\n\n시간에 따라 주어진 환경에 상태 변화를 상태 전이 확률로 기술하는 과정\n어떤 미래를 예측할 때 과거를 고려하지 않고 현재 상태 만 참고함\n과거를 저장할 필요가 없으므로 메모리나 모델 사이즈를 크게 줄일 수 있다는 것이 장점\n\n확률이 독립 변수인 경우에는 과거의 데이터가 무쓸모이므로 해당 작업에서 큰 빛을 바램\n\n\n\n확률 과정\n\n시간이 변함에 따라 상태가 확률적으로 변하는 과정\n\n\n확률변수 x: 무작위 실험 시 특정확률로 발생할 수 있는 현상을 수치화\n\n예) 동전던지기 표본 공간 S = { (앞,앞), (앞,뒤), (뒤,앞), (뒤,뒤) }\n이때 두 동전 모두 뒤가 나올 확률= 1/4\n\n\n확률 과정은 시간에 따라 환률 변수가 바뀌는 것에 초점을 둔다\n\nMarkov property\n\n어떤 타임스탭에서 A state 에 도달하던 그 이전에 다른 N state 를 거쳐 왔던 상관 없이 다음 State 로 갈 확률은 항상 같다\n\n단순 과정\n\n\n위 Markov Chain 예시처럼 과거에 어떤 곳을 거쳐왔건 상관없이 현재 상태에서 다음 스탭으로 이동하기 위한 확률에만 의존한다\n에피소드: 각각에 확률변수로 나올수 있는 경우에 수를 나타내는 단위\n\n보상과정 (MRP)\n\n\n각 state 에 대해 이것이 얼마나 가치있는 행위인지를 나타내는 변수 R을 추가\n\n딥러닝에 가중치와 비슷하다\n\n\n이득 (return): 임의의 타입 스탭에서 얻는 이득\n\n감가이득\n\nAgant가 현재 상태에서 받을 보상들의 총합을 할인률(r)을 활용해 현재가치로 환산한것\n\n\n강화학습에 목표가 장기적인 보상을 최대화 하는것 이라 미래의 보상을 고려 하되, 시간이 지남에 따라 중요도를 점진적으로 낮춤\n즉 너무 현재가치만 중요시하지도 않고, 너무 먼 미래에만 집중하지 않도록 하는것임\n\n감가율 (r)\n\n\n가치 = 현재가치(R)+감가율(r)*현재가치(R)\n\n\nr=0: 즉각적인 보상만 고려\nr=1: 모든 보상을 동일한 가치로 고려\n0 &lt; r &lt; 1: 미래의 보상 가치를 점차 할인시켜 가까운 미래에 보상을 더 중요하게 여김\n\n마르코프 결정 과정 (MDP)\n\nMRP+action\n\n\n위에는 단순히 환경에 대해서 가치평가를 하였다면 Agent가 하는 행동에 대해서도 각각의 가치를 계산함\n사실상 위에있는건 다 집어치우고 이것만 쓴다\n\n동적 프로그래밍\n\n큰 문제를 작게 쪼게서 해결하는 장식\n마르코프 결정 과정에서 쓰이는 모든 수를 계산하는 방식\n\n최적 가치 함수\n벨만 방적식\n\n상태 및 행동 가치를 구하는 핵심 수식\n\n\n상태가치:\n\n현재 상태에 대한 가치=실행한 행동에 대한 보상+미래 가치의 합\n재귀적으로 표현됨\n\n\n행동가치:\n\n현재 상태+특정 행동을 취한 미래 가치\n특정 행동을 했을때 보상과 이후에 또다시 했을 경우의 가치를 더함\n\n\n\n한계\n\n이게 환경의 규모가 커지면 커질 수록 모든 경우에 수를 다 따지므로\n시간 복잡도가 기하 급수적으로 상승\n\n몬테카를로 방법\n\n위 벨만 방적식의 시간 복잡도 문제를 해결하기 위해 나온방법\n\n\n환경과의 상호작용 또는 시물레이션을 통해 얻는 샘플들을 사용하여\n가치함수와 최적정책 갱신\n가치함수와 정책이 서로 상호작용 하면서 최적의 접점을 향에 나아감\n종료 상태가 존재하는 에피소딕 환경에서만 사용 가능, 타입 스텝 사이에서는 정책 개선 불가, 부트스트랩 사용안함\n\n역 갱신 다이어그램\n\n종료 상태를 만나면 해당 종료 상태로부터 역 방향으로\n생성 되면서 얻은 여러 누적 보상을 평균하여 상태(s) 의 가치를 갱신\n\n방법\n\n모든 방문 MC 예측\n\n매번 방문할 때 마다 모든 보상의 합 평균\n\n\n첫 방문 MC 예측\n\n동일 에피소드 내 처음 방문했을 때에만 그 이후 모든 보상들의 합 평균\n\n\n탐험적 시작\n\n위 두 방법에서 방문하지 못한 에피소드 환경이 발생 하는데 이걸 극복하기위해\n초기 상태를 임의로 시작\n\n\n\n한계\n\n근사값이라 결국은 완벽한 최적값은 아님\n정확도를 높이기 위해서 더욱더 많은 반복을 하는것이 좋으나 문제는 몬테카를로 보다 훨씬 시간복잡도가 높아질 수 있음\n\n시간차 학습 (TD)\n\n몬테카를로 + 동적 프로그래밍\n\n\n매 타임 스텝마다 가치 함수 및 정책을 업데이트를 수행\n몬테카를로와 유사하게 직접 경험 샘플 데이터로 학습\n동적 프로그래밍과 유사하게 에피소드 완료를 기다리지 않고 타임 스텝별로 모델 갱신, 부트스트랩 사용\n\n배치 업데이트\n\n배치 내 타겟 값들을 각 상태별로 모두 합치고 평균을 내어 상태가치 함수를 한번에 갱신하는 방식\n\n사용이유\n\n임의의 상태의 가치를 보다 정확하게 추출 가능\n\n여러 샘플을 고려하여 타겟값을 추출 하므로\n\n\nMDP 의 최적 경로를 가장 정확하게 알아냄\n\n장점\n\n동적프로그래밍 보다\n\n보상체계, 상태전이 확률등 완벽한 모델을 요구하진 않음\n\n\n몬테카를로 보다\n\n하나의 스텝 진행 이후에 상태 가치 갱신이 이루짐 때문에 학습이 실시간으로 이루어 질 수 있음\n더 범용적인 모든 문제에 적용가능\n\n\n\n참고사항\n\nTD 학습과 MC 중 어느것이 더 학습속도가 빠른지 증명은 안되었음\n다만 실험적으로는 TD가 MC보다 빠르다\n\nDP &amp; MC &amp; DP\n"},"시험-정리/3-2/중간---시스템서버운영/시스템서버운영-중간":{"title":"시스템서버운영 중간","links":[],"tags":[],"content":"\nShell: 사용자가 입력하는 명령어를 이해하고 실행하는 역할\n데이터 블록: 파일에서 데이터를 저장하는 블록\n파일 종류\n\n일반파일, 디렉터리파일, 특수파일\n\n\nI-node:\n\n종류, 소유권, 엑세스 모드, 타임스탬프\n\n\n파일에 3가지 요소:\n\n파일이름, I-node, 데이터 블록\n\n\nGNU\n\n모두가 공유할 수 있는 소프트웨어\n\n\n\nVim\n모드\n명령모드 (일반 모드)\n\n입력 모드에서 Esc 키로 전환가능\n\n\n맨 처음 실행 시\n단축키로 명령을 CRUD 명령을 수행 할 수 있다\n\n단축키\n\n\ni: 커서 위치부터 입력모드\n\n\no: 커서 다음 위치부터 입력모드\n\n\na: 커서 위치 한글자 지우고 입력모드\n\n\nyy: 행 복사\n\n\np: 붙여넣기\n\n\nu: 되돌리기\n\n\nr: 현재 글자 수정\n\n\nctrl+r: 다시 실행\n\n\n입력모드\n\n명령 모드에서 i, a, o 키로 전환 가능\n\n\n기본적인 텍스트 편집\n\n마지막 행 모드 (명령 라인 모드)\n\n명령 모드에서 : , ?, / 키로 전환 가능\n\n\n특정 명령어를 입력함\n\n명령어\n\nwq!: 저장 나가기\n w!: 변경사항 저장\n q!: 그냥 나가기\n\n네임드 버퍼\n\n언네임드 버퍼:\n\n이름을 붙이지 않은 버퍼. yy 명령키로 복사하거나 dd 명령으로 붙어넣거나\n\n\n네임드 버퍼:\n\n이름을 붙어서 버퍼를 사용\n버퍼에 이름을 붙어서 붙어넣거나 할때 해당 이름으로 복사하거나 붙어넣거나 할 수 있음\n\n\n\n파일속성\n\n파일 종류 기호\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n기호설명-일반 정규적인 파일d디렉터리 파일l심볼릭 링크 파일b블록 단위의 Read/Writec섹터 단위의 Read/Write 문자장치 파일p프로세스 간 통신에 사용되는 특수 파일 (파이프 파일)s네트워크 통신에 사용되는 특수 파일 (소켓 파일)\n명령어 정리\n\n\nls:\n\n-F: 파일 종류 기호 표시 (@: 심볼릭, * 실행파일)\n-l: 파일 상세정보(권한 출력)\n-i: 아이노드 번호 확인\n\n\n\nfind &lt;경로&gt; &lt;옵션&gt;:\n\n-name &#039;&lt;정규표현식&gt;&#039;: 파일이름 검색\n-type &lt;f | d&gt;: 파일 타입 선택\n-empty: 빈파일 검색\n-exec &lt;명령어&gt; {} \\;: 찾은 파일명들을 {}에 대입시켜 해당 명령어 실행\n\n\n\nchmod &lt;u+x,g+w&gt; &lt;파일명&gt;: 심볼릭 모드 (파일권한)\n\n\ncat:\n\n만약 단순 텍스트 편집이라면 ctrl+d 로 나가기\n\n\n\nman &lt;프로그램&gt;: 프로그램 메뉴얼 확인\n\n\nwhich: 실행파일 위치\n\n\nwhereis: 고정경로 에서 실행파일 검색\n\n\ntouch:\n\n-t &lt;년,월,일,시,분&gt; | &lt;시,분,초&gt;: 시간 수정\n-d &lt;년,월,일&gt;: 날짜 수정\n\n\n\nuname: 운영체제 확인\n\n-a: 모든 정보 확인\n-m: arch 와 같음\n\n\n\nmkdir &lt;폴더명&gt;:\n\n-p\n\n하위 폴더 까지 생성\n이게 원래는 하위 폴더 없이 /test/test2 이러면 오류\n\n\n\n\n\necho:\n\n-e: 개행 넣을때 (echo -e \\n하이)\n\n\n\nhistory:\n\n-c: 삭제\n\n\n\nln &lt;옵션&gt; &lt;원본파일명&gt; &lt;링크파일명&gt;: 하드링크\n\n-s: 심볼릭 링크\n\n\n\nrm:\n\n-i: 삭제 할것인지 확인\n\n\n\ncp:\n\n-r: 디렉토리 복사\n\n\n\npwd: 현재 작업중인 디렉토리\n\n\nusers: 사용중인 사용자 ID 확인\n\n\ncal: 날짜 출력\n\n\nwho: 접속자 확인\n\n\n8진수 파일 권한\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrwx421"},"시험-정리/3-2/중간---예술과인성/예술과-인성-중간-정리":{"title":"예술과 인성 중간 정리","links":[],"tags":[],"content":"\n계몽주의 시대의 예술: 자연 모방\n예술 다른점: 장소, 시간, 개인의 인지적 배경\n\n관계아님\n\n\n예술가 직업적 특성:\n\n인간성의 단면\n사회 문제 알리기\n\n\n어린왕자 &quot;저 밀밭을 보면 네 생각이 날꺼야: 여우\n의사 소통: 사람과 사람 뿐 아니라 동물 간에 의사소통도 함\n연주된 오페라: 하나의 메시지\n협동성: 본능\n인간의 인지: 주제, 의도, 시나리오, 표제\n\n결과는 아님\n\n\n예술성:\n\n진지하고 본질 적인 고민\n기대밖의 충격\n인지적 요소\n\n\n스티브 잡스 창의:  기존의 것에 새로운 정의를 만들어 내는 것\n고정관념(외과의사=남성): think in the box\n설치미술 3요소: 장소, 담화, 미디어\n\n기획 아님\n\n\n문제해결 과정 순서:\n\n문제 발견 → 계획수립 → 계획실행 → 결과평가\n\n\n확대 활성화: 특정 단어를 말했을때 아무 연관도 없는것이 떠오르는 현상\n\n여우 밀밭 → 어린왕자\n\n\n\n의사 소통 과정\n생산자\n\n메시지를 만든다\n예를들어 생산자가 한국인 이라면 한국어로 부호화\n이때 소통채널 은 한국어라는 언어 체계\n해당 메시지는 수용자가 부호해독\n수용자 한테 전달\n\n수신자\n\n수용자는 응답 메시지로 피드백을 보냄\n생산자가 거친 과정과 동일하게 진행이 된다\n\n이때 부호화, 부호 해독이 재대로 안된 경우엔 그에대한 피드백\n\n\n\n오페라의 의사소통 과정\n\n\n작곡가는 성악, 가악에 대한 지식을 바탕으로 부호화 하여 악보를 완성\n지휘자는 오페라 악보를 음악적 부호체계(오선지)를 사용하여 해독\n지휘자는 자신이 해독한 걸 악단과 함께 연주\n\n이것은 작곡자에개 보내는 피드백\n\n\n연주는 곧 관객에게 보내는 메시지 이며 관객은 수용자가 돤다\n관객은 박수 갈채, 또는 야유로 해당 연주에 대한 피드백을 보내게 된다\n그럼 메시지를 생산 했던 작곡자, 연주자/지휘자에게 피드백이 들어온다\n\n마르셀 뒤샹의 샘이 예술적 가치를 인정받는 이유\n\n예술이란 예술가에 손에서 만들어저야 한다는 사람들에 인식을 뒤샹에 주장하는\n예술가가 무엇인가를 선택하여 거기에 예술적 의미를 담았다면 그 역시\n예술이라고 인식이 바뀌었다\n"},"시험-정리/3-2/중간---컴퓨터보안개론/컴퓨터보안개론-중간":{"title":"컴퓨터보안개론 중간","links":[],"tags":[],"content":"연습문제 정리\n\n해킹: 남의 컴퓨터 시스템에 허락 없이 침입하여 데이터를 빼내거나 파괴하는 일\n보안의 3대요소:\n\n기밀성: 인가된 사용자만 정보자산의 접근\n무결성: 적절한 권한을 가진 사용자가 인가된 방법으로만 정보 변경\n가용성: 필요한 시점에 정보 자산에 접근 가능하도록\n\n\n세션: 사용자와 시스템 사이 또는 두 시스템 사이의 활성화된 접속\n최초 이메일: 1970\nsnmp: IP 기반 네트워크상의 각 호스트로부터 정기적으로 여러 관리 정보\nSFP: 함수 호출시 스택 프레임 구조를 유지하기 위해 저장되는 이전 함수 프레임 포인터\nMAC 주소: 12개 16진\n\nDB 권한 관리\n\n뷰를 사용하지 않는경우 접근제한, 모든 권한은 뷰를 생성해서 하도록\n네트워크 트래픽을 탐지 할 수 있는 태핑 장비를 DB 서버 중간에 설치\n즉 일종의 하드웨어 방화벽을 설치하는 것\n\n네트워크 패킷 중 SQL 질의문을 탐지하여 수상한 접근 케치\n\n\n\nAAA 요소\n\n시스템 사용자가 로그인한 후 명령을 내리는 과정에 대한 시스템 동작\n\n\nAuthentication (인증):\n\n아이디, 비번 입력하는 과정\n\n\nAuthorization (인가):\n\n로그인을 허락된 사용자로 판명하여 로그인 하는 과정 즉, 신원 확인 과정\n\n\nAccounting:\n\n로그인 했을 때 시스템에 로그를 남기는 과정\n\n\n\n리눅스 시스템 로그\n\n/usr/adm: 초기 유닉스\n/var/adm: 최근 유닉스\n/var/log: 리눅스\n\n로그 관리\n\n시스템 내부나 네트워크를 통해 외부에서 시스템에 어떤 영향을 미칠 경우 그 내용 을 기록하여 관리하는 것\n\n로그 종류\n\nhistory: 명령창 로그\nsyslog: 시스템 운영체제 전반의 로그\nutmp: 유저 로그인 기록\n\n블루투스 취약점\n블루프린팅\n\n블루투스 공격장치의 검색 활동을 의미\n\n\n블루투스는 장치의 종류 식별을 위해 SDP 를 보내고 받음\n공격자는 이 SDP 패킷을 활용하여 공격 가능 장치를 식별\n\n블루스나프\n\n블루투스의 OPP 기능을 이용한 파일 접근 공격\n\n\n장치인증 없이 간편 정보 교환 프로토콜 OPP 기능을 이용해 주소록이나 달력등 민감한 정보를 탈취\n\n블루버그\n\n블루투스 장비 간의 취약한 연결을 이용\n\n\n공격장치와 대상장치를 연결하여 임의의 조작\n블루투스는 한 번 페어링 하면 그 다음부터는 자동으로 다시 연결되는데 이 점을 이용\n\nIP 계산\n\n네트워크 IP, 브로드캐스트 IP를 구하시오 (IP: 192.168.25.10, 서브넷 마스크: 255.255.252.0)\n\n192.168.25.10 와 255.255.252.0 를 AND 연산\n네트워크 IP=192.168.24.0\n구한 네트워크 IP 에 네트워크주소로 쓰인 22비트 제와하고 남은 10비트를 모 1로\n브로드캐스트IP=192.168.27.255\n\n\n\n네트워크 보안\nDoS (서비스 거부 공격) &amp; DDoS (분산 서비스 거부 공격)\n\nDoS는 그냥 혼자하는거고\nDDoS는 여러 사람이 하는 것\n\n공격 유형\n\n취약점 공격형: 오류가 있는 네트워크 패킷을 보낼경우 서비스에 오작동이 발생할 경우 그걸 이용해서 공격하는\n자원 고갈형 공격:네트워크 대역폭이나 시스템 자원을 소모시키는\n\nDoS 공격 방법\n\n보잉크/봉크/티어드롭 공격:\n\n프로토콜의 오류 제어 로직을 악용하는 방식\n반복적인 재요청과 수정을 해서 TCP 쪽에서 오류 처리를 유도하여 시스템을 느리게 만듬\n\n\n랜드 공격:\n\n출발지, 목적지 IP 주소를 동일하게 만들어서 패킷을 뺑뺑 돌리게 만든다\n\n\n죽음의 핑 공격:\n\nping 에 사용하는 ICMP 패킷을 사용하여 더럽게 많이 보내서 마비시키게 만듬\n그래서 내부적으로 ICMP 포트를 막아주면 해결 가능\n\n\nSYN 플러딩 공격:\n\n사용자가 많은 것처럼 가상의 사용자를 만들어서 한꺼번에 많이 접속하여 3-way 동작을 유도시켜 마비시켜 버리는\n\n\n스머프 공격:\n\nICMP 브로드캐스트을 사용하여 네트워크에 존재하는 임의의 시스템을 통해 여러 클라이언트로 확장시켜 공격\n다이렉트 브로드캐스트:\n\n외부 네트워크에서 특정 네트워크에 브로드캐스팅 할 수 있는 기능\n192.168.0.0 네트워크라면  192.168.0.255 로 브로드캐스팅\n공격자는 시작 IP 주소를 위조하여 192.168.0.255 네트워크에 ICMP request 하여 192.168.0.255 에 연결된 모든 장치들이 위조된 IP 장치에 ICMP 응답을 받게 하여 마비 시킨다\n\n\n\n\n\nDDoS 공격 방법\n\n공격자: 메인 두목PC\n마스터: 에이전트를 관리하는\n\n핸들러 프로그램: 마스터 시스템에 역할\n\n\n에이전트: 직접공격을 가하는 PC\n\n데몬 프로그램: 실제 공격을 하는 프로그램\n\n\n\n스니핑\n\n네트워크 통신을 하는 시스템들의 데이터를 엿보는\n\n\n보통 공유기나, 스위치는 모든 패킷을 브로드케스팅 한다,\n다만 실제 브로드케스팅 요청이 아닌 경우 필터링 과정을 거치기 때문에\n다른 클라이언트에 패킷을 받아 볼 수 없는것\n스니핑은 필터링을 무시하고 모든 패킷을 받아보는\n\n종류\n\n스위치 재밍 공격:\n\n랜덤으로 MAC 주소 생성해서 스위치에 존나게 보내서 MAC 테이블 저장용량을 초과 시킴\n\n\nSPAN 포트 공격:\n\n스위치에 송/수신 되는 데이터를 미러링 함\n\n\n\n프러미스큐어스 모드\n\n데이터 링크 계층과 네트워크 계층의 필터링을 해제하는 랜 카드의 모드\n\n스니핑 탐지\n\nPing을 이용한 방법:\n\n의심가는 Host에 존재하지 않는 MAC 주소로 ping을 보내서 원래는 응답을 못받아야 정상인데 받으면 해당 Host는 스니퍼임\n\n\nARP를 이용한 방법:\n\nARP를 위조하여 없는 MAC 주소로 설정했는데 정상적이라면 아무런 응답이 없어야 하는데 스니퍼 Host는 응답을 보냄으로 탐지가능\n\n\nDNS를 이용한 스니퍼 탐지:\n\nDNS 응답 과정을 이용한 탐지\n\n\nARP watch:\n\nARP 모니터링 하여 탐지\n\n\n\n스푸핑\n\n서버와 클라이언트 통신 과정을 스푸퍼가 처리를 하게 되는\n\n\nARP 스푸핑:\n\n서버와 클라이언트 통신 과정 시 공격자의 MAC 주소로 속여서 요청/응답 을 모두 공격자가 하게 되는\n2계층 에서 일어남\n\n\nIP 스푸핑:\n\n다른사람의 IP 주소를 강탈하여 어떠한 권한을 획득\n서버가 IP 주소로 권한의 대한 처리가 들어가 있다면\nICMP 리다이렉트: 공격자를 네트워크에 속한 라우터라고 알려 패킷에 흐름을 바꿈\n\n\nDNS 스푸핑:\n\n실제 DNS 서버보다 빨리 대상에게 DNS Response 패킷을 보내서 공격대상이 잘못된 ip 주소로 가게 설계\n\n\n\n세션 하이재킹\n\n두 시스템간 연결이 활성화 된 상태, 즉 로그인된 상태를 가로채는것\n누군가 네이버에 로그인 한 채로 자리를 비웠을 때 그 자리로 가서 그 사람 계정으로 뭔 짓거리를 하는거 같은것\n\n무선 보안\nSSID 브로드캐스팅 금지\n\n보통 wifi 창가면 wifi 이름이 보이는데 이걸 SSID 브로드캐스팅 이라고 함\n브로드캐스팅을 해제 하고 수동으로 SSID 를 사용자가 입력 해줘야만 접속 가능하게 설계 하는\n\n암호화 프로토콜\n\nWEP:\n\n단순한 비밀번호로 요청하여 Wifi 를 연결하는\n\n\nWPA-PSK:\n\nWEP의 복호화가 간단하다는 문제를 해결하기 위해 나온 프로토콜\n\n\nEAP:\n\n기업환경을 위한 암호화 방법\n중앙 암호화 RADIUS DB 서버 하나 두고 둘 사이를 컨트롤\n\n\n\n웹 보안\n웹 프록시\n\n프록시 서버를 하나 생성해놓고\n프록시 서버가 패킷을 받아서 위변조\n\n구글 고급 검색기능\n\nsite\nfiletype:&lt;확장자&gt;: 특정 유형 파일에 검색하는 문자가 있는지\nintitle:&lt;검색어&gt;: 특정 사이트 제목\n\n검색 엔진 우회\nUser-agent: googlebot\nUser-agent: * \nDisallow: dbconn.ini\nDisallow: /admin\n\n주요 취약점\nSQL Injection\n\n웹에 URL에 SQL 구문을 넣어 DB 접근을 유도함\n애플리케이션이 사용자로부터 입력받은 데이터를 검증 없이 SQL 쿼리에 직접 포함할 때 발생\n\n인증 및 세션 관리 취약점\n\n첫 접속시 자기의 아이디, 비번을 재대로 입력\n인증 과정 시 나온 세션키를 사용하여 유저 아이디만 다르게 수정하여 다시 접속\n만약 아이디 &amp;&amp; 세션 키 조합 인증이 안된경우 아이디만 알면 해당 계정으로 접속하여 사이트를 이용할 수 있게된다는 뜻\n\nXSS\n\n공격자가 서버에 작성한 스크립트가 다른 사람에게 전달되는 것\n\n\n서버에 공격 스크립트(js등) 작성하여 저장\n\n이때 서버는 WAS와 같이 SSR인 경우 효과적\n\n\n그러면 사용자는 서버에 접속하는 순간 해당 코드가 실행됨\n해당 취약점을 방지하기 위해 일부 특수문자에 대한 예외 처리가 필요\n\nBroken Access Control\n\n인증 권한 Level 설정으로 인해 일반 사용자가 관리자 페이지 같은곳에 접근 할 수있는\n\n\n예를들어 웹서버에 파일 탐색권한을 제대로 막지 않았다던가\n\nSecurity Misconfiguration\n\n디렉터리 리스팅:\n\n웹 브라우저에서 웹 서버의 특정 디렉터리를 열면 그 디렉터리에 있는 파일과 목록이 모두 나열되는 것을 말함\n\n\n리버스 텔넷:\n\n시스템 권한 획득 후 텔넷과 같은 쉘에 명령을 입력 할 수 있는\n예를들어 서버에 ssh 터널링을 활성화 한 경우 서버에 권한을 획득해서 명령을 수행 할 수 있게되는거\n\n\n\n민감한 데이터 노출\n\n해킹으로 인해 클라이언트 유저의 민감한 데이터 유출\n\n공격 방어 취약점\n\nAPT 공격이 일반화되면서 보안 솔루션으로 탐지가 어려워 웹 에플리케이션 수준에서 탐지, 로깅 을 권고\n\nCSRF 취약점\n\n\n구조는 XSS 와 비슷\nCSRF는 스크립트 서버에 전송하여 서버가 해당 스크립트를 실행하도록 설계\n즉 만약 관리자(서버) 가 해당 요청을 받으면 해당 요청을 처리할때 모든 클라이언트에 CSRF 스크립트가 실행이 됨 즉, 관리자, 사용자 모두가 피해자가 된 상황\n\nXSS, CSRF 차이점\n\nXSS 에 경우 사용자 클라이언트에 스크립트를 실행시켜 해당 사용자만 해킹당한다\nCSRF는 스크립트가 서버에서 실행시켜 해당 서비스를 이용하는 모든 클라이언트를 해킹시킨다.\n\n메모리 구조\n\n스택 영역:\n\n프로그램이 동작하기 위한 인자, 프로세스 상태 저장\n레지스터의 임시 저장공간\n가장 윗단 이라 레지스터와 가까움\n\n\n힙 영역:\n\n실제 프로그램 실행 시 사용되는 메모리 공간\n\n\n\n어셈블리 레지스터 정리\n\npushl %ebp: 메모리 스택 최상단(자기 기준) 주소값을 가르킴\nmovl %esp,%ebp: esp(스택 최하단) 레지스터 값을 ebp(스텍최상단)으로 복사\ncall function: function 함수 호출\nsubl $12,%esp: 12 바이트의 공간을 할당하기 위해 esp 값을 12로 줄임\naddl %eax,8(%ebp): eax 값에  *(ebp+8)값을 더함\ncx: 반복적으로 실행되는 특정 명령에 사용\n\n시스템 공격\n\nSetUID:\n\n유닉스 권한이 rwsr-xr-x인 경우 누군가 해당 파일을 실행해도 파일 소유자 권한을 가짐\n\n\n\n버퍼 오버플로 공격\n\nstrcpy() 함수같이 명시한 버퍼 크기를 초과하여 값을 입력하는 경우 해당 버퍼 공간 이상으로 메모리를 침범하여 문제가됨\n\n\nstrcpy() 에 10바이트 공간을 할당했는데 만일 이를 무시하고 15바이트 데이터를 보낸다 하더라도 해당 넘치는 5바이트에도 문제없이 값이 대체되게 된다\n즉 일종의 메모리 변조가 가능한 셈이다\n만일 10바이트 할당한 공간 이후에 오는 1바이트 공간이 사용자 로그인을 허용하는 값이라고 치자\n그럼 해커 입장에서는 마지막 11바이트 공간에 1 값을 할당시켜서 손쉽게 로그인을 할 수있게 되는 거다\n그러므로 안전하게 strcpy_s 를 쓰도록하자\n\n포멧 스트링 공격\n\nprintf() 함수를 사용할때 %d 와 같은 서식 문자를 입력할때 취약점\n\n\n사용자가 문자열을 입력하고 그걸 printf() 로 출력하는 상황일 경우\n이전입력 문자 개수를 더해 참조하는 메모리 주소에 쓰는 서식문자인 %x 가 있다\n\n그렇다면 사용자가 aaa%x 라고 입력하면 어떤일이 발생하겠는가?\n\n\n취약한 서식 문자:\n\n%n: int(쓰인 바이트 수, 문자면 문자 길이)\n\n\n\n악성코드\n바이러스\n\n숙주가 되는 파일을 감염시켜 퍼지는 악성코드\n다른 파일이나 프로그램에 자신을 삽입\n\n\n부팅 바이러스: 부팅시 MBR 정보를 메모리에 저장할때 사용되는 모든 프로그램 감염\n\n부팅순서: POST → CMOS → MBR or 윈도우 부트매니저\n\n\n\n파일 바이러스\n\n\n1세대: 바이러스가 프로그램 뒤에 위치함\n\n대표적으로 예루살렘, 선데이 등\n\n\n2세대: 암호형 바이러스\n\n하드에 있을때는 암호와 되어있는데 메모리에 올라오면서 암호가 풀림\n백신은 그럼 메모리를 스캔 때려서 바이러스 분석\n대표적으로 슬로 등\n\n\n3세대: 은폐형 바이러스\n\n감염된 파일이 일정기간 잠복기간 가지도록 설계\n대표적으로 브레인, 조시 등\n\n\n4세대: 다형성 바이러스\n\n코드를 다양하게 조합하여 암호형 바이러스와 덧붙여 백신이 식별을 어렵게함\n\n\n5세대: 매크로 바이러스\n\nMS 오피스 같은 사무용 프로그램에 매크로 기능으로 감염시킴\n\n\n차세대:\n\n네트워크와 이메일로 전파\n파일감염을 넘어 정보탈취와 백도어 기능\n\n\n\n웜\n\n스스로 복제하여 네트워크를 통해 전파되는 악성코드\n스스로 증식하는 것이 목적이라 파일이나 운영체제 자체에 이런 기능이 있거나 운영체제를 감염 시킴\n\n\n메스 메일러:\n\n자기를 포함하여 대량으로 메일 전송\n사용자가 읽었을때 감염\n\n\n시스템 공격형:\n\n운영체제 취약점을 이용하여 내부정보 파괴하거나 사용할 수 없게 만듬\n\n\n네트워크 공격형:\n\nSYN 플러딩 DOS 공격을 수행하는\n\n\n\n트로이 목마\n\n악성 루틴이 숨어있는 프로그램 (백도어)\n\n\n겉으로는 멀정해 보이나 사용자가 실행하면 악성코드 실행\n백도어: 제품 출시 시 삭제해야 하지만 제품 출시 할때까지 남아있는 경우가 있음\n\nPUP (Potentially Unwanted Program)\n\n사용자에게 동의를 구하지만 용도를 파악하기 어려운 프로그램을 설치하는 경우\n\n\n프로그램 설치 시 더미로 설치하려는 프로그램들을 의미함\n"},"유용한-것들-모음/Apache-Tomcat-관련":{"title":"Apache Tomcat 관련","links":[],"tags":[],"content":"UTF-8 설정\n\nserver.xml\nConnector 태그 안에 해당 구문 추가\n\n&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;\n    connectionTimeout=&quot;20000&quot;\n    redirectPort=&quot;8443&quot;\n    maxParameterCount=&quot;1000&quot;\n\tURIEncoding=&quot;UTF-8&quot; &lt;!-- 추가 --&gt;\n/&gt;\n\nweb.xml\n해당 부분 주석 해제\n\n&lt;filter&gt;\n    &lt;filter-name&gt;setCharacterEncodingFilter&lt;/filter-name&gt;\n    &lt;filter-class&gt;org.apache.catalina.filters.SetCharacterEncodingFilter&lt;/filter-class&gt;\n    &lt;init-param&gt;\n        &lt;param-name&gt;encoding&lt;/param-name&gt;\n        &lt;param-value&gt;UTF-8&lt;/param-value&gt;\n    &lt;/init-param&gt;\n    &lt;async-supported&gt;true&lt;/async-supported&gt;\n&lt;/filter&gt;\n\nlogging.properties\n해당 부분 EUC-KR 로 설정\n\njava.util.logging.ConsoleHandler.encoding = EUC-KR\n\n배포\n\nmaven package 명령 실행\n명령실행 후 나온 target/[파일이름.war] 파일 복사\nTomcat 폴더에서 webapps 폴더내부에 붙어넣기\nbin 폴더에서 startup.bat을 실행하여 서버 실행\n주소창에 http://localhost:8080/[파일이름]으로 접속\n\n라우팅 변경\n\n기존에 http://localhost:8080/[파일이름] 이렇게 접근하는걸 변경함\nserver.xml에서  &lt;Host&gt; 태그 내부에 해당 내용 추가\n\n&lt;host ...내용&gt; \n&lt;!-- 기존내용--&gt;\n&lt;Context path=&quot;/&quot; docBase=&quot;/기존경로이름&quot; reloadable=&quot;false&quot;/&gt;\n&lt;!-- 기존내용--&gt;\n&lt;/host&gt;\n\n이렇게 하면 뒤에 파일이름 안 붙이고 접속가능\n\n포트 변경\n\nserver.xml에서 해당부분 변경\n\n&lt;Connector port=&quot;80&quot; //변경\n\t...기존내용들\n/&gt;"},"유용한-것들-모음/CLI-옵션-국룰":{"title":"CLI 옵션 국룰","links":[],"tags":[],"content":"\n\n문자는 -, 문자열은 --\ntest.exe -p --name test\n\n\n문자 하나로 축약이 가능한 경우 문자와 문자열 둘 다 가능하게 하자\ntest.exe --passwd test || test.exe -p test\n\n\n값이 없는 옵션은 보통 해당 설정을 켠다라는 의미이다\n해당 옵션을 추가하지 않는 경우 false 추가하면 true\ntest.exe --on\n\n\n옵션의 값이 없고, 문자 하나로 이루어진 옵션끼리는 하나로 합칠 수 있게 해야한다\ntest.exe -p -a ==&gt; test.exe -pa\n\n"},"유용한-것들-모음/Git-로그인":{"title":"Git 로그인","links":["API-키-목록"],"tags":[],"content":"초기 로그인\n\ngit config —global user.name Lseoksee\ngit config —global user.email da864268@naver.com\n\n토큰 로그인\n\n\n                  \n                  참고 \n                  \n                \n\nGitHub 토큰\n\n\n영구 로그인\necho https://&lt;Username&gt;:&lt;토큰&gt;@github.com &gt; ~/.git-credentials\n임시 로그인\n\n\n일단 Private 리포지토리나, push 작업을 한번 시도하면 유저이름이랑, 패스워드 물어봄\n\nUsername: Lseoksee\nPassword: 토큰\n\n\n\n이후에 아래 명령어를 입력하면 해당 리포지토리에 한해서 토큰이 만료되기 전까지는 계정 인증이 됨\ngit config credential.helper store\n\n\nGPG 인증 받기\n\n윈도우에 경우에는 Gpg4win 다운로드\n리눅스는 해당 링크 참고\n\n신규 생성\nGPG 키 생성\ngpg --full-generate-key\n등록 과정\nPlease select what kind of key you want\n\t- 여기서 1 입력\n \nRSA keys may be between 1024 and 4096 bits long.\n\t- 4096\n \nRSA keys may be between 1024 and 4096 bits long.\n\t- 2년 정도로 잡는게 좋으므로 `2y`\n \nYou need a user ID to identify your key; the software constructs the user ID\n\t- 본인 실명이랑 여러 정보 입력\n\n이후 정보가 맞냐는 입력을 요구할텐데 맞으면 o 입력\n그 다음 암호 설정 창 나올텐데 원하는 암호 입력\n\ngpg 키 불러오거나 커밋 서명할때 요구함\n\n\n\n핑거프린트 알아내기\ngpg --list-secret-keys --keyid-format=long\n\nsec 바로 아래 나오는 40 자리 값이 중요 이것을 핑거프린트 라고함\n어딘가에 기록해두자\n\n공개키랑 개인키를 저장\ngpg --output &lt;공개키 파일&gt;.gpg --armor --export &lt;핑거프린트&gt;\ngpg --output &lt;비밀키 파일&gt;.gpg --armor --export-secret-key &lt;핑거프린트&gt;\nGithub에 공개키 등록\n\n저장한 공개키 파일.gpg 파일을 열어서 안에 내용을 복사\nGitHub 계정 설정 → SSH and  GPG keys\n복사한 gpg 키를 등록\n\n기존 생성\ngpg --import &lt;공개키 파일&gt;.gpg\ngpg --allow-secret-key-import --import &lt;공개키 파일&gt;.gpg\n핑거프린트 알아내기\ngpg --list-secret-keys --keyid-format=long\n공통 사항\ngit에 등록하기\ngit config --global user.signingkey &lt;핑거포인트&gt;\ngit config --global commit.gpgsign true\n(윈도우 한정) gpg 프로그램 경로 잡기\n# 보통은 이경로\ngit config --global gpg.program &quot;C:\\Program Files (x86)\\GnuPG\\bin\\gpg.exe&quot;\n(윈도우 한정) GNUPGHOME 환경변수 설정\n\n\n                  \n                  참고자료 \n                  \n                \n\n\nGit GPG 서명 설정 후 커밋 시 개인 키 없음 오류 해결\n\n\n\n# 보통은 이 경로\nsetx -m GNUPGHOME &quot;C:\\Users\\Seoksee\\AppData\\Roaming\\gnupg&quot;\n\n해당 명령어는 관리자 권한으로 실행\n\nGUI 프로그램들의 로그인 방식\n\n보통은 Git Credential Manager를 사용하는듯 하다\n"},"유용한-것들-모음/Google-API-사용법":{"title":"Google API 사용법","links":[],"tags":[],"content":"\n구글 클라우드 OAuth 동의 화면으로 이동하여 앱 수정을 클릭\n맨 밑에 저장 후 계속을 클릭\n범위 추가 또는 삭제를 눌러 추가 해야할 범위를 추가 하고 저장한다\n참고\naccounts.google.com/o/oauth2/v2/auth로 로그인 요청해서 access token을 발급해야한다. 이때 scope 값을 필요한 요청 승인 url로 수정하고 include_granted_scopes=true로 설정한다.\n발급한 access token으로 api주소에 하단처럼 요청하면 된다.\nmethod: &quot;get&quot;,\nheaders: {\n\tAuthorization: `Bearer [access token]`,\n},\n\n구글드라이브 파일다운 요청 url\nwww.googleapis.com/drive/v3/files/[파일id]?alt=media\n\n\n\n\nGET 요청을 할때 해더를 사용할수 없는경우\nurl?Authorization=Bearer [access token]\n이런 식으로도 가능하다.\n"},"유용한-것들-모음/Gradle-스크립트-한글-깨짐-문제-해결":{"title":"Gradle 스크립트 한글 깨짐 문제 해결","links":[],"tags":[],"content":"원인\n\nGradle 스크립트의 기반이 되는 gradle-wrapper.jar 의 기본 출력 인코딩 셋이\nUTF-8 이다\n그러나 윈도우 콘솔 환경에 기본 인코딩 셋은 MS949(CP-949 | EUC-KR)임\n\n참고로 리눅스 Bash는 UTF-8이 기본 인것으로 안다\n\n\n그러다 보니 Gradle 스크립트는 UTF-8로 출력되고 윈도우는 그걸 MS949 로 받고 있으니 깨지는 것\n\n해결 방법\n\n개같은 것이 인터넷에 해결방법 이라 떠돌아 다니는 것들은 Gradle 프로젝트 자체의 인코딩을 건드는 것들 뿐이다\n최종 출력은 gradle-wrapper.jar이 담당하는거라 이쪽 출력을 수정하지 않으면 안되는데 이딴 건 나오지도 않는다\n\ngradlew.bat 파일 수정\n\n먼저 Gradle의 실행 스크립트인 gradlew 의 동작 원리를 보면 윈도우에서는 배치파일 인데\n\nJVM 인자들이랑 여러 사용자 입력을 배치파일로 처리해서 gradle-wrapper.jar 에 전달하는 구조임\n해당 파일에서 gradle-wrapper.jar 을 MS949 로 인코딩 해서 출력할 수 있도록 하면됨\n\n\n\n# 해당 부분 찾기\nset DEFAULT_JVM_OPTS=&quot;-Xmx64m&quot; &quot;-Xms64m&quot;\n \n# 이렇게 수정\n===\nset DEFAULT_JVM_OPTS=&quot;-Xmx64m&quot; &quot;-Xms64m&quot; &quot;-Dfile.encoding=MS949&quot; &quot;-Dstdout.encoding=MS949&quot; &quot;-Dstderr.encoding=MS949&quot;\n===\nbuild.gradle 수정\n\n프로젝트 java 파일은 gradle 이 내부적으로 UTF-8로 읽을 수 있게 수정 해야 함\n\nallprojects {\n    compileJava.options.encoding = &#039;UTF-8&#039;\n    tasks.withType(JavaCompile) {\n        options.encoding = &#039;UTF-8&#039;\n    }\n}\n추가 설명\n\n아마 gradle-wrapper.jar 내부 클래스에서 내쪽 클래스를 jvm 인자를 전달하여\njava 로 서브 프로세스 생성해서 실행되는 구조인듯\n그래서 gradle-wrapper.jar 내부 클래스에서 실행할때 UTF-8로 해석되니까 거기에 맞춰 설정을 해줘야 한다는\n"},"유용한-것들-모음/JVM-Args":{"title":"JVM Args","links":[],"tags":[],"content":"\n-Xmx [MB | G]: 최소 램\n-Xms [MB | G]: 최대 램\n\n인코딩\n\n-Dfile.encoding=[Charset]: 파일 인코딩\n\n~java17: 해당 국가 인코딩 (한국: MS949)\njava18~: UTF-8\n\n\n-Dstdout.encoding=[Charset]: 표준 출력 인코딩\n-Dstderr.encoding=[Charset]: 표준 오류 출력 인코딩\n"},"유용한-것들-모음/Obsidian-Publish-방법들":{"title":"Obsidian Publish 방법들","links":[],"tags":[],"content":"\nFlowershow\n\n이건 next.js라 github page 로 호스팅 불가\n\n\nquartz\nObsidian Vault to Hugo Content\n\n이친구는 아예 Obsidian MarkDown을 변환하는것들\n\n\n\n참고 디자인\n\niingang.github.io/\n"},"유용한-것들-모음/VS-code-SSH-접속-시-문제-해결":{"title":"VS code SSH 접속 시 문제 해결","links":[],"tags":[],"content":"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\n\nC:\\Users\\&lt;유저&gt;\\.ssh 폴더의 known_hosts 파일을 지우자\n\nWARNING: UNPROTECTED PRIVATE KEY FILE!\n\n해당 OpenSSH 키 파일 속성 화면 보안 탭 들어가기\n고급 클릭해서 상속사용안함 클릭\n이후 현재 사용자만 추가하여 저장\n"},"유용한-것들-모음/뭔가-문서로-분리하긴-뭐한":{"title":"뭔가 문서로 분리하긴 뭐한","links":[],"tags":[],"content":"수학기호\n\ndanbi-ncsoft.github.io/study/2018/05/30/study-how_to_read_mathematical_expression.html\n\n과학적 기수법\n\n숫자를 1e+6 이런식으로 지수(e) 로 표기하는법\n\n\n1e+6:\n\n숫자 1 다음에 0이 6게 온다\n1000000\n\n\n2e+6: 2000000\n\n배치파일 표준 입력 프로그램 처리\necho 보낼꺼 | 프로그램.exe\n \n# 여러줄\n( echo 보낼꺼1 echo 보낼꺼2 echo 보낼꺼3 ) | your_program.exe \n서버 포트 바인딩 규칙\n\n반드시 외부 접속을 위해서는 0.0.0.0 으로!\n"},"유용한-것들-모음/안드로이드-adb-무선-디버깅":{"title":"안드로이드 adb 무선 디버깅","links":[],"tags":[],"content":"\n안드로이드 스튜디오 없이 ADB 명령만으로 무선디버깅을 사용하는 법\n당연하지만 adb는 환경변수 설정 하거나, adb가 있는 디렉토리에서 실행\n\n방법\n\n\n                  \n                  참고자료 \n                  \n                \n\n\n명령줄을 사용한 Wi-Fi 연결\n\n\n\n\n폰에서 무선디버깅 활성화\n무선디버깅 화면에 IP 주소 및 포트 확인\n아래 명령어를 입력\nadb connect &lt;IP주소&gt;:&lt;포트번호&gt;\n \n===사용안하거나 문제 발생 시===\nadb kill-server\n\n\n해당 wifi 에서 최초 진행 시\n\n무선 디버깅 활성화\n패어링 코드로 기기 페어링 클릭\n아래 명령어 입력\nadb pair &lt;패어링 IP주소&gt;:&lt;페어링 포트번호&gt;\n\n이후 패어링 코드를 입력하라고 하면 화면에 보이는 코드 입력\n\n주의 사항\n\n안드로이드 폰에 Wifi 가 활성화 되어있고, 디버깅 환경과 폰이 같은 네트워크에 존재 해야함\n따라서 다른 네트워크 상태라면 VPN을 사용 하던가 해야함\n"},"유용한-것들-모음/짜잘한-윈도우-팁-모음":{"title":"짜잘한 윈도우 팁 모음","links":[],"tags":[],"content":"윈도우 우클릭 콘택스트 메뉴 편집\n\noldnew.tistory.com/322\n\nVS Code 터미널 권한 오류 해결\n\nPowerShell을 관리자 권한으로 실행 후 해당 명령 입력\n[set-executionpolicy remotesigned]\n\n윈도우 콘솔 한글 인코딩 문제\n\n윈도우 콘솔은 MS949(CP-949 | EUC-KR) 로 출력 됨\n프로그램의 한글이 깨지는 이유는 그 프로그램이 UTF-8 로 출력하거나 그 외 다른 인코딩으로 출력하기 때문임\n\n콘솔 인코딩은 MS949 로 해석하는데 받은 출력은 UTF-8 이니 그걸 MS949 로 해석 해서 한글이 깨지는 것\n\n\n\n해결방법\n\n해당 프로그램의 출력 인코딩을 변경할 수 있으면 MS949 로 바꾸던가\n아님 chcp 명령어로 해당 프로그램에 맞춰 인코딩을 바꾸던가\n하지만 해당 설정은 1회성 이긴함 (닫으면 끝)\n# 콘솔 인코딩을 UTF-8로 변경\nchcp 65001\n\n\n윈도우11 볼륨컨트롤 미디어 뷰어 복원\n\n인터넷에 검색할때 media flyout 이라고 검색하면 됨\n\n\nModernFlyouts\n"},"유용한-것들-모음/클라우드-컴퓨팅-비교":{"title":"클라우드 컴퓨팅 비교","links":[],"tags":[],"content":"\nAWS:\n\n근본 중에 근본. 새 계정 1년 모든리전 프리티어 제공\n\n\nGCP:\n\n계정별 100일 정도 사용가능한 300$ 크래딧 제공\nGPU 쓸 때 좋음\n평생무료 티어가 있긴 한데 미국한정이라 별 쓸모 없음\n\n\n오라클 클라우드:\n\n프리티어 개 해자\nARM 4코어 CPU 24GB RAM 인스턴트가 평생무료티어 임…\n근데 신용카드 없으면 가입불가 한듯\n\n\nms azure:\n\n써보진 않음\n\n\nvultr:\n\n어느 서비스보다 개 저렴함\n제일 작은 인스턴스 하나 돌릴려면 AWS 기준으로 월 7.4$ 정도 드는데\n여기선 5$ 정도면 됨ㄷㄷ\n이것도 GCP 와 마찬가지로 평생무료 티어가 있긴한데 미국, 독일 한정이라 별 쓸모 없음\n\n\n"},"유용한-것들-모음/헷갈리는-프로그래밍-연산":{"title":"헷갈리는 프로그래밍 연산","links":[],"tags":[],"content":"나눗셈 처리\n\n\n1/2 를 프로그래밍 에서 하면\n// 답 0\nSystem.out.println(1/2);\n \n// 답 0.5\nSystem.out.println((float) 1/2);\n\n\n1%2 를 프로그래밍 에서 하면\n// 답 1\n// 입력된 수 그대로 나옴\nSystem.out.println(1%2);\n\n\n뺄/덧샘 할당\n\n\n덧샘 할당에 연산식 있다면 연산식 먼저 처리\n대입 연산, 대입/할당 연산자 모두 같은 레벨에 연산자 우선순위를 가지며,\n가장 우선순위가 낮음\nint a = 10;\na -= 3+2;\n \n// 답 5\n// a-= 5\nSystem.out.println(a);\n\n\n비트 쉬프트\n\n\n1 &lt;&lt; 2:\n\n뒤에 비트를 2만큼 추가\n1 → 100\n\n\n\n16 &gt;&gt; 2\n\n뒤에 비트를 2만큼 제거\n16=10000 → 4=100\n\n\n"}}